[
  {
    "objectID": "ml.html",
    "href": "ml.html",
    "title": "ML",
    "section": "",
    "text": "See all code for this workbook’s plots and results: * Code for this workbook\nOur machine learning work tied our NLP data of comments from the r/investing, r/cryptocurrency, and r/finance subreddits together to investigate whether these forums might influence the price movement of cryptocurrency. We investigated this connection because of the much-discussed relationship between so-called ‘meme stock’ investors (retail investors who favor risky assets and cause market volatility) and cryptocurrency as an asset class. In particular, we wanted to investigate the relationship between the Bitcoin market and the volume of comments on these subreddits, the volume of comments mentioning particular topics (cryptocurrencies, the s&p 500, and the federal reserve), and the sentiment of those comments. Unfortunately, upon training and comparing several models, we found little to no measurable relationship between the comment data and the movement in Bitcoins price, and our models could not outperform naive baselines.\nQuestion 1: Does the number and sentiment of Reddit comments in the studied subreddits predict whether Bitcoin’s price will increase the next day?\nQuestion 2: Does the number and sentiment of Reddit comments in the studied subreddits predict Bitcoin’s volatility the next day?\n\n\n\n\n\n\nFor the ML tab, each unit of observation is one day in our dataset. So even though we have 3 million comments we only have around 424 observations. We found this to be the best approach to our topic, because markets move at a large scale. As such, we thought we would be more likely to see an affect of the subreddits in the market across many aggregated comments together, rather than a model which operated at the comment level.\nWe also though it important to note that our apriori expectation is that the null hypothesis is very likely, given that financial markets are known to be noisy. Futher, the efficient market hypothesis dictates that if there were truly a relationship bebtween a particular social media website and market movements, this should be found and exploited quickly, until the pattern was nullified. On the other hand, we are exploring cryptocurrency markets, which historically have been more responsive to online community sentiment and other inefficient forces, so we still thought this was an interesting avenue to pursue.\n\n\n\n\n\nOur data on Bitcoin pricing comes from the investing.com website, which has daily historic price data, including the daily high, open, and low prices. From these raw numbers, we defined an increase as a day where the price of Bitcoin closed higher than it opened, and we defined volatility as the daily range (high price minus low price) measured as a percent of the opening price. These two features became our response variables for the models.\nOur predictor variables started with the outputs of our NLP model. The first 3 features was the daily tally of comments mentioning each of the three topics outlined above (federal reserve, crypto, and the S&P) as identified by regex matching. The next 3 were the daily tally of comments flagged as positive, negative, and neutral by the pretrained deep learning sentiment model.\nOur pipeline for these variables was to: 1) Merge the predictors and response variables into a single PySpark data frame, joining with the dates as unique IDs. Each day’s predictor variables were matched to the NEXT days price movements, so that we were predicting future price movements and never previous or contemporaneous ones. 2) We vectorized and normalized the variables using a PySpark pipeline, 3) We split the data 70-20-10 into training, testing, and validation datasets. 4) We fit the pipeline ONLY on the training data to avoid information leak, and then passed all of the datasets through it.\n\n\n\n\nWhile we set out to answer two different predictive tasks, one being a classification task (next day direction) and the other being a regression task (next day volatility), we followed a similar procedure for both.\nIn each case we fit a regression model and a gradient boosted trees model. All models were fully implemented in PySpark. The regression model was linear regression for the next-day volatility, and a logistic regression model for next-day price direction. The gradient boosted trees models were the GBTRegressor and the GBTClassifier for each task, respectively. After fitting a version of all four models with default parameters, we used the CrossValidator tool to create another version with tuned hyperparameters. Hyperparameters were evaluated across several folds within the training data, and the best model was selected according to the chosen evaluation metric (rmse for regression and accuracy for classification). Then, we compared the efficacy of these models to answer the questions.\n\n\n\nAs expected, the data was highly noisy, and all four models performed poorly at predicting the next-days price movements.\n\n\n\nROC Curves logistic\n\n\nThe reasoning behind the poor logistic regression result appears to lie in the intercept value. Essentially, the logistic regression model trained to be a most-commonly-occuring-class naive predictor, because no information contained in the predictors enabled the model to beat simply guessing the price of Bitcoint would increase every day.\n\n\n\nROC Curves for Trained and Untrained GBT Classifiers\n\n\nThe GBT models, however, made differentiated predictions based on the data. Unfortunately, neither of these could beat the naive predictor (with area 0.5). However, the tuned model did perform slightly better than the untuned model, suggesting that careful selection of hyperparameters did make a difference. Overall however, there was little reason to think that any of the four models contained useful information on the classification task at hand.\n\nAccuracy by Model\n\n\nDefault\nLogistic\nLogistic Tuned\nGBT\nGBT Tuned\n\n\n\n\nTrain\n0.548\n0.548\n0.828\n0.786\n\n\nTest\n0.439\n0.439\n0.439\n0.476\n\n\n\nThe gradient boosted classifier which did not have hyperparameter tuning shows clear overfitting in the disparity between the training accuracy (0.83) and testing accuracy (0.44). However, across the models, the accuracy was quite low in the test set, with all four failing to achieve 50% accuracy. As such, we would conclude that the number and sentiment of Reddit comments on the subreddits we studied did not have a predictive relationship with Bitcoin price increases, at least that we could discern with the models tests. Amongst the models however, the hyperparameter tuned Gradient Boosted Trees was the best at avoiding overfitting of the training data.\n\n\n\nThe same procedure was followed for the regression task, and once again the results did not suggest strong predictive power amongst the predictors for the target variable (the next day’s Bitcoin price volatility).\n\n\n\nTest results for the regression task, GBT regressor vs. Linear Regression\n\n\nThe two approaches’ predictions in the chart above demonstrate opposite problems. While the Gradient Boosted Trees regressor makes oversized predictions erroneously, the linear regression model nearly always predicts the mean value with little deviance. The tuned models both had regularization terms which prompted them to reduce coefficients to 0 and simply predict the mean value, so the predictions shown above are from the untuned values. That result follows from the low explanatory power of the variables and lack of relationship with the predictor.\n\nRMSE by Model\n\n\n\n\n\n\n\n\n\nDefault\nLinear Reg.\nLinear Reg. Tuned\nGBT Regressor\nGBT Regressor Tuned\n\n\n\n\nTrain\n0.022\n0.023\n0.015\n0.022\n\n\nTest\n0.022\n0.022\n0.025\n0.022\n\n\n\nAs shown above, the RMSE of the models was quite high, at around 2% daily volatilty. However, while the models overall did not fit the data, the coefficients do provide some insight to answer the research question. The most important features in the linear regression model were the number of comments mentioning particular topics, especially cryptocurrency and the S&P. In particular, each comment mentioning the S&P on a given day was tied to about .006% more daily volatility in Bitcoin’s price, and 0.002% per comment mentioning cryptocurrency. Overall, however, these coefficients were part of a model with very little explanatory power, but they at least demonstrate the direction of the relationship between the variables according to once appraoch. As such, we would conclude that the number and sentiment of Reddit comments has no bearing on the next-day volatility of Bitcoin prices."
  },
  {
    "objectID": "ml.html#introduction",
    "href": "ml.html#introduction",
    "title": "ML",
    "section": "",
    "text": "See all code for this workbook’s plots and results: * Code for this workbook\nOur machine learning work tied our NLP data of comments from the r/investing, r/cryptocurrency, and r/finance subreddits together to investigate whether these forums might influence the price movement of cryptocurrency. We investigated this connection because of the much-discussed relationship between so-called ‘meme stock’ investors (retail investors who favor risky assets and cause market volatility) and cryptocurrency as an asset class. In particular, we wanted to investigate the relationship between the Bitcoin market and the volume of comments on these subreddits, the volume of comments mentioning particular topics (cryptocurrencies, the s&p 500, and the federal reserve), and the sentiment of those comments. Unfortunately, upon training and comparing several models, we found little to no measurable relationship between the comment data and the movement in Bitcoins price, and our models could not outperform naive baselines.\nQuestion 1: Does the number and sentiment of Reddit comments in the studied subreddits predict whether Bitcoin’s price will increase the next day?\nQuestion 2: Does the number and sentiment of Reddit comments in the studied subreddits predict Bitcoin’s volatility the next day?"
  },
  {
    "objectID": "ml.html#ml-analysis-report",
    "href": "ml.html#ml-analysis-report",
    "title": "ML",
    "section": "",
    "text": "For the ML tab, each unit of observation is one day in our dataset. So even though we have 3 million comments we only have around 424 observations. We found this to be the best approach to our topic, because markets move at a large scale. As such, we thought we would be more likely to see an affect of the subreddits in the market across many aggregated comments together, rather than a model which operated at the comment level.\nWe also though it important to note that our apriori expectation is that the null hypothesis is very likely, given that financial markets are known to be noisy. Futher, the efficient market hypothesis dictates that if there were truly a relationship bebtween a particular social media website and market movements, this should be found and exploited quickly, until the pattern was nullified. On the other hand, we are exploring cryptocurrency markets, which historically have been more responsive to online community sentiment and other inefficient forces, so we still thought this was an interesting avenue to pursue.\n\n\n\n\n\nOur data on Bitcoin pricing comes from the investing.com website, which has daily historic price data, including the daily high, open, and low prices. From these raw numbers, we defined an increase as a day where the price of Bitcoin closed higher than it opened, and we defined volatility as the daily range (high price minus low price) measured as a percent of the opening price. These two features became our response variables for the models.\nOur predictor variables started with the outputs of our NLP model. The first 3 features was the daily tally of comments mentioning each of the three topics outlined above (federal reserve, crypto, and the S&P) as identified by regex matching. The next 3 were the daily tally of comments flagged as positive, negative, and neutral by the pretrained deep learning sentiment model.\nOur pipeline for these variables was to: 1) Merge the predictors and response variables into a single PySpark data frame, joining with the dates as unique IDs. Each day’s predictor variables were matched to the NEXT days price movements, so that we were predicting future price movements and never previous or contemporaneous ones. 2) We vectorized and normalized the variables using a PySpark pipeline, 3) We split the data 70-20-10 into training, testing, and validation datasets. 4) We fit the pipeline ONLY on the training data to avoid information leak, and then passed all of the datasets through it.\n\n\n\n\nWhile we set out to answer two different predictive tasks, one being a classification task (next day direction) and the other being a regression task (next day volatility), we followed a similar procedure for both.\nIn each case we fit a regression model and a gradient boosted trees model. All models were fully implemented in PySpark. The regression model was linear regression for the next-day volatility, and a logistic regression model for next-day price direction. The gradient boosted trees models were the GBTRegressor and the GBTClassifier for each task, respectively. After fitting a version of all four models with default parameters, we used the CrossValidator tool to create another version with tuned hyperparameters. Hyperparameters were evaluated across several folds within the training data, and the best model was selected according to the chosen evaluation metric (rmse for regression and accuracy for classification). Then, we compared the efficacy of these models to answer the questions.\n\n\n\nAs expected, the data was highly noisy, and all four models performed poorly at predicting the next-days price movements.\n\n\n\nROC Curves logistic\n\n\nThe reasoning behind the poor logistic regression result appears to lie in the intercept value. Essentially, the logistic regression model trained to be a most-commonly-occuring-class naive predictor, because no information contained in the predictors enabled the model to beat simply guessing the price of Bitcoint would increase every day.\n\n\n\nROC Curves for Trained and Untrained GBT Classifiers\n\n\nThe GBT models, however, made differentiated predictions based on the data. Unfortunately, neither of these could beat the naive predictor (with area 0.5). However, the tuned model did perform slightly better than the untuned model, suggesting that careful selection of hyperparameters did make a difference. Overall however, there was little reason to think that any of the four models contained useful information on the classification task at hand.\n\nAccuracy by Model\n\n\nDefault\nLogistic\nLogistic Tuned\nGBT\nGBT Tuned\n\n\n\n\nTrain\n0.548\n0.548\n0.828\n0.786\n\n\nTest\n0.439\n0.439\n0.439\n0.476\n\n\n\nThe gradient boosted classifier which did not have hyperparameter tuning shows clear overfitting in the disparity between the training accuracy (0.83) and testing accuracy (0.44). However, across the models, the accuracy was quite low in the test set, with all four failing to achieve 50% accuracy. As such, we would conclude that the number and sentiment of Reddit comments on the subreddits we studied did not have a predictive relationship with Bitcoin price increases, at least that we could discern with the models tests. Amongst the models however, the hyperparameter tuned Gradient Boosted Trees was the best at avoiding overfitting of the training data.\n\n\n\nThe same procedure was followed for the regression task, and once again the results did not suggest strong predictive power amongst the predictors for the target variable (the next day’s Bitcoin price volatility).\n\n\n\nTest results for the regression task, GBT regressor vs. Linear Regression\n\n\nThe two approaches’ predictions in the chart above demonstrate opposite problems. While the Gradient Boosted Trees regressor makes oversized predictions erroneously, the linear regression model nearly always predicts the mean value with little deviance. The tuned models both had regularization terms which prompted them to reduce coefficients to 0 and simply predict the mean value, so the predictions shown above are from the untuned values. That result follows from the low explanatory power of the variables and lack of relationship with the predictor.\n\nRMSE by Model\n\n\n\n\n\n\n\n\n\nDefault\nLinear Reg.\nLinear Reg. Tuned\nGBT Regressor\nGBT Regressor Tuned\n\n\n\n\nTrain\n0.022\n0.023\n0.015\n0.022\n\n\nTest\n0.022\n0.022\n0.025\n0.022\n\n\n\nAs shown above, the RMSE of the models was quite high, at around 2% daily volatilty. However, while the models overall did not fit the data, the coefficients do provide some insight to answer the research question. The most important features in the linear regression model were the number of comments mentioning particular topics, especially cryptocurrency and the S&P. In particular, each comment mentioning the S&P on a given day was tied to about .006% more daily volatility in Bitcoin’s price, and 0.002% per comment mentioning cryptocurrency. Overall, however, these coefficients were part of a model with very little explanatory power, but they at least demonstrate the direction of the relationship between the variables according to once appraoch. As such, we would conclude that the number and sentiment of Reddit comments has no bearing on the next-day volatility of Bitcoin prices."
  },
  {
    "objectID": "get_ml_data.html",
    "href": "get_ml_data.html",
    "title": "Group 12",
    "section": "",
    "text": "!pip install cryptocmd\n\nCollecting cryptocmd\n  Downloading cryptocmd-0.6.4-py3-none-any.whl (9.1 kB)\nRequirement already satisfied: requests in c:\\users\\corwi\\anaconda3\\lib\\site-packages (from cryptocmd) (2.32.3)\nCollecting tablib (from cryptocmd)\n  Downloading tablib-3.7.0-py3-none-any.whl (47 kB)\n                                              0.0/47.5 kB ? eta -:--:--\n     ---------------------------------------- 47.5/47.5 kB 2.3 MB/s eta 0:00:00\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in c:\\users\\corwi\\anaconda3\\lib\\site-packages (from requests-&gt;cryptocmd) (2.0.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in c:\\users\\corwi\\anaconda3\\lib\\site-packages (from requests-&gt;cryptocmd) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in c:\\users\\corwi\\anaconda3\\lib\\site-packages (from requests-&gt;cryptocmd) (1.26.16)\nRequirement already satisfied: certifi&gt;=2017.4.17 in c:\\users\\corwi\\anaconda3\\lib\\site-packages (from requests-&gt;cryptocmd) (2024.8.30)\nInstalling collected packages: tablib, cryptocmd\nSuccessfully installed cryptocmd-0.6.4 tablib-3.7.0\n\n\n\n# alpha vnatage API key: 53CG7RIYNCPMFA34\nimport requests\nurl = 'https://www.alphavantage.co/query?function=DIGITAL_CURRENCY_DAILY&symbol=BTC&market=USD&RANGE=2022-07-01&RANGE=2023-08-31&apikey=53CG7RIYNCPMFA34&datatype=csv'\nr = requests.get(url)\n#data = r.json()\n\n\nurl = 'https://www.alphavantage.co/query?function=CURRENCY_EXCHANGE_RATE&from_currency=BTC&to_currency=USD&market=USD&RANGE=2022-07-01&RANGE=2023-08-31&apikey=53CG7RIYNCPMFA34&datatype=csv'\n\n\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\n\ntest_df = pd.DataFrame.from_dict(data, orient = 'index')\ntest2_df = json_normalize(data)\n\nC:\\Users\\corwi\\AppData\\Local\\Temp\\ipykernel_1820\\2078720994.py:6: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead.\n  test2_df = json_normalize(data)\n\n\n\ndef flatten_nested_json_df(df):\n    \n    df = df.reset_index()\n    \n    print(f\"original shape: {df.shape}\")\n    print(f\"original columns: {df.columns}\")\n    \n    \n    # search for columns to explode/flatten\n    s = (df.applymap(type) == list).all()\n    list_columns = s[s].index.tolist()\n    \n    s = (df.applymap(type) == dict).all()\n    dict_columns = s[s].index.tolist()\n    \n    print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n    while len(list_columns) &gt; 0 or len(dict_columns) &gt; 0:\n        new_columns = []\n        \n        for col in dict_columns:\n            print(f\"flattening: {col}\")\n            # explode dictionaries horizontally, adding new columns\n            horiz_exploded = pd.json_normalize(df[col]).add_prefix(f'{col}.')\n            horiz_exploded.index = df.index\n            df = pd.concat([df, horiz_exploded], axis=1).drop(columns=[col])\n            new_columns.extend(horiz_exploded.columns) # inplace\n        \n        for col in list_columns:\n            print(f\"exploding: {col}\")\n            # explode lists vertically, adding new columns\n            df = df.drop(columns=[col]).join(df[col].explode().to_frame())\n            # Prevent combinatorial explosion when multiple\n            # cols have lists or lists of lists\n            df = df.reset_index(drop=True)\n            new_columns.append(col)\n        \n        # check if there are still dict o list fields to flatten\n        s = (df[new_columns].applymap(type) == list).all()\n        list_columns = s[s].index.tolist()\n\n        s = (df[new_columns].applymap(type) == dict).all()\n        dict_columns = s[s].index.tolist()\n        \n        print(f\"lists: {list_columns}, dicts: {dict_columns}\")\n        \n    print(f\"final shape: {df.shape}\")\n    print(f\"final columns: {df.columns}\")\n    return df\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\n1. Information\n2. Digital Currency Code\n3. Digital Currency Name\n4. Market Code\n5. Market Name\n6. Last Refreshed\n7. Time Zone\n2024-12-04\n2024-12-03\n2024-12-02\n...\n2023-12-30\n2023-12-29\n2023-12-28\n2023-12-27\n2023-12-26\n2023-12-25\n2023-12-24\n2023-12-23\n2023-12-22\n2023-12-21\n\n\n\n\nMeta Data\nDaily Prices and Volumes for Digital Currency\nBTC\nBitcoin\nEUR\nEuro\n2024-12-04 00:00:00\nUTC\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nTime Series (Digital Currency Daily)\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n{'1. open': '91249.72000000', '2. high': '9188...\n{'1. open': '91279.95000000', '2. high': '9175...\n{'1. open': '92096.25000000', '2. high': '9309...\n...\n{'1. open': '38216.07000000', '2. high': '3868...\n{'1. open': '38498.12000000', '2. high': '3906...\n{'1. open': '39156.55000000', '2. high': '3944...\n{'1. open': '38533.41000000', '2. high': '3938...\n{'1. open': '39583.10000000', '2. high': '3959...\n{'1. open': '39195.20000000', '2. high': '3986...\n{'1. open': '39810.33000000', '2. high': '4000...\n{'1. open': '39998.13000000', '2. high': '4001...\n{'1. open': '39877.59000000', '2. high': '4039...\n{'1. open': '39898.26000000', '2. high': '4026...\n\n\n\n\n2 rows × 357 columns\n\n\n\n\ntest3 = flatten_nested_json_df(test_df.transpose())\n\noriginal shape: (357, 3)\noriginal columns: Index(['index', 'Meta Data', 'Time Series (Digital Currency Daily)'], dtype='object')\nlists: [], dicts: []\nfinal shape: (357, 3)\nfinal columns: Index(['index', 'Meta Data', 'Time Series (Digital Currency Daily)'], dtype='object')\n\n\n\ncustom_col = list(test_df.columns)\ndaily_dat = list(test_df.iloc[1,:])\n\nprint(daily_dat)\nprint(custom_col)\n\n\n[nan, nan, nan, nan, nan, nan, nan, {'1. open': '91249.72000000', '2. high': '91880.00000000', '3. low': '90935.75000000', '4. close': '91778.68000000', '5. volume': '14.68991966'}, {'1. open': '91279.95000000', '2. high': '91751.07000000', '3. low': '88886.03000000', '4. close': '91229.73000000', '5. volume': '378.30146135'}, {'1. open': '92096.25000000', '2. high': '93098.99000000', '3. low': '90011.05000000', '4. close': '91275.31000000', '5. volume': '371.87919391'}, {'1. open': '91143.07000000', '2. high': '92599.99000000', '3. low': '90470.00000000', '4. close': '92099.58000000', '5. volume': '195.27489538'}, {'1. open': '92138.65000000', '2. high': '92154.35000000', '3. low': '90722.66000000', '4. close': '91143.29000000', '5. volume': '147.37387973'}, {'1. open': '90609.11000000', '2. high': '93535.47000000', '3. low': '90296.09000000', '4. close': '92147.09000000', '5. volume': '370.72697579'}, {'1. open': '90831.46000000', '2. high': '91601.62000000', '3. low': '89731.44000000', '4. close': '90616.99000000', '5. volume': '340.65417656'}, {'1. open': '87604.60000000', '2. high': '92207.53000000', '3. low': '87423.34000000', '4. close': '90854.12000000', '5. volume': '716.47333372'}, {'1. open': '88922.48000000', '2. high': '90777.81000000', '3. low': '86543.90000000', '4. close': '87600.65000000', '5. volume': '1087.63732168'}, {'1. open': '93421.69000000', '2. high': '94400.00000000', '3. low': '88070.46000000', '4. close': '88950.00000000', '5. volume': '946.05680701'}, {'1. open': '93200.09000000', '2. high': '94118.48000000', '3. low': '91129.86000000', '4. close': '93421.96000000', '5. volume': '520.88749517'}, {'1. open': '94705.80000000', '2. high': '94705.80000000', '3. low': '92680.84000000', '4. close': '93208.66000000', '5. volume': '363.23357291'}, {'1. open': '93564.35000000', '2. high': '95501.25000000', '3. low': '93075.21000000', '4. close': '94718.42000000', '5. volume': '1041.06144805'}, {'1. open': '89274.21000000', '2. high': '94235.00000000', '3. low': '89140.88000000', '4. close': '93554.33000000', '5. volume': '1637.55645015'}, {'1. open': '87182.40000000', '2. high': '90088.98000000', '3. low': '86446.65000000', '4. close': '89280.02000000', '5. volume': '671.56658014'}, {'1. open': '85409.31000000', '2. high': '88742.51000000', '3. low': '85337.81000000', '4. close': '87157.00000000', '5. volume': '693.28942984'}, {'1. open': '85046.28000000', '2. high': '87599.00000000', '3. low': '84594.12000000', '4. close': '85423.38000000', '5. volume': '703.24208600'}, {'1. open': '85667.74000000', '2. high': '86397.38000000', '3. low': '83913.99000000', '4. close': '85016.99000000', '5. volume': '389.00874594'}, {'1. open': '86256.73000000', '2. high': '86900.22000000', '3. low': '85171.44000000', '4. close': '85665.95000000', '5. volume': '220.55102249'}, {'1. open': '82893.06000000', '2. high': '87076.83000000', '3. low': '82621.18000000', '4. close': '86260.52000000', '5. volume': '532.44187883'}, {'1. open': '85286.00000000', '2. high': '87076.83000000', '3. low': '82317.57000000', '4. close': '82917.20000000', '5. volume': '877.49846428'}, {'1. open': '82311.26000000', '2. high': '88000.00000000', '3. low': '80903.15000000', '4. close': '85278.66000000', '5. volume': '1364.44961113'}, {'1. open': '82273.98000000', '2. high': '84457.98000000', '3. low': '75700.00000000', '4. close': '82334.41000000', '5. volume': '1838.12262283'}, {'1. open': '74780.73000000', '2. high': '82900.00000000', '3. low': '74639.10000000', '4. close': '82274.06000000', '5. volume': '1441.28614835'}, {'1. open': '71562.42000000', '2. high': '75699.99000000', '3. low': '71388.80000000', '4. close': '74757.19000000', '5. volume': '760.24717970'}, {'1. open': '71418.19000000', '2. high': '71777.77000000', '3. low': '70664.61000000', '4. close': '71550.49000000', '5. volume': '166.98838724'}, {'1. open': '70324.72000000', '2. high': '72088.30000000', '3. low': '70132.73000000', '4. close': '71421.23000000', '5. volume': '269.30354873'}, {'1. open': '70340.00000000', '2. high': '71193.11000000', '3. low': '68901.04000000', '4. close': '70292.42000000', '5. volume': '400.31083733'}, {'1. open': '63517.03000000', '2. high': '71149.78000000', '3. low': '63423.44000000', '4. close': '70304.54000000', '5. volume': '1364.77495693'}, {'1. open': '62348.64000000', '2. high': '64637.98000000', '3. low': '62022.59000000', '4. close': '63491.53000000', '5. volume': '282.78464079'}, {'1. open': '63408.26000000', '2. high': '63791.42000000', '3. low': '61410.68000000', '4. close': '62357.98000000', '5. volume': '301.31622731'}, {'1. open': '64132.60000000', '2. high': '64169.66000000', '3. low': '62351.09000000', '4. close': '63426.11000000', '5. volume': '359.15274360'}, {'1. open': '64144.26000000', '2. high': '64537.47000000', '3. low': '63776.12000000', '4. close': '64124.73000000', '5. volume': '48.98607369'}, {'1. open': '64506.42000000', '2. high': '65968.48000000', '3. low': '63230.24000000', '4. close': '64175.87000000', '5. volume': '258.86695384'}, {'1. open': '66595.95000000', '2. high': '66893.60000000', '3. low': '63946.30000000', '4. close': '64506.43000000', '5. volume': '308.79554682'}, {'1. open': '67148.64000000', '2. high': '67174.82000000', '3. low': '65914.41000000', '4. close': '66607.40000000', '5. volume': '252.20350371'}, {'1. open': '64636.79000000', '2. high': '68000.00000000', '3. low': '64422.84000000', '4. close': '67134.23000000', '5. volume': '707.11838829'}, {'1. open': '62928.68000000', '2. high': '64932.23000000', '3. low': '62623.13000000', '4. close': '64634.89000000', '5. volume': '326.05451082'}, {'1. open': '62155.83000000', '2. high': '63198.15000000', '3. low': '61985.78000000', '4. close': '62928.34000000', '5. volume': '106.17886596'}, {'1. open': '61740.05000000', '2. high': '62453.74000000', '3. low': '61565.45000000', '4. close': '62136.66000000', '5. volume': '65.10975685'}, {'1. open': '62979.27000000', '2. high': '63463.91000000', '3. low': '60776.28000000', '4. close': '61744.48000000', '5. volume': '235.57270922'}, {'1. open': '61803.03000000', '2. high': '63574.05000000', '3. low': '61629.62000000', '4. close': '62976.58000000', '5. volume': '313.65572182'}, {'1. open': '62400.56000000', '2. high': '62400.56000000', '3. low': '60465.00000000', '4. close': '61791.39000000', '5. volume': '165.08982555'}, {'1. open': '62160.00000000', '2. high': '62777.00000000', '3. low': '61420.41000000', '4. close': '62408.10000000', '5. volume': '387.06914029'}, {'1. open': '63445.78000000', '2. high': '63900.00000000', '3. low': '61649.96000000', '4. close': '62154.91000000', '5. volume': '513.43364230'}, {'1. open': '62840.49000000', '2. high': '63779.18000000', '3. low': '62597.49000000', '4. close': '63433.63000000', '5. volume': '126.77171012'}, {'1. open': '62966.41000000', '2. high': '63186.47000000', '3. low': '62268.10000000', '4. close': '62835.98000000', '5. volume': '91.10108229'}, {'1. open': '62200.36000000', '2. high': '63499.99000000', '3. low': '61956.89000000', '4. close': '62976.42000000', '5. volume': '452.48284802'}, {'1. open': '62235.03000000', '2. high': '62509.07000000', '3. low': '61444.00000000', '4. close': '62185.80000000', '5. volume': '204.80844458'}, {'1. open': '61574.43000000', '2. high': '62759.72000000', '3. low': '61274.78000000', '4. close': '62222.06000000', '5. volume': '284.82937583'}, {'1. open': '60413.47000000', '2. high': '62200.00000000', '3. low': '59377.80000000', '4. close': '61565.84000000', '5. volume': '405.94917438'}, {'1. open': '57516.93000000', '2. high': '60999.00000000', '3. low': '57148.57000000', '4. close': '60393.03000000', '5. volume': '497.97698980'}, {'1. open': '57721.17000000', '2. high': '57789.17000000', '3. low': '56653.98000000', '4. close': '57519.27000000', '5. volume': '148.87016103'}, {'1. open': '57164.90000000', '2. high': '58003.83000000', '3. low': '57119.07000000', '4. close': '57728.29000000', '5. volume': '101.28575501'}, {'1. open': '55142.72000000', '2. high': '57977.43000000', '3. low': '54950.51000000', '4. close': '57140.17000000', '5. volume': '224.21897559'}, {'1. open': '55370.10000000', '2. high': '56058.68000000', '3. low': '53904.09000000', '4. close': '55146.64000000', '5. volume': '220.05098312'}, {'1. open': '56623.09000000', '2. high': '57004.21000000', '3. low': '55090.00000000', '4. close': '55379.26000000', '5. volume': '184.88185859'}, {'1. open': '56703.35000000', '2. high': '57573.30000000', '3. low': '56373.07000000', '4. close': '56607.21000000', '5. volume': '159.12921466'}, {'1. open': '57256.48000000', '2. high': '58723.00000000', '3. low': '56600.00000000', '4. close': '56722.36000000', '5. volume': '207.05337233'}, {'1. open': '56522.61000000', '2. high': '57354.13000000', '3. low': '56300.00000000', '4. close': '57248.79000000', '5. volume': '92.42354624'}, {'1. open': '56588.84000000', '2. high': '56831.74000000', '3. low': '56187.41000000', '4. close': '56505.53000000', '5. volume': '58.20658723'}, {'1. open': '55047.76000000', '2. high': '56945.23000000', '3. low': '54785.63000000', '4. close': '56572.45000000', '5. volume': '229.25658222'}, {'1. open': '54893.39000000', '2. high': '55719.54000000', '3. low': '54237.15000000', '4. close': '55053.60000000', '5. volume': '325.96403893'}, {'1. open': '54908.65000000', '2. high': '56446.85000000', '3. low': '54301.00000000', '4. close': '54905.63000000', '5. volume': '314.17660861'}, {'1. open': '56850.00000000', '2. high': '57713.90000000', '3. low': '54301.02000000', '4. close': '54900.00000000', '5. volume': '332.62712344'}, {'1. open': '58752.02000000', '2. high': '58757.86000000', '3. low': '56421.56000000', '4. close': '56853.11000000', '5. volume': '284.50661756'}, {'1. open': '59006.02000000', '2. high': '59130.89000000', '3. low': '58620.64000000', '4. close': '58742.25000000', '5. volume': '99.07191940'}, {'1. open': '58959.93000000', '2. high': '59348.82000000', '3. low': '58610.19000000', '4. close': '58982.04000000', '5. volume': '83.33974660'}, {'1. open': '58329.32000000', '2. high': '59528.74000000', '3. low': '58057.40000000', '4. close': '58936.22000000', '5. volume': '271.49194403'}, {'1. open': '56715.02000000', '2. high': '58940.47000000', '3. low': '56281.39000000', '4. close': '58315.28000000', '5. volume': '324.94929400'}, {'1. open': '57453.84000000', '2. high': '57924.98000000', '3. low': '56526.64000000', '4. close': '56730.08000000', '5. volume': '169.19799330'}, {'1. open': '57003.00000000', '2. high': '57822.22000000', '3. low': '56259.42000000', '4. close': '57426.47000000', '5. volume': '218.53383368'}, {'1. open': '56988.75000000', '2. high': '57970.81000000', '3. low': '56080.15000000', '4. close': '56985.67000000', '5. volume': '223.72967754'}, {'1. open': '56772.76000000', '2. high': '57321.60000000', '3. low': '55883.34000000', '4. close': '57001.09000000', '5. volume': '146.12246921'}, {'1. open': '56605.64000000', '2. high': '56937.10000000', '3. low': '56211.31000000', '4. close': '56753.13000000', '5. volume': '53.43438343'}, {'1. open': '56396.66000000', '2. high': '57414.44000000', '3. low': '55813.70000000', '4. close': '56634.75000000', '5. volume': '198.00698897'}, {'1. open': '55571.01000000', '2. high': '57241.75000000', '3. low': '55438.04000000', '4. close': '56385.95000000', '5. volume': '329.06309665'}, {'1. open': '54261.74000000', '2. high': '55604.62000000', '3. low': '53223.45000000', '4. close': '55562.82000000', '5. volume': '337.00000766'}, {'1. open': '52290.93000000', '2. high': '55193.91000000', '3. low': '51790.06000000', '4. close': '54226.05000000', '5. volume': '364.06498608'}, {'1. open': '53320.98000000', '2. high': '53409.25000000', '3. low': '51650.17000000', '4. close': '52276.88000000', '5. volume': '227.43877981'}, {'1. open': '54168.55000000', '2. high': '54488.83000000', '3. low': '52959.70000000', '4. close': '53343.95000000', '5. volume': '156.99599343'}, {'1. open': '54674.74000000', '2. high': '54756.51000000', '3. low': '53700.00000000', '4. close': '54164.78000000', '5. volume': '96.08980267'}, {'1. open': '52489.57000000', '2. high': '54781.85000000', '3. low': '52007.60000000', '4. close': '54673.01000000', '5. volume': '250.20037654'}, {'1. open': '52130.02000000', '2. high': '53116.26000000', '3. low': '51955.86000000', '4. close': '52494.18000000', '5. volume': '260.61066202'}, {'1. open': '52319.39000000', '2. high': '52657.81000000', '3. low': '50490.85000000', '4. close': '52074.02000000', '5. volume': '281.19770213'}, {'1. open': '51723.22000000', '2. high': '52649.00000000', '3. low': '51105.34000000', '4. close': '52311.79000000', '5. volume': '212.94548070'}, {'1. open': '49526.79000000', '2. high': '52624.00000000', '3. low': '49315.32000000', '4. close': '51718.32000000', '5. volume': '448.96083805'}, {'1. open': '48922.03000000', '2. high': '49905.18000000', '3. low': '48417.29000000', '4. close': '49505.02000000', '5. volume': '188.58514830'}, {'1. open': '48768.11000000', '2. high': '49585.80000000', '3. low': '48548.32000000', '4. close': '48900.38000000', '5. volume': '218.48358020'}, {'1. open': '50560.77000000', '2. high': '51382.12000000', '3. low': '47428.66000000', '4. close': '48768.11000000', '5. volume': '631.99766844'}, {'1. open': '52320.31000000', '2. high': '52652.63000000', '3. low': '50084.23000000', '4. close': '50556.80000000', '5. volume': '241.17270351'}, {'1. open': '52036.77000000', '2. high': '52835.55000000', '3. low': '50354.20000000', '4. close': '52323.86000000', '5. volume': '397.29997213'}, {'1. open': '53444.24000000', '2. high': '54065.45000000', '3. low': '51964.87000000', '4. close': '52033.15000000', '5. volume': '287.13574297'}, {'1. open': '51911.95000000', '2. high': '53700.00000000', '3. low': '51709.51000000', '4. close': '53430.92000000', '5. volume': '233.61573622'}, {'1. open': '53385.08000000', '2. high': '53476.90000000', '3. low': '51802.75000000', '4. close': '51931.36000000', '5. volume': '203.81632762'}, {'1. open': '53553.35000000', '2. high': '53855.82000000', '3. low': '53200.00000000', '4. close': '53405.31000000', '5. volume': '60.22093735'}, {'1. open': '53585.98000000', '2. high': '54096.18000000', '3. low': '52280.00000000', '4. close': '53546.16000000', '5. volume': '231.76194306'}, {'1. open': '53109.73000000', '2. high': '55248.92000000', '3. low': '52921.58000000', '4. close': '53604.00000000', '5. volume': '286.41399895'}, {'1. open': '53225.11000000', '2. high': '54189.93000000', '3. low': '52100.00000000', '4. close': '53088.39000000', '5. volume': '326.99133822'}, {'1. open': '56289.73000000', '2. high': '56613.32000000', '3. low': '51950.00000000', '4. close': '53225.98000000', '5. volume': '464.08729339'}, {'1. open': '57406.50000000', '2. high': '57632.29000000', '3. low': '56256.75000000', '4. close': '56298.34000000', '5. volume': '303.87103799'}, {'1. open': '57272.31000000', '2. high': '58113.94000000', '3. low': '56905.96000000', '4. close': '57402.19000000', '5. volume': '167.58380223'}, {'1. open': '57301.11000000', '2. high': '57632.96000000', '3. low': '56735.97000000', '4. close': '57269.55000000', '5. volume': '151.91637930'}, {'1. open': '54311.20000000', '2. high': '58047.53000000', '3. low': '54311.19000000', '4. close': '57308.88000000', '5. volume': '447.89770675'}, {'1. open': '54830.06000000', '2. high': '55146.00000000', '3. low': '53625.00000000', '4. close': '54327.75000000', '5. volume': '345.61932192'}, {'1. open': '53062.12000000', '2. high': '55458.37000000', '3. low': '52862.69000000', '4. close': '54841.80000000', '5. volume': '371.62377308'}, {'1. open': '53657.34000000', '2. high': '55469.03000000', '3. low': '52747.33000000', '4. close': '53076.19000000', '5. volume': '301.28743363'}, {'1. open': '53049.34000000', '2. high': '53800.00000000', '3. low': '52408.27000000', '4. close': '53652.29000000', '5. volume': '224.94083142'}, {'1. open': '54009.94000000', '2. high': '54688.34000000', '3. low': '53019.52000000', '4. close': '53046.89000000', '5. volume': '130.08025933'}, {'1. open': '53452.49000000', '2. high': '54194.89000000', '3. low': '53380.64000000', '4. close': '54000.00000000', '5. volume': '74.08506959'}, {'1. open': '52501.36000000', '2. high': '54340.13000000', '3. low': '52064.92000000', '4. close': '53464.65000000', '5. volume': '253.81422526'}, {'1. open': '53326.55000000', '2. high': '54508.09000000', '3. low': '51195.03000000', '4. close': '52491.11000000', '5. volume': '424.84476460'}, {'1. open': '55146.13000000', '2. high': '56081.19000000', '3. low': '53100.00000000', '4. close': '53328.00000000', '5. volume': '308.51748484'}, {'1. open': '54312.58000000', '2. high': '56141.89000000', '3. low': '53510.41000000', '4. close': '55129.90000000', '5. volume': '286.50224950'}, {'1. open': '53822.14000000', '2. high': '55555.00000000', '3. low': '52780.03000000', '4. close': '54307.22000000', '5. volume': '359.11681215'}, {'1. open': '55860.58000000', '2. high': '56707.04000000', '3. low': '53436.91000000', '4. close': '53835.66000000', '5. volume': '239.45474186'}, {'1. open': '55800.62000000', '2. high': '56358.00000000', '3. low': '55240.07000000', '4. close': '55859.19000000', '5. volume': '135.27159029'}, {'1. open': '56546.67000000', '2. high': '56593.13000000', '3. low': '54533.74000000', '4. close': '55789.08000000', '5. volume': '313.86928219'}, {'1. open': '50512.15000000', '2. high': '57499.00000000', '3. low': '50129.82000000', '4. close': '56542.08000000', '5. volume': '460.84007542'}, {'1. open': '51369.66000000', '2. high': '52950.00000000', '3. low': '50010.13000000', '4. close': '50495.77000000', '5. volume': '549.41047371'}, {'1. open': '49417.12000000', '2. high': '52299.98000000', '3. low': '49371.86000000', '4. close': '51374.96000000', '5. volume': '963.93343161'}, {'1. open': '53330.01000000', '2. high': '53448.81000000', '3. low': '45021.36000000', '4. close': '49408.05000000', '5. volume': '3196.63707067'}, {'1. open': '55719.17000000', '2. high': '56131.17000000', '3. low': '52521.27000000', '4. close': '53309.69000000', '5. volume': '1023.84859285'}, {'1. open': '56352.63000000', '2. high': '57110.64000000', '3. low': '55000.00000000', '4. close': '55717.27000000', '5. volume': '553.06040776'}, {'1. open': '60555.08000000', '2. high': '60794.76000000', '3. low': '56100.00000000', '4. close': '56347.78000000', '5. volume': '473.47114375'}, {'1. open': '59709.38000000', '2. high': '60811.58000000', '3. low': '57709.31000000', '4. close': '60559.44000000', '5. volume': '417.82100327'}, {'1. open': '61184.48000000', '2. high': '61804.95000000', '3. low': '59604.99000000', '4. close': '59715.77000000', '5. volume': '336.73080284'}, {'1. open': '61701.63000000', '2. high': '61910.98000000', '3. low': '60411.71000000', '4. close': '61177.08000000', '5. volume': '290.33895527'}, {'1. open': '62866.50000000', '2. high': '64749.00000000', '3. low': '61368.32000000', '4. close': '61700.00000000', '5. volume': '427.70514922'}, {'1. open': '62490.46000000', '2. high': '62887.22000000', '3. low': '61744.87000000', '4. close': '62854.02000000', '5. volume': '107.12922846'}, {'1. open': '62561.24000000', '2. high': '63876.72000000', '3. low': '61400.00000000', '4. close': '62488.02000000', '5. volume': '292.01454843'}, {'1. open': '60652.22000000', '2. high': '62830.25000000', '3. low': '60571.05000000', '4. close': '62563.79000000', '5. volume': '323.17954685'}, {'1. open': '60318.31000000', '2. high': '61004.00000000', '3. low': '58512.00000000', '4. close': '60650.01000000', '5. volume': '335.29952639'}, {'1. open': '60766.59000000', '2. high': '61800.00000000', '3. low': '60065.73000000', '4. close': '60333.99000000', '5. volume': '240.22230240'}, {'1. open': '62026.78000000', '2. high': '62220.35000000', '3. low': '60224.14000000', '4. close': '60755.96000000', '5. volume': '292.25848749'}, {'1. open': '62526.97000000', '2. high': '62821.99000000', '3. low': '61166.88000000', '4. close': '62030.69000000', '5. volume': '250.65858945'}, {'1. open': '61646.16000000', '2. high': '62721.17000000', '3. low': '60372.50000000', '4. close': '62529.01000000', '5. volume': '203.01047905'}, {'1. open': '61281.39000000', '2. high': '62064.88000000', '3. low': '60871.92000000', '4. close': '61651.61000000', '5. volume': '145.74291043'}, {'1. open': '58709.62000000', '2. high': '62000.00000000', '3. low': '58111.70000000', '4. close': '61283.57000000', '5. volume': '410.28483968'}, {'1. open': '58599.56000000', '2. high': '59624.66000000', '3. low': '58031.47000000', '4. close': '58699.58000000', '5. volume': '214.24423714'}, {'1. open': '59707.23000000', '2. high': '60675.00000000', '3. low': '58347.46000000', '4. close': '58600.56000000', '5. volume': '346.32186003'}, {'1. open': '59421.37000000', '2. high': '59998.99000000', '3. low': '57211.79000000', '4. close': '59699.03000000', '5. volume': '405.94613726'}, {'1. open': '55876.94000000', '2. high': '59585.00000000', '3. low': '55750.00000000', '4. close': '59432.32000000', '5. volume': '490.78590908'}, {'1. open': '54274.50000000', '2. high': '56400.63000000', '3. low': '54257.68000000', '4. close': '55876.86000000', '5. volume': '424.42726971'}, {'1. open': '53103.25000000', '2. high': '54863.10000000', '3. low': '52977.57000000', '4. close': '54274.18000000', '5. volume': '208.66300848'}, {'1. open': '52733.04000000', '2. high': '53662.74000000', '3. low': '52017.01000000', '4. close': '53109.92000000', '5. volume': '645.82671205'}, {'1. open': '53276.99000000', '2. high': '54744.40000000', '3. low': '52591.29000000', '4. close': '52728.22000000', '5. volume': '1016.80072263'}, {'1. open': '53663.86000000', '2. high': '54950.27000000', '3. low': '52789.11000000', '4. close': '53276.98000000', '5. volume': '605.65539207'}, {'1. open': '52267.68000000', '2. high': '53855.82000000', '3. low': '51865.03000000', '4. close': '53631.95000000', '5. volume': '1109.47042884'}, {'1. open': '51589.15000000', '2. high': '53655.00000000', '3. low': '50144.21000000', '4. close': '52254.91000000', '5. volume': '1457.34061510'}, {'1. open': '53664.31000000', '2. high': '53861.02000000', '3. low': '51450.59000000', '4. close': '51589.15000000', '5. volume': '293.21252865'}, {'1. open': '52267.47000000', '2. high': '53892.23000000', '3. low': '51726.29000000', '4. close': '53664.51000000', '5. volume': '707.35991637'}, {'1. open': '52790.03000000', '2. high': '53175.96000000', '3. low': '49437.20000000', '4. close': '52271.97000000', '5. volume': '1085.18051491'}, {'1. open': '55772.09000000', '2. high': '56001.96000000', '3. low': '52549.74000000', '4. close': '52790.43000000', '5. volume': '597.16889862'}, {'1. open': '57747.42000000', '2. high': '57888.74000000', '3. low': '55048.15000000', '4. close': '55766.78000000', '5. volume': '383.47526387'}, {'1. open': '58523.46000000', '2. high': '58894.41000000', '3. low': '57510.25000000', '4. close': '57733.05000000', '5. volume': '206.03530587'}, {'1. open': '58416.69000000', '2. high': '59467.74000000', '3. low': '58090.34000000', '4. close': '58527.56000000', '5. volume': '387.64433677'}, {'1. open': '56881.68000000', '2. high': '58647.31000000', '3. low': '56625.31000000', '4. close': '58407.41000000', '5. volume': '155.48993236'}, {'1. open': '56345.64000000', '2. high': '57117.87000000', '3. low': '56307.44000000', '4. close': '56888.03000000', '5. volume': '136.28146021'}, {'1. open': '57546.99000000', '2. high': '58162.14000000', '3. low': '55965.89000000', '4. close': '56344.17000000', '5. volume': '286.13125887'}, {'1. open': '56946.44000000', '2. high': '58164.61000000', '3. low': '56622.20000000', '4. close': '57545.90000000', '5. volume': '263.66774666'}, {'1. open': '57713.47000000', '2. high': '58294.10000000', '3. low': '56800.20000000', '4. close': '56935.04000000', '5. volume': '341.91484885'}, {'1. open': '56169.13000000', '2. high': '58215.85000000', '3. low': '56122.32000000', '4. close': '57713.87000000', '5. volume': '570.89956745'}, {'1. open': '59127.81000000', '2. high': '59261.30000000', '3. low': '54419.19000000', '4. close': '56186.44000000', '5. volume': '697.40788768'}, {'1. open': '59996.32000000', '2. high': '60246.86000000', '3. low': '59085.16000000', '4. close': '59107.12000000', '5. volume': '89.24138584'}, {'1. open': '59995.39000000', '2. high': '60336.51000000', '3. low': '59797.74000000', '4. close': '59992.10000000', '5. volume': '74.27808157'}, {'1. open': '60577.71000000', '2. high': '60719.65000000', '3. low': '59287.61000000', '4. close': '59987.59000000', '5. volume': '270.54473097'}, {'1. open': '60466.69000000', '2. high': '61937.39000000', '3. low': '60249.86000000', '4. close': '60550.86000000', '5. volume': '323.10050356'}, {'1. open': '60698.63000000', '2. high': '61218.74000000', '3. low': '60200.01000000', '4. close': '60456.22000000', '5. volume': '168.56697690'}, {'1. open': '61908.22000000', '2. high': '61988.95000000', '3. low': '59654.85000000', '4. close': '60685.06000000', '5. volume': '363.99799435'}, {'1. open': '62279.16000000', '2. high': '62696.68000000', '3. low': '60710.72000000', '4. close': '61904.82000000', '5. volume': '236.82698332'}, {'1. open': '61832.47000000', '2. high': '62539.34000000', '3. low': '61687.66000000', '4. close': '62262.17000000', '5. volume': '97.60381266'}, {'1. open': '61752.49000000', '2. high': '62111.00000000', '3. low': '61494.65000000', '4. close': '61829.19000000', '5. volume': '90.88944269'}, {'1. open': '62189.17000000', '2. high': '63090.90000000', '3. low': '60801.06000000', '4. close': '61747.33000000', '5. volume': '300.22901419'}, {'1. open': '63136.04000000', '2. high': '63396.45000000', '3. low': '61617.67000000', '4. close': '62171.28000000', '5. volume': '253.89933232'}, {'1. open': '62719.92000000', '2. high': '64579.89000000', '3. low': '62243.20000000', '4. close': '63140.41000000', '5. volume': '352.66408515'}, {'1. open': '64528.44000000', '2. high': '64588.76000000', '3. low': '61507.14000000', '4. close': '62726.38000000', '5. volume': '530.41475073'}, {'1. open': '64664.14000000', '2. high': '65298.24000000', '3. low': '64320.22000000', '4. close': '64535.72000000', '5. volume': '200.93621452'}, {'1. open': '64277.11000000', '2. high': '64842.00000000', '3. low': '64066.62000000', '4. close': '64648.18000000', '5. volume': '159.25443232'}, {'1. open': '64257.12000000', '2. high': '64500.00000000', '3. low': '64096.56000000', '4. close': '64278.15000000', '5. volume': '126.90552547'}, {'1. open': '64985.78000000', '2. high': '66132.47000000', '3. low': '63309.84000000', '4. close': '64258.71000000', '5. volume': '352.28449963'}, {'1. open': '65358.06000000', '2. high': '65766.87000000', '3. low': '64380.61000000', '4. close': '64984.68000000', '5. volume': '283.87105652'}, {'1. open': '64789.75000000', '2. high': '66063.64000000', '3. low': '64688.44000000', '4. close': '65357.71000000', '5. volume': '335.41897846'}, {'1. open': '63089.34000000', '2. high': '65294.96000000', '3. low': '62931.48000000', '4. close': '64777.18000000', '5. volume': '328.10654650'}, {'1. open': '62428.48000000', '2. high': '64762.10000000', '3. low': '62283.90000000', '4. close': '63080.70000000', '5. volume': '304.92947467'}, {'1. open': '62445.22000000', '2. high': '63050.23000000', '3. low': '62026.91000000', '4. close': '62452.47000000', '5. volume': '118.29560654'}, {'1. open': '62234.29000000', '2. high': '62561.82000000', '3. low': '62130.99000000', '4. close': '62441.25000000', '5. volume': '87.06024806'}, {'1. open': '63105.19000000', '2. high': '63537.51000000', '3. low': '61440.00000000', '4. close': '62230.86000000', '5. volume': '277.52115562'}, {'1. open': '62551.71000000', '2. high': '64124.08000000', '3. low': '62068.52000000', '4. close': '63095.75000000', '5. volume': '268.44322450'}, {'1. open': '62968.22000000', '2. high': '63480.38000000', '3. low': '62062.99000000', '4. close': '62548.69000000', '5. volume': '230.22853881'}, {'1. open': '63874.98000000', '2. high': '63968.61000000', '3. low': '61900.00000000', '4. close': '62969.80000000', '5. volume': '340.08480991'}, {'1. open': '63145.80000000', '2. high': '64989.91000000', '3. low': '62808.92000000', '4. close': '63872.07000000', '5. volume': '236.89637878'}, {'1. open': '63921.45000000', '2. high': '64114.63000000', '3. low': '62843.54000000', '4. close': '63138.93000000', '5. volume': '102.45131389'}, {'1. open': '63249.14000000', '2. high': '64198.39000000', '3. low': '63196.06000000', '4. close': '63939.25000000', '5. volume': '103.66428959'}, {'1. open': '62839.02000000', '2. high': '63878.65000000', '3. low': '61540.66000000', '4. close': '63232.74000000', '5. volume': '245.38978772'}, {'1. open': '63816.00000000', '2. high': '64541.13000000', '3. low': '61333.00000000', '4. close': '62836.76000000', '5. volume': '470.83392201'}, {'1. open': '64375.99000000', '2. high': '65138.74000000', '3. low': '63653.03000000', '4. close': '63816.00000000', '5. volume': '376.19678098'}, {'1. open': '65521.16000000', '2. high': '65933.02000000', '3. low': '63494.57000000', '4. close': '64375.99000000', '5. volume': '628.58154046'}, {'1. open': '60930.51000000', '2. high': '65700.00000000', '3. low': '60750.00000000', '4. close': '65521.16000000', '5. volume': '542.13763679'}, {'1. open': '61588.52000000', '2. high': '62261.43000000', '3. low': '60587.46000000', '4. close': '60927.75000000', '5. volume': '202.69537382'}, {'1. open': '61685.45000000', '2. high': '62012.19000000', '3. low': '61300.00000000', '4. close': '61579.64000000', '5. volume': '194.99132039'}, {'1. open': '60030.50000000', '2. high': '62081.49000000', '3. low': '59963.62000000', '4. close': '61687.99000000', '5. volume': '323.11749946'}, {'1. open': '60779.28000000', '2. high': '61400.00000000', '3. low': '59000.00000000', '4. close': '60018.57000000', '5. volume': '394.26686267'}, {'1. open': '56924.68000000', '2. high': '61022.00000000', '3. low': '56680.09000000', '4. close': '60777.05000000', '5. volume': '383.64791289'}, {'1. open': '58298.94000000', '2. high': '58478.73000000', '3. low': '56500.00000000', '4. close': '56924.05000000', '5. volume': '320.35856449'}, {'1. open': '57085.97000000', '2. high': '58770.58000000', '3. low': '56418.03000000', '4. close': '58293.80000000', '5. volume': '325.85438232'}, {'1. open': '56516.31000000', '2. high': '57435.99000000', '3. low': '56299.00000000', '4. close': '57075.98000000', '5. volume': '123.98655238'}, {'1. open': '56524.04000000', '2. high': '57167.19000000', '3. low': '56208.31000000', '4. close': '56496.94000000', '5. volume': '198.07642651'}, {'1. open': '58522.09000000', '2. high': '58882.51000000', '3. low': '55876.05000000', '4. close': '56514.18000000', '5. volume': '447.06249428'}, {'1. open': '56941.52000000', '2. high': '58835.13000000', '3. low': '56459.29000000', '4. close': '58521.43000000', '5. volume': '279.82735664'}, {'1. open': '57978.20000000', '2. high': '58657.29000000', '3. low': '56638.90000000', '4. close': '56949.08000000', '5. volume': '357.82329052'}, {'1. open': '58671.32000000', '2. high': '59857.37000000', '3. low': '57920.00000000', '4. close': '57961.38000000', '5. volume': '305.09384379'}, {'1. open': '59515.05000000', '2. high': '60834.00000000', '3. low': '58239.85000000', '4. close': '58681.47000000', '5. volume': '415.09319745'}, {'1. open': '59398.45000000', '2. high': '60038.98000000', '3. low': '58461.27000000', '4. close': '59506.98000000', '5. volume': '231.06694571'}, {'1. open': '58509.32000000', '2. high': '59999.00000000', '3. low': '58165.60000000', '4. close': '59392.98000000', '5. volume': '274.70356453'}, {'1. open': '55076.36000000', '2. high': '58931.62000000', '3. low': '54811.98000000', '4. close': '58507.14000000', '5. volume': '502.09441795'}, {'1. open': '54378.17000000', '2. high': '55643.98000000', '3. low': '53121.21000000', '4. close': '55082.13000000', '5. volume': '635.05368310'}, {'1. open': '56844.50000000', '2. high': '56991.58000000', '3. low': '52943.49000000', '4. close': '54372.60000000', '5. volume': '1258.79096130'}, {'1. open': '59581.09000000', '2. high': '60365.57000000', '3. low': '55311.31000000', '4. close': '56852.18000000', '5. volume': '772.86685399'}, {'1. open': '58965.50000000', '2. high': '59875.00000000', '3. low': '57745.77000000', '4. close': '59549.34000000', '5. volume': '503.46241480'}, {'1. open': '59364.36000000', '2. high': '60136.13000000', '3. low': '58647.82000000', '4. close': '58947.56000000', '5. volume': '181.01826606'}, {'1. open': '59639.85000000', '2. high': '59793.40000000', '3. low': '58411.49000000', '4. close': '59364.40000000', '5. volume': '226.12657975'}, {'1. open': '60143.74000000', '2. high': '60545.76000000', '3. low': '59242.36000000', '4. close': '59635.80000000', '5. volume': '354.61187190'}, {'1. open': '60074.92000000', '2. high': '60867.16000000', '3. low': '58755.00000000', '4. close': '60126.24000000', '5. volume': '389.91996958'}, {'1. open': '62060.81000000', '2. high': '62678.55000000', '3. low': '59418.00000000', '4. close': '60083.17000000', '5. volume': '431.91218967'}, {'1. open': '62755.47000000', '2. high': '63046.18000000', '3. low': '61693.41000000', '4. close': '62068.17000000', '5. volume': '386.33059160'}, {'1. open': '60985.08000000', '2. high': '63134.63000000', '3. low': '60520.08000000', '4. close': '62755.47000000', '5. volume': '428.47715681'}, {'1. open': '61073.94000000', '2. high': '61721.09000000', '3. low': '60349.25000000', '4. close': '60953.54000000', '5. volume': '274.07021648'}, {'1. open': '59992.83000000', '2. high': '61521.37000000', '3. low': '59310.61000000', '4. close': '61075.19000000', '5. volume': '334.26412593'}, {'1. open': '59713.79000000', '2. high': '61521.37000000', '3. low': '56170.00000000', '4. close': '59991.74000000', '5. volume': '918.21029260'}, {'1. open': '57485.45000000', '2. high': '60263.09000000', '3. low': '56917.92000000', '4. close': '59694.80000000', '5. volume': '565.62223035'}, {'1. open': '60115.61000000', '2. high': '60782.51000000', '3. low': '56129.00000000', '4. close': '57485.91000000', '5. volume': '987.19193770'}, {'1. open': '59769.15000000', '2. high': '60682.79000000', '3. low': '58042.43000000', '4. close': '60117.86000000', '5. volume': '762.53253187'}, {'1. open': '61870.64000000', '2. high': '62869.53000000', '3. low': '58717.47000000', '4. close': '59747.50000000', '5. volume': '1009.51373592'}, {'1. open': '61005.18000000', '2. high': '62043.20000000', '3. low': '59021.70000000', '4. close': '61867.60000000', '5. volume': '760.75833798'}, {'1. open': '63330.19000000', '2. high': '64027.39000000', '3. low': '57251.00000000', '4. close': '61007.42000000', '5. volume': '1559.92725418'}, {'1. open': '65281.03000000', '2. high': '66588.53000000', '3. low': '61300.00000000', '4. close': '63328.63000000', '5. volume': '984.35762317'}, {'1. open': '65773.86000000', '2. high': '66369.00000000', '3. low': '64846.83000000', '4. close': '65278.57000000', '5. volume': '383.48269915'}, {'1. open': '63684.68000000', '2. high': '66222.00000000', '3. low': '62636.91000000', '4. close': '65761.16000000', '5. volume': '444.69384970'}, {'1. open': '65894.26000000', '2. high': '66017.61000000', '3. low': '62881.66000000', '4. close': '63710.86000000', '5. volume': '508.61027479'}, {'1. open': '64046.80000000', '2. high': '67094.17000000', '3. low': '63770.22000000', '4. close': '65914.02000000', '5. volume': '566.93663259'}, {'1. open': '63618.34000000', '2. high': '64849.98000000', '3. low': '63525.00000000', '4. close': '64048.85000000', '5. volume': '219.83451485'}, {'1. open': '62640.42000000', '2. high': '64318.25000000', '3. low': '62283.52000000', '4. close': '63613.31000000', '5. volume': '181.73176199'}, {'1. open': '63272.08000000', '2. high': '63464.37000000', '3. low': '61077.30000000', '4. close': '62635.57000000', '5. volume': '457.05471445'}, {'1. open': '60962.64000000', '2. high': '63919.68000000', '3. low': '60089.91000000', '4. close': '63262.09000000', '5. volume': '453.02671506'}, {'1. open': '60868.59000000', '2. high': '61980.00000000', '3. low': '60000.00000000', '4. close': '60944.89000000', '5. volume': '512.74702544'}, {'1. open': '64905.24000000', '2. high': '64940.52000000', '3. low': '60011.64000000', '4. close': '60853.75000000', '5. volume': '1100.29393570'}, {'1. open': '66093.32000000', '2. high': '66101.60000000', '3. low': '63422.75000000', '4. close': '64909.14000000', '5. volume': '374.26140114'}, {'1. open': '64582.40000000', '2. high': '66193.77000000', '3. low': '64551.58000000', '4. close': '66099.78000000', '5. volume': '191.18973257'}, {'1. open': '64800.06000000', '2. high': '65187.26000000', '3. low': '64531.79000000', '4. close': '64567.04000000', '5. volume': '128.52661545'}, {'1. open': '65609.90000000', '2. high': '65784.03000000', '3. low': '64000.00000000', '4. close': '64804.53000000', '5. volume': '258.96970172'}, {'1. open': '64228.88000000', '2. high': '66265.73000000', '3. low': '63636.15000000', '4. close': '65616.11000000', '5. volume': '372.85584733'}, {'1. open': '64592.34000000', '2. high': '66339.18000000', '3. low': '63188.14000000', '4. close': '64187.14000000', '5. volume': '723.56355016'}, {'1. open': '64440.88000000', '2. high': '65971.86000000', '3. low': '63893.68000000', '4. close': '64578.78000000', '5. volume': '462.38197499'}, {'1. open': '62194.73000000', '2. high': '65689.95000000', '3. low': '61391.43000000', '4. close': '64434.81000000', '5. volume': '786.75081788'}, {'1. open': '59269.04000000', '2. high': '62558.63000000', '3. low': '59058.70000000', '4. close': '62197.99000000', '5. volume': '354.37775492'}, {'1. open': '59121.32000000', '2. high': '61146.40000000', '3. low': '58377.10000000', '4. close': '59270.27000000', '5. volume': '308.58241208'}, {'1. open': '60314.15000000', '2. high': '61527.63000000', '3. low': '57723.98000000', '4. close': '59092.76000000', '5. volume': '778.44754940'}, {'1. open': '62098.75000000', '2. high': '62450.49000000', '3. low': '59452.00000000', '4. close': '60311.85000000', '5. volume': '792.84807062'}, {'1. open': '57002.05000000', '2. high': '62418.00000000', '3. low': '55904.42000000', '4. close': '62104.86000000', '5. volume': '1320.89865175'}, {'1. open': '62150.25000000', '2. high': '62630.24000000', '3. low': '56617.26000000', '4. close': '56992.73000000', '5. volume': '1643.82438685'}, {'1. open': '62809.05000000', '2. high': '63323.33000000', '3. low': '61175.19000000', '4. close': '62167.44000000', '5. volume': '686.58471196'}, {'1. open': '59904.58000000', '2. high': '63275.93000000', '3. low': '59147.18000000', '4. close': '62799.62000000', '5. volume': '848.16305427'}, {'1. open': '63883.90000000', '2. high': '64377.91000000', '3. low': '59470.50000000', '4. close': '59904.06000000', '5. volume': '703.58792108'}, {'1. open': '65632.21000000', '2. high': '66573.90000000', '3. low': '60178.01000000', '4. close': '63873.31000000', '5. volume': '1580.24010395'}, {'1. open': '66773.00000000', '2. high': '67467.86000000', '3. low': '62942.02000000', '4. close': '65631.65000000', '5. volume': '1111.71424647'}, {'1. open': '65411.00000000', '2. high': '67437.05000000', '3. low': '65279.06000000', '4. close': '66772.14000000', '5. volume': '703.39502589'}, {'1. open': '65704.92000000', '2. high': '66943.61000000', '3. low': '62952.84000000', '4. close': '65424.91000000', '5. volume': '1000.88978827'}, {'1. open': '63085.63000000', '2. high': '66498.88000000', '3. low': '61332.21000000', '4. close': '65689.34000000', '5. volume': '1005.81234192'}, {'1. open': '62549.27000000', '2. high': '63945.01000000', '3. low': '62334.78000000', '4. close': '63064.25000000', '5. volume': '414.00852885'}, {'1. open': '62414.69000000', '2. high': '62799.99000000', '3. low': '62168.35000000', '4. close': '62553.18000000', '5. volume': '217.19679871'}, {'1. open': '61043.74000000', '2. high': '64012.00000000', '3. low': '60410.00000000', '4. close': '62408.25000000', '5. volume': '764.59105869'}, {'1. open': '60635.54000000', '2. high': '62320.76000000', '3. low': '60133.38000000', '4. close': '61027.58000000', '5. volume': '594.19610322'}, {'1. open': '58825.04000000', '2. high': '62297.53000000', '3. low': '57910.32000000', '4. close': '60629.09000000', '5. volume': '823.94642282'}, {'1. open': '62779.25000000', '2. high': '63600.00000000', '3. low': '48500.00000000', '4. close': '58821.41000000', '5. volume': '2198.24205859'}, {'1. open': '58226.26000000', '2. high': '62960.00000000', '3. low': '57439.00000000', '4. close': '62779.25000000', '5. volume': '1449.33430287'}, {'1. open': '57224.17000000', '2. high': '58328.78000000', '3. low': '56628.55000000', '4. close': '58227.89000000', '5. volume': '309.28890918'}, {'1. open': '57565.56000000', '2. high': '57627.13000000', '3. low': '56837.19000000', '4. close': '57211.22000000', '5. volume': '261.24992375'}, {'1. open': '56509.14000000', '2. high': '58300.00000000', '3. low': '56106.21000000', '4. close': '57572.75000000', '5. volume': '649.14989943'}, {'1. open': '57472.17000000', '2. high': '58640.40000000', '3. low': '55753.70000000', '4. close': '56477.23000000', '5. volume': '1065.49802200'}, {'1. open': '52600.59000000', '2. high': '59097.20000000', '3. low': '52309.80000000', '4. close': '57460.80000000', '5. volume': '1487.12728374'}, {'1. open': '49978.02000000', '2. high': '53063.37000000', '3. low': '49978.02000000', '4. close': '52587.36000000', '5. volume': '853.57840760'}, {'1. open': '47818.86000000', '2. high': '50487.20000000', '3. low': '46932.18000000', '4. close': '49979.74000000', '5. volume': '644.68082895'}, {'1. open': '47632.45000000', '2. high': '47979.99000000', '3. low': '47395.70000000', '4. close': '47803.84000000', '5. volume': '180.67373572'}, {'1. open': '46883.14000000', '2. high': '47757.48000000', '3. low': '46732.47000000', '4. close': '47632.45000000', '5. volume': '160.86393428'}, {'1. open': '47370.10000000', '2. high': '47589.09000000', '3. low': '46677.17000000', '4. close': '46873.10000000', '5. volume': '347.17726725'}, {'1. open': '47938.77000000', '2. high': '48117.60000000', '3. low': '47024.34000000', '4. close': '47359.21000000', '5. volume': '427.09798542'}, {'1. open': '48366.57000000', '2. high': '48474.07000000', '3. low': '46878.06000000', '4. close': '47938.43000000', '5. volume': '444.01980631'}, {'1. open': '48063.67000000', '2. high': '49025.00000000', '3. low': '46986.06000000', '4. close': '48364.32000000', '5. volume': '611.91851120'}, {'1. open': '48369.79000000', '2. high': '48706.16000000', '3. low': '47962.97000000', '4. close': '48056.14000000', '5. volume': '432.82093359'}, {'1. open': '47964.57000000', '2. high': '48642.33000000', '3. low': '47519.42000000', '4. close': '48354.00000000', '5. volume': '237.63179525'}, {'1. open': '48410.66000000', '2. high': '48458.09000000', '3. low': '46957.52000000', '4. close': '47959.18000000', '5. volume': '415.62814737'}, {'1. open': '48198.71000000', '2. high': '48950.00000000', '3. low': '47910.97000000', '4. close': '48405.14000000', '5. volume': '509.08758712'}, {'1. open': '48299.88000000', '2. high': '49068.78000000', '3. low': '47672.54000000', '4. close': '48194.01000000', '5. volume': '838.80532868'}, {'1. open': '46440.04000000', '2. high': '48599.40000000', '3. low': '45995.64000000', '4. close': '48300.68000000', '5. volume': '821.35946398'}, {'1. open': '46210.36000000', '2. high': '46661.94000000', '3. low': '45085.10000000', '4. close': '46434.80000000', '5. volume': '640.41108637'}, {'1. open': '44569.87000000', '2. high': '46692.70000000', '3. low': '44133.62000000', '4. close': '46210.59000000', '5. volume': '857.91750188'}, {'1. open': '44034.81000000', '2. high': '44653.24000000', '3. low': '43868.85000000', '4. close': '44552.73000000', '5. volume': '315.15008875'}, {'1. open': '43140.66000000', '2. high': '44400.00000000', '3. low': '42794.10000000', '4. close': '44027.18000000', '5. volume': '406.57912918'}, {'1. open': '42025.18000000', '2. high': '44598.99000000', '3. low': '41992.67000000', '4. close': '43124.72000000', '5. volume': '823.95348518'}, {'1. open': '41133.90000000', '2. high': '42308.54000000', '3. low': '41118.48000000', '4. close': '42012.50000000', '5. volume': '454.20625106'}, {'1. open': '40070.02000000', '2. high': '41199.11000000', '3. low': '39752.84000000', '4. close': '41128.84000000', '5. volume': '375.21982565'}, {'1. open': '39736.46000000', '2. high': '40355.00000000', '3. low': '39591.11000000', '4. close': '40065.55000000', '5. volume': '255.90086225'}, {'1. open': '39502.82000000', '2. high': '40486.42000000', '3. low': '39209.28000000', '4. close': '39731.54000000', '5. volume': '318.08445442'}, {'1. open': '39864.34000000', '2. high': '39950.68000000', '3. low': '39154.49000000', '4. close': '39489.16000000', '5. volume': '168.68083541'}, {'1. open': '40010.18000000', '2. high': '40162.50000000', '3. low': '39739.32000000', '4. close': '39852.73000000', '5. volume': '109.33799414'}, {'1. open': '39628.64000000', '2. high': '40227.41000000', '3. low': '39328.63000000', '4. close': '40012.45000000', '5. volume': '283.62426731'}, {'1. open': '39403.42000000', '2. high': '39827.92000000', '3. low': '38738.07000000', '4. close': '39621.22000000', '5. volume': '413.92845202'}, {'1. open': '39634.04000000', '2. high': '40351.30000000', '3. low': '39100.00000000', '4. close': '39410.58000000', '5. volume': '542.94919408'}, {'1. open': '39979.26000000', '2. high': '40497.75000000', '3. low': '39383.70000000', '4. close': '39633.57000000', '5. volume': '448.09912086'}, {'1. open': '38774.95000000', '2. high': '40077.97000000', '3. low': '38620.34000000', '4. close': '39975.64000000', '5. volume': '468.82367944'}, {'1. open': '38848.94000000', '2. high': '39477.03000000', '3. low': '38384.28000000', '4. close': '38770.89000000', '5. volume': '251.82164524'}, {'1. open': '38562.43000000', '2. high': '38928.11000000', '3. low': '38155.34000000', '4. close': '38857.09000000', '5. volume': '183.88120856'}, {'1. open': '36853.93000000', '2. high': '38931.47000000', '3. low': '36740.98000000', '4. close': '38562.87000000', '5. volume': '553.72491938'}, {'1. open': '36845.20000000', '2. high': '37024.19000000', '3. low': '36448.01000000', '4. close': '36855.82000000', '5. volume': '350.07687683'}, {'1. open': '36770.49000000', '2. high': '37236.52000000', '3. low': '36254.69000000', '4. close': '36845.18000000', '5. volume': '669.25689196'}, {'1. open': '36345.19000000', '2. high': '36858.04000000', '3. low': '35455.56000000', '4. close': '36765.32000000', '5. volume': '844.33204031'}, {'1. open': '38174.41000000', '2. high': '38236.90000000', '3. low': '36138.33000000', '4. close': '36345.19000000', '5. volume': '876.86122511'}, {'1. open': '38280.44000000', '2. high': '38456.48000000', '3. low': '38108.08000000', '4. close': '38178.38000000', '5. volume': '103.40935595'}, {'1. open': '38239.70000000', '2. high': '38443.43000000', '3. low': '38058.69000000', '4. close': '38283.48000000', '5. volume': '150.32543212'}, {'1. open': '37998.87000000', '2. high': '38723.22000000', '3. low': '37000.00000000', '4. close': '38236.24000000', '5. volume': '536.44714448'}, {'1. open': '39276.18000000', '2. high': '39407.88000000', '3. low': '37379.64000000', '4. close': '37988.66000000', '5. volume': '718.50309977'}, {'1. open': '39675.59000000', '2. high': '39728.85000000', '3. low': '38849.96000000', '4. close': '39272.26000000', '5. volume': '405.98425404'}, {'1. open': '38854.04000000', '2. high': '40087.58000000', '3. low': '38700.00000000', '4. close': '39685.64000000', '5. volume': '386.11020222'}, {'1. open': '38125.62000000', '2. high': '39626.61000000', '3. low': '38111.56000000', '4. close': '38856.28000000', '5. volume': '565.02130209'}, {'1. open': '39205.76000000', '2. high': '39384.66000000', '3. low': '38107.26000000', '4. close': '38127.68000000', '5. volume': '382.57458439'}, {'1. open': '39150.25000000', '2. high': '39645.59000000', '3. low': '38799.22000000', '4. close': '39207.07000000', '5. volume': '468.52145693'}, {'1. open': '42225.03000000', '2. high': '42403.63000000', '3. low': '37900.00000000', '4. close': '39146.18000000', '5. volume': '1471.06363884'}, {'1. open': '42588.61000000', '2. high': '44887.99000000', '3. low': '41588.00000000', '4. close': '42227.19000000', '5. volume': '1639.99550313'}, {'1. open': '42250.10000000', '2. high': '43571.51000000', '3. low': '40529.96000000', '4. close': '42576.83000000', '5. volume': '1436.15821126'}, {'1. open': '42922.51000000', '2. high': '43977.39000000', '3. low': '41017.14000000', '4. close': '42224.79000000', '5. volume': '1282.63926189'}, {'1. open': '40212.32000000', '2. high': '43178.99000000', '3. low': '39534.33000000', '4. close': '42919.85000000', '5. volume': '1133.08295922'}, {'1. open': '40259.98000000', '2. high': '40730.42000000', '3. low': '39933.18000000', '4. close': '40205.88000000', '5. volume': '410.42694407'}, {'1. open': '40426.56000000', '2. high': '40484.98000000', '3. low': '39750.00000000', '4. close': '40264.14000000', '5. volume': '237.16452420'}, {'1. open': '40419.60000000', '2. high': '40675.80000000', '3. low': '38811.00000000', '4. close': '40424.44000000', '5. volume': '1089.89064410'}, {'1. open': '39325.86000000', '2. high': '40949.76000000', '3. low': '38999.00000000', '4. close': '40412.75000000', '5. volume': '874.41827334'}, {'1. open': '41126.14000000', '2. high': '41614.69000000', '3. low': '37350.00000000', '4. close': '39332.21000000', '5. volume': '1482.34923329'}, {'1. open': '40091.39000000', '2. high': '41912.00000000', '3. low': '40054.41000000', '4. close': '41120.45000000', '5. volume': '989.26435722'}, {'1. open': '38418.87000000', '2. high': '40100.00000000', '3. low': '38294.70000000', '4. close': '40090.38000000', '5. volume': '365.72069768'}, {'1. open': '38280.09000000', '2. high': '38948.18000000', '3. low': '38111.66000000', '4. close': '38427.75000000', '5. volume': '378.74827432'}, {'1. open': '38216.07000000', '2. high': '38682.77000000', '3. low': '37689.55000000', '4. close': '38278.51000000', '5. volume': '517.29575131'}, {'1. open': '38498.12000000', '2. high': '39067.83000000', '3. low': '37544.31000000', '4. close': '38217.53000000', '5. volume': '673.86755739'}, {'1. open': '39156.55000000', '2. high': '39449.76000000', '3. low': '38158.41000000', '4. close': '38495.00000000', '5. volume': '647.48504211'}, {'1. open': '38533.41000000', '2. high': '39386.00000000', '3. low': '38200.00000000', '4. close': '39151.63000000', '5. volume': '569.19493673'}, {'1. open': '39583.10000000', '2. high': '39599.75000000', '3. low': '37755.39000000', '4. close': '38530.92000000', '5. volume': '509.61012104'}, {'1. open': '39195.20000000', '2. high': '39867.88000000', '3. low': '38952.89000000', '4. close': '39582.57000000', '5. volume': '284.21827708'}, {'1. open': '39810.33000000', '2. high': '40005.45000000', '3. low': '38814.95000000', '4. close': '39172.06000000', '5. volume': '426.16169940'}, {'1. open': '39998.13000000', '2. high': '40014.22000000', '3. low': '39395.00000000', '4. close': '39813.94000000', '5. volume': '201.46719337'}, {'1. open': '39877.59000000', '2. high': '40390.00000000', '3. low': '39405.23000000', '4. close': '39996.65000000', '5. volume': '499.54311232'}, {'1. open': '39898.26000000', '2. high': '40264.94000000', '3. low': '39474.31000000', '4. close': '39875.32000000', '5. volume': '672.40043398'}]\n['1. Information', '2. Digital Currency Code', '3. Digital Currency Name', '4. Market Code', '5. Market Name', '6. Last Refreshed', '7. Time Zone', '2024-12-04', '2024-12-03', '2024-12-02', '2024-12-01', '2024-11-30', '2024-11-29', '2024-11-28', '2024-11-27', '2024-11-26', '2024-11-25', '2024-11-24', '2024-11-23', '2024-11-22', '2024-11-21', '2024-11-20', '2024-11-19', '2024-11-18', '2024-11-17', '2024-11-16', '2024-11-15', '2024-11-14', '2024-11-13', '2024-11-12', '2024-11-11', '2024-11-10', '2024-11-09', '2024-11-08', '2024-11-07', '2024-11-06', '2024-11-05', '2024-11-04', '2024-11-03', '2024-11-02', '2024-11-01', '2024-10-31', '2024-10-30', '2024-10-29', '2024-10-28', '2024-10-27', '2024-10-26', '2024-10-25', '2024-10-24', '2024-10-23', '2024-10-22', '2024-10-21', '2024-10-20', '2024-10-19', '2024-10-18', '2024-10-17', '2024-10-16', '2024-10-15', '2024-10-14', '2024-10-13', '2024-10-12', '2024-10-11', '2024-10-10', '2024-10-09', '2024-10-08', '2024-10-07', '2024-10-06', '2024-10-05', '2024-10-04', '2024-10-03', '2024-10-02', '2024-10-01', '2024-09-30', '2024-09-29', '2024-09-28', '2024-09-27', '2024-09-26', '2024-09-25', '2024-09-24', '2024-09-23', '2024-09-22', '2024-09-21', '2024-09-20', '2024-09-19', '2024-09-18', '2024-09-17', '2024-09-16', '2024-09-15', '2024-09-14', '2024-09-13', '2024-09-12', '2024-09-11', '2024-09-10', '2024-09-09', '2024-09-08', '2024-09-07', '2024-09-06', '2024-09-05', '2024-09-04', '2024-09-03', '2024-09-02', '2024-09-01', '2024-08-31', '2024-08-30', '2024-08-29', '2024-08-28', '2024-08-27', '2024-08-26', '2024-08-25', '2024-08-24', '2024-08-23', '2024-08-22', '2024-08-21', '2024-08-20', '2024-08-19', '2024-08-18', '2024-08-17', '2024-08-16', '2024-08-15', '2024-08-14', '2024-08-13', '2024-08-12', '2024-08-11', '2024-08-10', '2024-08-09', '2024-08-08', '2024-08-07', '2024-08-06', '2024-08-05', '2024-08-04', '2024-08-03', '2024-08-02', '2024-08-01', '2024-07-31', '2024-07-30', '2024-07-29', '2024-07-28', '2024-07-27', '2024-07-26', '2024-07-25', '2024-07-24', '2024-07-23', '2024-07-22', '2024-07-21', '2024-07-20', '2024-07-19', '2024-07-18', '2024-07-17', '2024-07-16', '2024-07-15', '2024-07-14', '2024-07-13', '2024-07-12', '2024-07-11', '2024-07-10', '2024-07-09', '2024-07-08', '2024-07-07', '2024-07-06', '2024-07-05', '2024-07-04', '2024-07-03', '2024-07-02', '2024-07-01', '2024-06-30', '2024-06-29', '2024-06-28', '2024-06-27', '2024-06-26', '2024-06-25', '2024-06-24', '2024-06-23', '2024-06-22', '2024-06-21', '2024-06-20', '2024-06-19', '2024-06-18', '2024-06-17', '2024-06-16', '2024-06-15', '2024-06-14', '2024-06-13', '2024-06-12', '2024-06-11', '2024-06-10', '2024-06-09', '2024-06-08', '2024-06-07', '2024-06-06', '2024-06-05', '2024-06-04', '2024-06-03', '2024-06-02', '2024-06-01', '2024-05-31', '2024-05-30', '2024-05-29', '2024-05-28', '2024-05-27', '2024-05-26', '2024-05-25', '2024-05-24', '2024-05-23', '2024-05-22', '2024-05-21', '2024-05-20', '2024-05-19', '2024-05-18', '2024-05-17', '2024-05-16', '2024-05-15', '2024-05-14', '2024-05-13', '2024-05-12', '2024-05-11', '2024-05-10', '2024-05-09', '2024-05-08', '2024-05-07', '2024-05-06', '2024-05-05', '2024-05-04', '2024-05-03', '2024-05-02', '2024-05-01', '2024-04-30', '2024-04-29', '2024-04-28', '2024-04-27', '2024-04-26', '2024-04-25', '2024-04-24', '2024-04-23', '2024-04-22', '2024-04-21', '2024-04-20', '2024-04-19', '2024-04-18', '2024-04-17', '2024-04-16', '2024-04-15', '2024-04-14', '2024-04-13', '2024-04-12', '2024-04-11', '2024-04-10', '2024-04-09', '2024-04-08', '2024-04-07', '2024-04-06', '2024-04-05', '2024-04-04', '2024-04-03', '2024-04-02', '2024-04-01', '2024-03-31', '2024-03-30', '2024-03-29', '2024-03-28', '2024-03-27', '2024-03-26', '2024-03-25', '2024-03-24', '2024-03-23', '2024-03-22', '2024-03-21', '2024-03-20', '2024-03-19', '2024-03-18', '2024-03-17', '2024-03-16', '2024-03-15', '2024-03-14', '2024-03-13', '2024-03-12', '2024-03-11', '2024-03-10', '2024-03-09', '2024-03-08', '2024-03-07', '2024-03-06', '2024-03-05', '2024-03-04', '2024-03-03', '2024-03-02', '2024-03-01', '2024-02-29', '2024-02-28', '2024-02-27', '2024-02-26', '2024-02-25', '2024-02-24', '2024-02-23', '2024-02-22', '2024-02-21', '2024-02-20', '2024-02-19', '2024-02-18', '2024-02-17', '2024-02-16', '2024-02-15', '2024-02-14', '2024-02-13', '2024-02-12', '2024-02-11', '2024-02-10', '2024-02-09', '2024-02-08', '2024-02-07', '2024-02-06', '2024-02-05', '2024-02-04', '2024-02-03', '2024-02-02', '2024-02-01', '2024-01-31', '2024-01-30', '2024-01-29', '2024-01-28', '2024-01-27', '2024-01-26', '2024-01-25', '2024-01-24', '2024-01-23', '2024-01-22', '2024-01-21', '2024-01-20', '2024-01-19', '2024-01-18', '2024-01-17', '2024-01-16', '2024-01-15', '2024-01-14', '2024-01-13', '2024-01-12', '2024-01-11', '2024-01-10', '2024-01-09', '2024-01-08', '2024-01-07', '2024-01-06', '2024-01-05', '2024-01-04', '2024-01-03', '2024-01-02', '2024-01-01', '2023-12-31', '2023-12-30', '2023-12-29', '2023-12-28', '2023-12-27', '2023-12-26', '2023-12-25', '2023-12-24', '2023-12-23', '2023-12-22', '2023-12-21']\n\n\n\nprint(pd.json_normalize(daily_dat[6]))\n\nNotImplementedError: \n\n\n\nclean_btc_data = pd.DataFrame()\nfor j in range(0,len(daily_dat)):\n    if j &lt;5:\n        continue\n\n    date = pd.DataFrame([custom_col[j]])\n    data = pd.json_normalize(daily_dat[j])\n\n    full = pd.concat([date,data], axis = 1)\n    clean_btc_data = pd.concat([clean_btc_data, full], axis = 0)\n\n\n\n\n\n\nNotImplementedError: \n\n\n\nclean_btc_data.shape\nclean_btc_data.head()\n\n\n\n\n\n\n\n\n0\n1. open\n2. high\n3. low\n4. close\n5. volume\n\n\n\n\n0\n2024-12-03\n227.2400\n229.1100\n226.6700\n229.0000\n3163815\n\n\n0\n2024-12-02\n227.5000\n228.3800\n225.5100\n227.3900\n2656181\n\n\n0\n2024-11-29\n227.7500\n230.3600\n227.1900\n227.4100\n2640253\n\n\n0\n2024-11-27\n228.8300\n229.1900\n224.2700\n226.9200\n2995121\n\n\n0\n2024-11-26\n226.7300\n228.9800\n225.5115\n228.8300\n4449543"
  },
  {
    "objectID": "nlp.html",
    "href": "nlp.html",
    "title": "NLP",
    "section": "",
    "text": "NLP\n\nOverview\nAs discussed in the\n\nNLP code for this workbook\n\n\nSelecting Comments\nFor NLP we used data from all three subreddits (investing, cryptocurrency, and finance), and focused on comment data only. The distribution of comments between the Subreddits was very uneven with finance having 25,291 comments, investing 516,497, and CryptoCurrency 3,360,896. For our initial visualizations and building the pipeline, we sampled around .1% of the data, to conserve computational resources. We then evaluated the distribution of comment lengths, and found that most comments were relatively short, with less than 100 characters.\n\n\n\nPercentile\nComment Length (characters)\n\n\n\n\n10%\n12\n\n\n10%\n15\n\n\n20%\n27\n\n\n30%\n40\n\n\n40%\n51\n\n\n50%\n63\n\n\n60%\n79\n\n\n70%\n118\n\n\n80%\n176\n\n\n90%\n332\n\n\n\nWe also looked at the distribution of comments over time, and found that there was a substantial difference in the number of comments across the subreddits month-to-month (i.e. September 2023 had 6x the comments of July 2024).\nIn terms of other exploratory work, we looked at the TDF-IDF scores of the documents, and found words such as ‘bull’ and ‘bear’ to have large importance, as well as several slang words such as ‘apes’ that are favored by users of these forums. Overall, however, the TDF-IDF analysis did not give us a clear direction to focus on for our predictive work.\n\n\nSentiment Pipeline\nTo clean and process the data, we: 1. Turn all strings to lowercase 2. Use universal sentence encoder to create embeddings of our sentences 3. Get sentiment scores from the pretrained twitter DL model.\nWe also tested several other components we hope to fit into the pipeline: 1. A Spark NLP lemmatizer 2. A Spark NLP stopword remover 3. A Spark NLP sentence detector\nWe tested two other sentiment models built into Spark NLP, but found they had a large class-imbalance. A basic sentiment encoder using a dictionary of words returned more than 95% positive outputs, which did not seem to match the makeup of the data. Based on these concerns, we preferred the Twitter DL sentiment model, which returned a more modest 50-70% positive outputs depending on the data used.\n\n\nIdentifying Subjects\nWe used RegEx matching to identify several central topics within investing, and whether they were present in each comment. As a proxy for equity markets, we looked for strings similar to “S&P 500,” representing the largest equity index, and the many shorthands used for the index online. As a proxy for cryptocurrency markets, we looked for strings and shorthands similar to “bitcoin” and “ethereum.” Finally, we looked for strings referring to the Federal Reserve, or its chair Jerome Powell.\nNext, we sought to understand whether any of these topics would correspond to differences in sentiment. We also found that Cryptoccurency was by far the most common topic, which made sense as this was also the largest subreddit we studied. Accounting for the imbalance, we looked at the share of comments mentioning each topic that espoused positive or negative sentiments. We found that comments which mentioned cryptocurrency or the S&P had a higher share of positive comments (~70% vs. 54%).\n\n\n\nComment sentiment vs. mentions of key topics\n\n\n\n\nSentiment over time\nNow that we have positive/negative/neutral sentiment calculated for the comments in our dataset, we can compare the proportion of comments which fell under each of these sentiments. We see that the proportion in each category has remained relatively constant, except for April 2024, when positive comments surged to around 70% of all comments, and negative comments dropped to around 10% of all comments, from close to 60% and 20% in the long run average, respectively.\n\n\n\nChange in sentiment over time\n\n\n\n\nConclusion - Input to ML\nBased on the work of our EDA and NLP components, we have found that there are certainly mentions of financial topics within the r/cryptocurrency, r/finance, and r/investing. subreddits. We also found reason to believe that cryptocurrency, in particular, might benefit from a more positive sentiment on Reddit than other financial instruments. Following this result, we next turned to evaluate whether this positive sentiment might translate into movements in Bitcoin prices via our ML models."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Link to full code:\nlink\nlink\n\n\nThis page presents an exploratory data analysis (EDA) of a CryptoCurrency and Investing subreddit data, focusing on data cleaning and exploring user activity in relation to market data. The analysis begins by ensuring the dataset’s integrity, followed by exploring the content, such as popular posts and comments, and their relation stock and security prices.\nBy combining Reddit discussions with market data, this part of the analysis aims to uncover patterns between user engagement and market movements, offering insights into how online conversations might influence or reflect investing trends.\n\n\n\nThe data used for the analysis consists of posts and comments from the CryptoCurrency subreddit, sourced from a subset of the Reddit Archive data. The comments dataset has 17 columns, with the most important being author, body, score, created_utc (timestamp), and parent_id for comment threads. The submissions dataset contains 21 columns, with key ones being author, created_utc, score, title, num_comments, and selftext. Both datasets rely on created_utc for analyzing timing and user activity patterns.\nIn order to check the quality of the data, a number of quality checks and cleaning processes were performed. First, for the submissions/posts data, null values were counted across all columns, both for numeric and non-numeric fields. It showed that there were not any nulls in columns of significance that would invalidate that post for any reason. Then rows with titles shorter than three characters were filtered out, to remove posts without substance. The reason the limit was set to three characters is that some of the cryptocurrency and stock symbols are three charcters long, and we wanted to avoid removing any such mentions. Additionally, the number of rows and columns was checked, and rows with the title “[removed]” were counted, but none were present. The final size of the posts data after cleaning came out to 205,391x21. For the comments data, similar processes were conducted, but with a focus on the body of the comment. For the comments, there were some that had a body of “[removed]”, and those were removed from the dataset. The final size of the comments data after cleaning came out to 3,667,315x17.\n\n\n\n\n\n\n\nUsing PySpark, the text of post titles and comment bodies was first converted to lowercase, split into individual words, and processed for relevance. A custom stop words list, which included both standard English stop words and context-specific terms like “get,” “need,” and “&gt;”, was used to filter out irrelevant words. Each title and comment was split into an array of words, and stop words were removed with PySpark’s StopWordsRemover function. The remaining words were then “exploded” to create a separate row for each word, facilitating an accurate count of each term’s frequency. This word count data was then sorted in descending order to display the most common terms.\n\n\n\n\n\nThe analysis of word frequencies highlights the CryptoCurrency subreddit community’s main interests. In both post titles and comments, terms like “crypto,” “bitcoin,” and “btc” dominate, showing a strong focus on general cryptocurrency discussions and key assets. Titles include terms like “binance,” “wallet,” “exchange,” and “trading,” indicating practical topics like trading tools, platforms, and investment strategies, along with “price,” “market,” and “buy,” which reflect a keen interest in market trends and guidance.\nIn comments, terms such as “like,” “people,” and “think” reflect an interactive community, where opinions, sentiments, and advice are frequently shared. Unique terms like “moons” suggest the culture of crypto discussions and meme references within the community. Overall, these findings reveal the subreddit’s focus on major cryptocurrencies, trading tools, and community-driven discussions around trends and market activities.\n\n\n\n\n\n\nSince word counts for posts and comments were already done in the process of the previous goal, the mentions of the top cryptocurrencies can easily be extrated from the general word counts. 12 of the most popular cryptocurrencies according to multiple online sources were looked, and the results can be seen below.\n\n\nUnsuprisingly, Bitcoin leads discussions on the CryptoCurrency subreddit by a large margin, with the highest mention counts in both posts and comments, signaling it as the most popular topic. Ethereum and Binance also rank high, reflecting strong interest. Solana, XRP, and Cardano have moderate mentions, suggesting growing community attention, while Polygon, Dogecoin, and Tether appear less frequently, showing niche interest. Overall, Bitcoin, Ethereum, and Binance dominate, while emerging tokens like Solana indicate shifting trends.\n\n\n\n\nUsing PySpark, the data from the CryptoCurrencies subreddit, including post timestamps and comment counts, was first merged with cryptocurrency price data from external sources. Two sets of price data was used, daily Bitcoin prices over time which were sourced from github, and the S&P Cryptocurrency Broad Digital Market Index, is intended to monitor the performance of digital assets listed on reputable open digital exchanges and was sourced from S&P Global website. The timestamps of posts and comments were then matched with the corresponding cryptocurrency prices at the time of posting. Data preprocessing included converting timestamps to a common format and handling missing or outlier values in both price and comment data. After aligning the data, the variables were plotted to see how they matched up against each other over time and if any trends could be noticed.\n\n\nAs can be seen in the graphs, suprisingly, at least in the time frame of the given data, there does not seem to be an increase in activity on the CryptoCurrency sub reddit when the price of Bitcoin and CryptoCurrencies in general are higher. The S&P index almost mirrors the price of Bitcoin, which makes sense as Bitcoin is by far the biggest cryptocurrency and makes up the majority of the crypto market. But the most activity on the subreddit occured when prices for Bitcoin and Cryptocurrencies were at their lowest. And as the prices increased, activity decreased. The reason for this decrease could be related to increased prices, as people may be more likely to post and engage in the subreddit when prices are down due to worries or potential opportunity to invest and make gains, but there are also likely other factors in play, such as news or new regulations. Further analysis would need to be conducted to say what drives increased acticity on the sub reddit, but it can at least be said that there does not seem to be a positive correlation between sub reddit activity and crypto prices.\n\n\n\n\n\n\n\n\nWords like “money,” “market,” “years,” and “invest” suggest that users are primarily discussing financial concepts, market conditions, and long-term investment strategies. The presence of words such as “think” and “get” might indicate that many comments involve opinions, advice, or inquiries about market trends. Frequent words like “please” and “question” imply that users may seek guidance, support, or clarification from others in the community. Understanding the frequent terms can help financial service providers tailor their content to address the most common concerns of the subreddit community, such as providing guidance on market strategies or long-term investing.\n\n\n\n\n\n\nWords like “money,” “year,” “post,” and “question” stand out as highly discussed topics. This indicates that users are focused on financial matters and often seek information or clarification. Terms such as “contact,” “invest,” “market,” and “action” suggest users are actively discussing investment actions, market conditions, and seeking advice. Words like “please” and “would” show a polite and inquisitive tone, suggesting that many comments are requests or inquiries rather than statements. This visualization helps content creators and financial service providers to focus on popular investment topics, tailoring educational resources and FAQs to address these specific areas of interest within the investing community.\n\n\n\n\n\n\nWe can observe periods of increased comment volume, possibly correlating with events like quarterly earnings reports, major economic announcements, or high market volatility. Peaks in comment volume could indicate heightened interest or concern, potentially tied to external factors impacting the stock market (e.g., inflation reports, geopolitical events). A sustained increase or decrease in comment volume could suggest changes in investor engagement or interest over time, which may be useful for predicting shifts in market sentiment. Financial analysts and investment firms can use this data to monitor when investors are most engaged, providing an early indicator of market sentiment and potential volatility. Recognizing these spikes can help them align their outreach efforts with periods of high investor activity.\n\n\n\n\n\n\nWe can observe variations in engagement levels across different hours of the day and weeks of the year. Certain weeks display heightened engagement during specific hours, possibly correlating with scheduled market events, such as major financial disclosures, earnings reports, or global economic announcements. Peaks during specific hours of the day could reflect global market overlaps or times when critical news is released, suggesting high investor activity. This temporal pattern could provide insights into investor behavior, helping financial institutions identify optimal times for market interventions or communications. Monitoring these engagement trends might also aid in forecasting potential shifts in market sentiment or identifying high-risk periods of volatility. Recognizing these hourly and weekly peaks allows analysts to better time their analyses, strategies, and outreach efforts to coincide with the most engaged audience.\n\n\n\n\n\n\n\nStocks with high mentions but varying prices show that price alone does not drive popularity. For example, highly discussed low-priced stocks may indicate interest in speculative or trending assets, while high-priced, frequently mentioned stocks likely represent established companies. Stocks with larger bubbles have the highest number of mentions, indicating strong interest or high relevance within the subreddit community. Stocks with both high prices and high mention volumes might be subject to more scrutiny and could exhibit higher volatility, as many investors track and discuss them closely. Understanding which stocks are most discussed can help analysts gauge retail investor sentiment and identify stocks that may be susceptible to price movements due to public interest. This data can inform investment strategies, risk management, and the focus of research reports.\n\n\n\n\n\nbtc prices: https://github.com/Habrador/Bitcoin-price-visualization/blob/main/Bitcoin-price-USD.csv\ns&p crypto index: https://www.spglobal.com/spdji/en/indices/digital-assets/sp-cryptocurrency-broad-digital-market-index/#overview\nstock prices : https://finance.yahoo.com"
  },
  {
    "objectID": "eda.html#introduction",
    "href": "eda.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This page presents an exploratory data analysis (EDA) of a CryptoCurrency and Investing subreddit data, focusing on data cleaning and exploring user activity in relation to market data. The analysis begins by ensuring the dataset’s integrity, followed by exploring the content, such as popular posts and comments, and their relation stock and security prices.\nBy combining Reddit discussions with market data, this part of the analysis aims to uncover patterns between user engagement and market movements, offering insights into how online conversations might influence or reflect investing trends."
  },
  {
    "objectID": "eda.html#data-overview-and-quality-check",
    "href": "eda.html#data-overview-and-quality-check",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The data used for the analysis consists of posts and comments from the CryptoCurrency subreddit, sourced from a subset of the Reddit Archive data. The comments dataset has 17 columns, with the most important being author, body, score, created_utc (timestamp), and parent_id for comment threads. The submissions dataset contains 21 columns, with key ones being author, created_utc, score, title, num_comments, and selftext. Both datasets rely on created_utc for analyzing timing and user activity patterns.\nIn order to check the quality of the data, a number of quality checks and cleaning processes were performed. First, for the submissions/posts data, null values were counted across all columns, both for numeric and non-numeric fields. It showed that there were not any nulls in columns of significance that would invalidate that post for any reason. Then rows with titles shorter than three characters were filtered out, to remove posts without substance. The reason the limit was set to three characters is that some of the cryptocurrency and stock symbols are three charcters long, and we wanted to avoid removing any such mentions. Additionally, the number of rows and columns was checked, and rows with the title “[removed]” were counted, but none were present. The final size of the posts data after cleaning came out to 205,391x21. For the comments data, similar processes were conducted, but with a focus on the body of the comment. For the comments, there were some that had a body of “[removed]”, and those were removed from the dataset. The final size of the comments data after cleaning came out to 3,667,315x17."
  },
  {
    "objectID": "eda.html#cryptocurrency-subreddit-analysis",
    "href": "eda.html#cryptocurrency-subreddit-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Using PySpark, the text of post titles and comment bodies was first converted to lowercase, split into individual words, and processed for relevance. A custom stop words list, which included both standard English stop words and context-specific terms like “get,” “need,” and “&gt;”, was used to filter out irrelevant words. Each title and comment was split into an array of words, and stop words were removed with PySpark’s StopWordsRemover function. The remaining words were then “exploded” to create a separate row for each word, facilitating an accurate count of each term’s frequency. This word count data was then sorted in descending order to display the most common terms.\n\n\n\n\n\nThe analysis of word frequencies highlights the CryptoCurrency subreddit community’s main interests. In both post titles and comments, terms like “crypto,” “bitcoin,” and “btc” dominate, showing a strong focus on general cryptocurrency discussions and key assets. Titles include terms like “binance,” “wallet,” “exchange,” and “trading,” indicating practical topics like trading tools, platforms, and investment strategies, along with “price,” “market,” and “buy,” which reflect a keen interest in market trends and guidance.\nIn comments, terms such as “like,” “people,” and “think” reflect an interactive community, where opinions, sentiments, and advice are frequently shared. Unique terms like “moons” suggest the culture of crypto discussions and meme references within the community. Overall, these findings reveal the subreddit’s focus on major cryptocurrencies, trading tools, and community-driven discussions around trends and market activities.\n\n\n\n\n\n\nSince word counts for posts and comments were already done in the process of the previous goal, the mentions of the top cryptocurrencies can easily be extrated from the general word counts. 12 of the most popular cryptocurrencies according to multiple online sources were looked, and the results can be seen below.\n\n\nUnsuprisingly, Bitcoin leads discussions on the CryptoCurrency subreddit by a large margin, with the highest mention counts in both posts and comments, signaling it as the most popular topic. Ethereum and Binance also rank high, reflecting strong interest. Solana, XRP, and Cardano have moderate mentions, suggesting growing community attention, while Polygon, Dogecoin, and Tether appear less frequently, showing niche interest. Overall, Bitcoin, Ethereum, and Binance dominate, while emerging tokens like Solana indicate shifting trends.\n\n\n\n\nUsing PySpark, the data from the CryptoCurrencies subreddit, including post timestamps and comment counts, was first merged with cryptocurrency price data from external sources. Two sets of price data was used, daily Bitcoin prices over time which were sourced from github, and the S&P Cryptocurrency Broad Digital Market Index, is intended to monitor the performance of digital assets listed on reputable open digital exchanges and was sourced from S&P Global website. The timestamps of posts and comments were then matched with the corresponding cryptocurrency prices at the time of posting. Data preprocessing included converting timestamps to a common format and handling missing or outlier values in both price and comment data. After aligning the data, the variables were plotted to see how they matched up against each other over time and if any trends could be noticed.\n\n\nAs can be seen in the graphs, suprisingly, at least in the time frame of the given data, there does not seem to be an increase in activity on the CryptoCurrency sub reddit when the price of Bitcoin and CryptoCurrencies in general are higher. The S&P index almost mirrors the price of Bitcoin, which makes sense as Bitcoin is by far the biggest cryptocurrency and makes up the majority of the crypto market. But the most activity on the subreddit occured when prices for Bitcoin and Cryptocurrencies were at their lowest. And as the prices increased, activity decreased. The reason for this decrease could be related to increased prices, as people may be more likely to post and engage in the subreddit when prices are down due to worries or potential opportunity to invest and make gains, but there are also likely other factors in play, such as news or new regulations. Further analysis would need to be conducted to say what drives increased acticity on the sub reddit, but it can at least be said that there does not seem to be a positive correlation between sub reddit activity and crypto prices."
  },
  {
    "objectID": "eda.html#investing-subreddit-analysis",
    "href": "eda.html#investing-subreddit-analysis",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Words like “money,” “market,” “years,” and “invest” suggest that users are primarily discussing financial concepts, market conditions, and long-term investment strategies. The presence of words such as “think” and “get” might indicate that many comments involve opinions, advice, or inquiries about market trends. Frequent words like “please” and “question” imply that users may seek guidance, support, or clarification from others in the community. Understanding the frequent terms can help financial service providers tailor their content to address the most common concerns of the subreddit community, such as providing guidance on market strategies or long-term investing.\n\n\n\n\n\n\nWords like “money,” “year,” “post,” and “question” stand out as highly discussed topics. This indicates that users are focused on financial matters and often seek information or clarification. Terms such as “contact,” “invest,” “market,” and “action” suggest users are actively discussing investment actions, market conditions, and seeking advice. Words like “please” and “would” show a polite and inquisitive tone, suggesting that many comments are requests or inquiries rather than statements. This visualization helps content creators and financial service providers to focus on popular investment topics, tailoring educational resources and FAQs to address these specific areas of interest within the investing community.\n\n\n\n\n\n\nWe can observe periods of increased comment volume, possibly correlating with events like quarterly earnings reports, major economic announcements, or high market volatility. Peaks in comment volume could indicate heightened interest or concern, potentially tied to external factors impacting the stock market (e.g., inflation reports, geopolitical events). A sustained increase or decrease in comment volume could suggest changes in investor engagement or interest over time, which may be useful for predicting shifts in market sentiment. Financial analysts and investment firms can use this data to monitor when investors are most engaged, providing an early indicator of market sentiment and potential volatility. Recognizing these spikes can help them align their outreach efforts with periods of high investor activity.\n\n\n\n\n\n\nWe can observe variations in engagement levels across different hours of the day and weeks of the year. Certain weeks display heightened engagement during specific hours, possibly correlating with scheduled market events, such as major financial disclosures, earnings reports, or global economic announcements. Peaks during specific hours of the day could reflect global market overlaps or times when critical news is released, suggesting high investor activity. This temporal pattern could provide insights into investor behavior, helping financial institutions identify optimal times for market interventions or communications. Monitoring these engagement trends might also aid in forecasting potential shifts in market sentiment or identifying high-risk periods of volatility. Recognizing these hourly and weekly peaks allows analysts to better time their analyses, strategies, and outreach efforts to coincide with the most engaged audience.\n\n\n\n\n\n\n\nStocks with high mentions but varying prices show that price alone does not drive popularity. For example, highly discussed low-priced stocks may indicate interest in speculative or trending assets, while high-priced, frequently mentioned stocks likely represent established companies. Stocks with larger bubbles have the highest number of mentions, indicating strong interest or high relevance within the subreddit community. Stocks with both high prices and high mention volumes might be subject to more scrutiny and could exhibit higher volatility, as many investors track and discuss them closely. Understanding which stocks are most discussed can help analysts gauge retail investor sentiment and identify stocks that may be susceptible to price movements due to public interest. This data can inform investment strategies, risk management, and the focus of research reports."
  },
  {
    "objectID": "eda.html#external-data-sources",
    "href": "eda.html#external-data-sources",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "btc prices: https://github.com/Habrador/Bitcoin-price-visualization/blob/main/Bitcoin-price-USD.csv\ns&p crypto index: https://www.spglobal.com/spdji/en/indices/digital-assets/sp-cryptocurrency-broad-digital-market-index/#overview\nstock prices : https://finance.yahoo.com"
  },
  {
    "objectID": "ml_copy.html",
    "href": "ml_copy.html",
    "title": "Join External and Comment Data",
    "section": "",
    "text": "# Setup - Run only once per Kernel App\n%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n\n# install PySpark\n%pip install pyspark==3.4.0\n\n# restart kernel\nfrom IPython.core.display import HTML\nHTML(\"&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;\")\n\n\nDownloading and Extracting Packages:\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - conda-forge/openjdk/11.0.1/download/linux-64::openjdk==11.0.1=hacce0ff_1021\n\n\nThe following NEW packages will be INSTALLED:\n\n  openjdk            conda-forge/openjdk/11.0.1/download/linux-64::openjdk-11.0.1-hacce0ff_1021 \n\n\n\nDownloading and Extracting Packages:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\nCollecting pyspark==3.4.0\n  Using cached pyspark-3.4.0-py2.py3-none-any.whl\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.0\nNote: you may need to restart the kernel to use updated packages.\nimport os\nimport sagemaker\nfrom pyspark.sql import functions as f\nsess = sagemaker.Session()\nbucket = sess.default_bucket()\n\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName(\"PySparkApp\")\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n    .config(\n        \"fs.s3a.aws.credentials.provider\",\n        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nprint(spark.version)\n\nWarning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\nIvy Default Cache set to: /home/sagemaker-user/.ivy2/cache\nThe jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\norg.apache.hadoop#hadoop-aws added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-d0b47657-4fe9-4512-b7ee-3f56686220c8;1.0\n    confs: [default]\n    found org.apache.hadoop#hadoop-aws;3.2.2 in central\n    found com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n:: resolution report :: resolve 230ms :: artifacts dl 7ms\n    :: modules in use:\n    com.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n    org.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-d0b47657-4fe9-4512-b7ee-3f56686220c8\n    confs: [default]\n    0 artifacts copied, 2 already retrieved (0kB/7ms)\n24/12/07 06:45:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n:: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n3.4.0"
  },
  {
    "objectID": "ml_copy.html#pipeline-in-spark",
    "href": "ml_copy.html#pipeline-in-spark",
    "title": "Join External and Comment Data",
    "section": "Pipeline in Spark",
    "text": "Pipeline in Spark\n\nfrom pyspark.sql.types import FloatType\n\n#comments_in = spark.read.options(header='True', inferSchema='True', delimiter=',').csv(\"./NLP_output_for_ml.csv\")\nfull_spark_raw = spark.createDataFrame(full_data).withColumn('Volatility', f.col('Volatility').cast(FloatType()) )\n\n\nfull_spark_raw.show()\n\n[Stage 3:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+\n|Unnamed: 0|year|month|day|total|postotal|negtotal|neutotal|snpcomments|fedcomments|cryptocomments| Volatility|Increase|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+\n|         0|2024|    3| 19|    8|       6|       2|       0|          0|          0|             3| 0.11569654|       1|\n|         1|2023|    9|  4|   24|      11|      11|       2|          1|          1|             6|0.010366165|       0|\n|         2|2023|    6| 29|   10|       5|       5|       0|          0|          0|             0|0.051271606|       1|\n|         3|2023|    8| 10|   21|      14|       7|       0|          0|          0|             1|0.009294065|       0|\n|         4|2023|    7|  4|    6|       1|       3|       2|          0|          1|             0|0.020875178|       0|\n|         5|2023|    8| 14|   30|      26|       3|       1|          0|          0|             3|0.012009502|       0|\n|         6|2023|    7|  2|   10|       4|       6|       0|          0|          0|             1|0.025981873|       1|\n|         7|2023|    7| 20|   30|      15|      14|       1|          0|          0|             4|0.010704339|       1|\n|         8|2023|    8| 31|   22|      14|       5|       3|          0|          0|             4| 0.03018702|       0|\n|         9|2023|    7| 22|   23|      12|       8|       3|          0|          0|             1|0.020185372|       1|\n|        10|2023|    8| 22|   25|      13|      11|       1|          0|          0|             4| 0.03821135|       1|\n|        11|2023|    7|  5|    8|       3|       5|       0|          0|          1|             1|0.052260034|       0|\n|        12|2023|    9| 16|   23|      14|       7|       2|          1|          0|             6|0.007744146|       0|\n|        13|2024|    7|  5|    6|       2|       4|       0|          0|          0|             3| 0.04299435|       1|\n|        14|2023|    8| 11|   25|      15|       9|       1|          0|          0|             4|0.003582499|       1|\n|        15|2023|    9| 23|   18|      12|       5|       1|          0|          0|             6|0.021272434|       0|\n|        16|2023|    9| 27|   19|      11|       6|       2|          0|          0|             2|0.035808284|       1|\n|        17|2023|    9| 11|   25|      15|      10|       0|          1|          0|             4|0.053209834|       1|\n|        18|2023|    8|  9|   35|      22|      12|       1|          0|          0|             6|0.011559267|       0|\n|        19|2023|    7| 26|   25|      15|       8|       2|          0|          0|             4|0.016175574|       0|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+\nonly showing top 20 rows\n\n\n\n\nfull_spark_raw.printSchema()\n\nroot\n |-- Unnamed: 0: long (nullable = true)\n |-- year: long (nullable = true)\n |-- month: long (nullable = true)\n |-- day: long (nullable = true)\n |-- total: long (nullable = true)\n |-- postotal: long (nullable = true)\n |-- negtotal: long (nullable = true)\n |-- neutotal: long (nullable = true)\n |-- snpcomments: long (nullable = true)\n |-- fedcomments: long (nullable = true)\n |-- cryptocomments: long (nullable = true)\n |-- Volatility: float (nullable = true)\n |-- Increase: long (nullable = true)\n\n\n\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n# select only the predictor columns we want in our model\nvectorAssembler_features = VectorAssembler(inputCols = ['postotal','negtotal','neutotal','snpcomments','fedcomments','cryptocomments'], outputCol = 'features')\nvectorAssembler_volatility = VectorAssembler(inputCols = ['Volatility'], outputCol = 'vol')\n#vectorAssembler_increase = VectorAssembler(inputCols = ['Increase'], outputCol = 'inc')\n\n\nfrom pyspark.ml.feature import Normalizer\n\n# normalize data \nnormalizer = Normalizer(inputCol = 'features', outputCol = 'features_norm')\n#normalizerVol = Normalizer(inputCol = 'vol', outputCol = 'vol_norm', p = 1.0)\nnormalizerInc = Normalizer(inputCol = 'inc', outputCol = 'inc_norm')\n\n\nfrom pyspark.ml import Pipeline\n\n\npipeline = Pipeline(stages = [vectorAssembler_features, vectorAssembler_volatility, normalizer])\n#  vectorAssembler_increase, normalizerVol, normalizerInc\n#data_model = pipeline.fit(full_spark_raw)\n#pipelined_data = data_model.transform(full_spark_raw)\n\n\npipelined_data.show()\n\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n|Unnamed: 0|year|month|day|total|postotal|negtotal|neutotal|snpcomments|fedcomments|cryptocomments| Volatility|Increase|            features|                 vol|       features_norm|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n|         0|2024|    3| 19|    8|       6|       2|       0|          0|          0|             3| 0.11569654|       1|[6.0,2.0,0.0,0.0,...|[0.11569654196500...|[0.85714285714285...|\n|         1|2023|    9|  4|   24|      11|      11|       2|          1|          1|             6|0.010366165|       0|[11.0,11.0,2.0,1....|[0.01036616507917...|[0.65272991200661...|\n|         2|2023|    6| 29|   10|       5|       5|       0|          0|          0|             0|0.051271606|       1| (6,[0,1],[5.0,5.0])|[0.05127160623669...|(6,[0,1],[0.70710...|\n|         3|2023|    8| 10|   21|      14|       7|       0|          0|          0|             1|0.009294065|       0|[14.0,7.0,0.0,0.0...|[0.00929406471550...|[0.89260739828867...|\n|         4|2023|    7|  4|    6|       1|       3|       2|          0|          1|             0|0.020875178|       0|[1.0,3.0,2.0,0.0,...|[0.02087517827749...|[0.25819888974716...|\n|         5|2023|    8| 14|   30|      26|       3|       1|          0|          0|             3|0.012009502|       0|[26.0,3.0,1.0,0.0...|[0.01200950238853...|[0.98623621435414...|\n|         6|2023|    7|  2|   10|       4|       6|       0|          0|          0|             1|0.025981873|       1|[4.0,6.0,0.0,0.0,...|[0.02598187327384...|[0.54944225579475...|\n|         7|2023|    7| 20|   30|      15|      14|       1|          0|          0|             4|0.010704339|       1|[15.0,14.0,1.0,0....|[0.01070433855056...|[0.71672772385124...|\n|         8|2023|    8| 31|   22|      14|       5|       3|          0|          0|             4| 0.03018702|       0|[14.0,5.0,3.0,0.0...|[0.03018702007830...|[0.89260739828867...|\n|         9|2023|    7| 22|   23|      12|       8|       3|          0|          0|             1|0.020185372|       1|[12.0,8.0,3.0,0.0...|[0.02018537186086...|[0.81274255377431...|\n|        10|2023|    8| 22|   25|      13|      11|       1|          0|          0|             4| 0.03821135|       1|[13.0,11.0,1.0,0....|[0.03821134939789...|[0.74194918919595...|\n|        11|2023|    7|  5|    8|       3|       5|       0|          0|          1|             1|0.052260034|       0|[3.0,5.0,0.0,0.0,...|[0.05226003378629...|[0.5,0.8333333333...|\n|        12|2023|    9| 16|   23|      14|       7|       2|          1|          0|             6|0.007744146|       0|[14.0,7.0,2.0,1.0...|[0.00774414604529...|[0.82783735438471...|\n|        13|2024|    7|  5|    6|       2|       4|       0|          0|          0|             3| 0.04299435|       1|[2.0,4.0,0.0,0.0,...|[0.04299435019493...|[0.37139067635410...|\n|        14|2023|    8| 11|   25|      15|       9|       1|          0|          0|             4|0.003582499|       1|[15.0,9.0,1.0,0.0...|[0.00358249898999...|[0.83462232611198...|\n|        15|2023|    9| 23|   18|      12|       5|       1|          0|          0|             6|0.021272434|       0|[12.0,5.0,1.0,0.0...|[0.02127243392169...|[0.83607961714994...|\n|        16|2023|    9| 27|   19|      11|       6|       2|          0|          0|             2|0.035808284|       1|[11.0,6.0,2.0,0.0...|[0.03580828383564...|[0.85634883857767...|\n|        17|2023|    9| 11|   25|      15|      10|       0|          1|          0|             4|0.053209834|       1|[15.0,10.0,0.0,1....|[0.05320983380079...|[0.81110710565381...|\n|        18|2023|    8|  9|   35|      22|      12|       1|          0|          0|             6|0.011559267|       0|[22.0,12.0,1.0,0....|[0.01155926659703...|[0.85312340776242...|\n|        19|2023|    7| 26|   25|      15|       8|       2|          0|          0|             4|0.016175574|       0|[15.0,8.0,2.0,0.0...|[0.01617557369172...|[0.85332018598286...|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "ml_copy.html#building-train-test-val-datasets",
    "href": "ml_copy.html#building-train-test-val-datasets",
    "title": "Join External and Comment Data",
    "section": "Building train-test-val datasets",
    "text": "Building train-test-val datasets\n\ntrain_data, test_data, val_data = full_spark_raw.randomSplit([0.7, 0.2, 0.1], 42)\n\n\nprint('train size: ', train_data.count()) \nprint('test size: ', test_data.count())\nprint('val size: ', val_data.count())\n\n                                                                                                                                                                [Stage 10:==============&gt;                                           (1 + 3) / 4]                                                                                \n\n\ntrain size:  290\ntest size:  98\nval size:  36\n\n\n\nFit pipeline on train data to avoid information leak\n\n\npipeline = Pipeline(stages = [vectorAssembler_features, vectorAssembler_volatility,  normalizer])\n# normalizerVol, normalizerInc, vectorAssembler_increase,\nfitted_pipeline = pipeline.fit(train_data)\ntrain_data = fitted_pipeline.transform(train_data)\ntest_data = fitted_pipeline.transform(test_data)\nval_data = fitted_pipeline.transform(val_data)\n\n\nval_data.show()\n\n[Stage 16:&gt;                                                         (0 + 3) / 3]                                                                                \n\n\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n|Unnamed: 0|year|month|day|total|postotal|negtotal|neutotal|snpcomments|fedcomments|cryptocomments| Volatility|Increase|            features|                 vol|       features_norm|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\n|         6|2023|    7|  2|   10|       4|       6|       0|          0|          0|             1|0.025981873|       1|[4.0,6.0,0.0,0.0,...|[0.02598187327384...|[0.54944225579475...|\n|         8|2023|    8| 31|   22|      14|       5|       3|          0|          0|             4| 0.03018702|       0|[14.0,5.0,3.0,0.0...|[0.03018702007830...|[0.89260739828867...|\n|        47|2023|    6| 22|   10|       4|       6|       0|          0|          0|             3|0.052619252|       1|[4.0,6.0,0.0,0.0,...|[0.05261925235390...|[0.51214751973158...|\n|        55|2023|    9| 19|   19|      10|       9|       0|          0|          0|             3|0.019361097|       0|[10.0,9.0,0.0,0.0...|[0.01936109736561...|[0.72547625011001...|\n|        77|2023|    9|  5|   25|      15|       9|       1|          1|          0|             6| 0.02352453|       0|[15.0,9.0,1.0,1.0...|[0.02352453023195...|[0.80874579902578...|\n|       100|2023|    9| 22|   22|      14|       7|       1|          0|          0|             5|0.004465478|       0|[14.0,7.0,1.0,0.0...|[0.00446547800675...|[0.85043943492310...|\n|       120|2023|    6|  9|   21|      11|      10|       0|          0|          0|             5| 0.03991042|       0|[11.0,10.0,0.0,0....|[0.03991042077541...|[0.70133438436967...|\n|       127|2023|    9| 18|   24|      16|       6|       2|          0|          0|             5|0.030214135|       1|[16.0,6.0,2.0,0.0...|[0.03021413460373...|[0.89303291549751...|\n|       161|2023|    7| 13|   13|       8|       3|       2|          0|          0|             1|0.052702334|       0|[8.0,3.0,2.0,0.0,...|[0.05270233377814...|[0.90582162731567...|\n|       162|2023|    7|  8|    5|       2|       3|       0|          0|          0|             2|0.011674282|       0|[2.0,3.0,0.0,0.0,...|[0.01167428214102...|[0.48507125007266...|\n|       165|2023|    6| 11|   11|       5|       6|       0|          0|          0|             1|0.017189978|       0|[5.0,6.0,0.0,0.0,...|[0.01718997769057...|[0.63500063500095...|\n|       166|2023|    8| 20|   16|       9|       7|       0|          0|          0|             4|0.014130916|       0|[9.0,7.0,0.0,0.0,...|[0.01413091644644...|[0.74484529974213...|\n|       174|2023|    9|  6|   27|      17|       9|       1|          0|          1|             4| 0.03071667|       1|[17.0,9.0,1.0,0.0...|[0.03071667067706...|[0.86304424036357...|\n|       191|2024|    3| 25|    7|       5|       2|       0|          0|          0|             0|0.030392166|       1| (6,[0,1],[5.0,2.0])|[0.03039216622710...|(6,[0,1],[0.92847...|\n|       198|2023|   10| 22|    4|       1|       2|       1|          0|          0|             0| 0.14955209|       1|[1.0,2.0,1.0,0.0,...|[0.1495520919561386]|[0.40824829046386...|\n|       221|2023|   12| 30|    4|       2|       2|       0|          0|          0|             3|0.021532169|       1|[2.0,2.0,0.0,0.0,...|[0.02153216861188...|[0.48507125007266...|\n|       222|2024|    4| 23|    4|       3|       1|       0|          0|          0|             2|0.052000377|       0|[3.0,1.0,0.0,0.0,...|[0.05200037732720...|[0.80178372573727...|\n|       223|2023|   11| 26|    5|       3|       1|       1|          0|          0|             2| 0.02167586|       0|[3.0,1.0,1.0,0.0,...|[0.02167586050927...|[0.77459666924148...|\n|       240|2024|    2| 22|    5|       4|       1|       0|          0|          0|             0|0.019385979|       0| (6,[0,1],[4.0,1.0])|[0.01938597857952...|(6,[0,1],[0.97014...|\n|       245|2024|    4| 14|    4|       2|       2|       0|          0|          0|             0|0.067364216|       0| (6,[0,1],[2.0,2.0])|[0.06736421585083...|(6,[0,1],[0.70710...|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+\nonly showing top 20 rows"
  },
  {
    "objectID": "ml_copy.html#build-classification-models-logistic-regression-gbt",
    "href": "ml_copy.html#build-classification-models-logistic-regression-gbt",
    "title": "Join External and Comment Data",
    "section": "Build Classification Models (logistic regression, GBT)",
    "text": "Build Classification Models (logistic regression, GBT)\n\nLR\n\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, featuresCol='features_norm',labelCol = 'Increase')\nlrModel = lr.fit(train_data)\n\n\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))\n\n\n\n                                                                                \n\n\nCoefficients: (6,[],[])\nIntercept: 0.1937068790190799\n\n\n\ndef eval_results_binary(df, model, real = 'Increase', pred = 'prediction'):\n    preds = model.transform(df)\n    evaluator = MulticlassClassificationEvaluator(labelCol=real, predictionCol=pred, metricName=\"accuracy\")\n    accuracy = evaluator.evaluate(preds)\n\n    print(\"Accuracy = %g\" % accuracy)\n    print(\"Train Error = %g\" % (1.0 - accuracy))\n\n\n\n\nlr = LogisticRegression(maxIter=10, regParam=0.5, elasticNetParam=0.1, featuresCol='features_norm',labelCol = 'Increase')\nlrModel = lr.fit(train_data)\n\neval_results_binary(train_data, lrModel)\neval_results_binary(test_data, lrModel)\n\n                                                                                [Stage 5001:============================&gt;                           (2 + 2) / 4]                                                                                \n\n\nAccuracy = 0.548276\nTrain Error = 0.451724\nAccuracy = 0.438776\nTrain Error = 0.561224\n\n\n\n\nLR with Hyperparameter Search\n\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit, CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n\n\n# fix label column name\ntrain_data_cv = train_data.withColumn('label', f.col('Increase'))\n\nparamGrid = ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.3,0.2, 0.1, 0.01]) \\\n    .addGrid(lr.fitIntercept, [False, True])\\\n    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n    .build()\n\n# devalre model\nlr_cv = LogisticRegression(maxIter=10, regParam=0.5, elasticNetParam=0.1, featuresCol='features_norm',labelCol = 'Increase')\n\ncrossval = CrossValidator(estimator=lr_cv,\n                      estimatorParamMaps=paramGrid,\n                      evaluator=RegressionEvaluator(),\n                      numFolds=2)  \n\n\n\n\"\"\"# In this case the estimator is simply the linear regression.\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\ntvs = TrainValidationSplit(estimator=lr,\n                           estimatorParamMaps=paramGrid,\n                           evaluator=RegressionEvaluator(),\n                           # 80% of the data will be used for training, 20% for validation.\n                           trainRatio=0.8)\"\"\"\n\n'# In this case the estimator is simply the linear regression.\\n# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\\ntvs = TrainValidationSplit(estimator=lr,\\n                           estimatorParamMaps=paramGrid,\\n                           evaluator=RegressionEvaluator(),\\n                           # 80% of the data will be used for training, 20% for validation.\\n                           trainRatio=0.8)'\n\n\n\ntuned_lr_model = crossval.fit(train_data_cv)\n\n                                                                                \n\n\n\n#tuned_lr_model.explainParam('fitIntercept')\neval_results_binary(train_data, tuned_lr_model.bestModel)\neval_results_binary(test_data, tuned_lr_model.bestModel)\n\nprint(tuned_lr_model.bestModel.explainParam('regParam'))\nprint(tuned_lr_model.bestModel.explainParam('fitIntercept'))\nprint(tuned_lr_model.bestModel.explainParam('elasticNetParam'))\n\n                                                                                [Stage 4995:&gt;                                                       (0 + 4) / 4]                                                                                \n\n\nAccuracy = 0.548276\nTrain Error = 0.451724\nAccuracy = 0.438776\nTrain Error = 0.561224\nregParam: regularization parameter (&gt;= 0). (default: 0.0, current: 0.5)\nfitIntercept: whether to fit an intercept term. (default: True)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.1)\n\n\nbest params: reg 0.5, intercept fit = true, elastic net = 0.1.\n\n\nGBT without tuning\n\ngbt = GBTClassifier(labelCol=\"Increase\", featuresCol=\"features_norm\", maxIter=10)\ngbt_model = gbt.fit(train_data)\n\n\n                                                                                \n\n\n\neval_results_binary(train_data, gbt_model)\n\neval_results_binary(test_data, gbt_model)\n\n                                                                                                                                                                \n\n\nAccuracy = 0.827586\nTrain Error = 0.172414\nAccuracy = 0.438776\nTrain Error = 0.561224\n\n\nmassive overfitting\n\n\nGBT with tuning\n\ntest_data_cv = test_data.withColumn('label', f.col('Increase'))\n\n\ngbt = GBTClassifier(labelCol=\"Increase\", featuresCol=\"features_norm\", maxIter=10)\n\nparamGrid_gbt = ParamGridBuilder()\\\n    .addGrid(gbt.maxDepth, [5,4, 3, 2]) \\\n    .addGrid(gbt.minInstancesPerNode, [1, 2,3])\\\n    .addGrid(gbt.minInfoGain, [0.0, 0.05, 0.1])\\\n    .build()\n\n\ncrossval_gbt = CrossValidator(estimator=gbt,\n                      estimatorParamMaps=paramGrid_gbt,\n                      evaluator=BinaryClassificationEvaluator(),\n                      numFolds=2)  \n\n\ntuned_gbt_model = crossval_gbt.fit(train_data_cv)\n\n                                                                                \n\n\n\neval_results_binary(train_data, gbt_model)\neval_results_binary(test_data, gbt_model)\n\n\neval_results_binary(train_data, tuned_gbt_model.bestModel)\neval_results_binary(test_data, tuned_gbt_model.bestModel)\n\n                                                                                                                                                                                                                                                [Stage 4991:==========================================&gt;             (3 + 1) / 4]                                                                                \n\n\nAccuracy = 0.827586\nTrain Error = 0.172414\nAccuracy = 0.438776\nTrain Error = 0.561224\nAccuracy = 0.475862\nTrain Error = 0.524138\nAccuracy = 0.785714\nTrain Error = 0.214286\n\n\n\nprint(tuned_gbt_model.bestModel.explainParam('maxDepth'))\nprint(tuned_gbt_model.bestModel.explainParam('minInstancesPerNode'))\nprint(tuned_gbt_model.bestModel.explainParam('minInfoGain'))\n\nmaxDepth: Maximum depth of the tree. (&gt;= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 5)\nminInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be &gt;= 1. (default: 1, current: 2)\nminInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0, current: 0.0)\n\n\nbest results were depth 5, min instances 2, min info gain 0\n\n\nClassification Evaluation Metrics\n\nfrom pyspark.mllib.evaluation import BinaryClassificationMetrics\n\n\nclass CurveMetrics(BinaryClassificationMetrics):\n    def __init__(self, *args):\n        super(CurveMetrics, self).__init__(*args)\n\n    def _to_list(self, rdd):\n        points = []\n        # Note this collect could be inefficient for large datasets \n        # considering there may be one probability per datapoint (at most)\n        # The Scala version takes a numBins parameter, \n        # but it doesn't seem possible to pass this from Python to Java\n        for row in rdd.collect():\n            # Results are returned as type scala.Tuple2, \n            # which doesn't appear to have a py4j mapping\n            points += [(float(row._1()), float(row._2()))]\n        return points\n\n    def get_curve(self, method):\n        rdd = getattr(self._java_model, method)().toJavaRDD()\n        return self._to_list(rdd)\n\n\npredictions.show()\n\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n|Unnamed: 0|year|month|day|total|postotal|negtotal|neutotal|snpcomments|fedcomments|cryptocomments| Volatility|Increase|            features|                 vol|       features_norm|       rawPrediction|         probability|prediction|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n|         2|2023|    6| 29|   10|       5|       5|       0|          0|          0|             0|0.051271606|       1| (6,[0,1],[5.0,5.0])|[0.05127160623669...|(6,[0,1],[0.70710...|[0.04072378081366...|[0.52035064163719...|       0.0|\n|         9|2023|    7| 22|   23|      12|       8|       3|          0|          0|             1|0.020185372|       1|[12.0,8.0,3.0,0.0...|[0.02018537186086...|[0.81274255377431...|[-0.7361010373105...|[0.18660814018841...|       1.0|\n|        13|2024|    7|  5|    6|       2|       4|       0|          0|          0|             3| 0.04299435|       1|[2.0,4.0,0.0,0.0,...|[0.04299435019493...|[0.37139067635410...|[-0.1529128864276...|[0.42413394598506...|       1.0|\n|        14|2023|    8| 11|   25|      15|       9|       1|          0|          0|             4|0.003582499|       1|[15.0,9.0,1.0,0.0...|[0.00358249898999...|[0.83462232611198...|[-0.6753426955932...|[0.20575834131895...|       1.0|\n|        15|2023|    9| 23|   18|      12|       5|       1|          0|          0|             6|0.021272434|       0|[12.0,5.0,1.0,0.0...|[0.02127243392169...|[0.83607961714994...|[0.36261892673148...|[0.67375938363221...|       0.0|\n|        19|2023|    7| 26|   25|      15|       8|       2|          0|          0|             4|0.016175574|       0|[15.0,8.0,2.0,0.0...|[0.01617557369172...|[0.85332018598286...|[0.10030486553126...|[0.54998491126510...|       0.0|\n|        21|2023|    7| 28|   17|      12|       3|       2|          0|          0|             5|0.004884871|       1|[12.0,3.0,2.0,0.0...|[0.00488487118855...|[0.88949917999332...|[0.16334880911301...|[0.58095564020636...|       0.0|\n|        23|2023|    9| 20|   23|      10|      13|       0|          0|          0|             5|0.028508546|       0|[10.0,13.0,0.0,0....|[0.02850854583084...|[0.58321184351980...|[0.04050837598431...|[0.52024311670070...|       0.0|\n|        24|2023|    6|  3|   11|       5|       6|       0|          0|          0|             2| 0.01677447|       1|[5.0,6.0,0.0,0.0,...|[0.01677446998655...|[0.62017367294604...|[1.23295505714401...|[0.92171716920551...|       0.0|\n|        28|2023|    7| 21|   26|      12|      12|       2|          0|          1|             3|0.011058536|       0|[12.0,12.0,2.0,0....|[0.01105853635817...|[0.69052240517812...|[1.26820819127500...|[0.92665563822624...|       0.0|\n|        29|2023|   12|  8|   12|       9|       3|       0|          1|          0|             2|0.016837386|       0|[9.0,3.0,0.0,1.0,...|[0.01683738641440...|[0.92338051687663...|[0.52726781183063...|[0.74164491181732...|       0.0|\n|        30|2023|    7|  7|   13|       8|       5|       0|          0|          0|             1|0.010676579|       0|[8.0,5.0,0.0,0.0,...|[0.01067657861858...|[0.84327404271156...|[-0.0590892792101...|[0.47048969789841...|       1.0|\n|        32|2024|    3|  5|   13|      10|       3|       0|          0|          0|             3|0.074554786|       1|[10.0,3.0,0.0,0.0...|[0.07455478608608...|[0.92057461789832...|[-0.6682806879796...|[0.20807610777871...|       1.0|\n|        34|2023|   10| 19|    6|       4|       1|       1|          0|          0|             1|0.054999303|       1|[4.0,1.0,1.0,0.0,...|[0.05499930307269...|[0.91766293548224...|[-0.4394189538902...|[0.29341865083119...|       1.0|\n|        35|2023|    8|  3|   17|       6|      10|       1|          0|          0|             4|0.015187176|       0|[6.0,10.0,1.0,0.0...|[0.01518717594444...|[0.48507125007266...|[-0.9201703097821...|[0.13701101308056...|       1.0|\n|        39|2023|    8| 15|   27|      17|      10|       0|          0|          0|             6| 0.01827349|       0|[17.0,10.0,0.0,0....|[0.01827348954975...|[0.82462112512353...|[0.21149793044922...|[0.60419991034895...|       0.0|\n|        42|2023|    8|  8|   23|      15|       8|       0|          0|          0|             2|0.024177648|       0|[15.0,8.0,0.0,0.0...|[0.02417764812707...|[0.87630935675547...|[-0.0388693196488...|[0.48057512171456...|       1.0|\n|        43|2023|    8| 28|   21|      14|       4|       3|          0|          0|             4|0.081407145|       1|[14.0,4.0,3.0,0.0...|[0.08140714466571...|[0.90939772344628...|[-0.4394189538902...|[0.29341865083119...|       1.0|\n|        45|2023|    9| 25|   15|       6|       8|       1|          0|          0|             2|0.011511951|       0|[6.0,8.0,1.0,0.0,...|[0.01151195075362...|[0.58554004376911...|[-1.1215306613341...|[0.09594966373682...|       1.0|\n|        46|2023|    6| 19|    7|       5|       2|       0|          0|          0|             1| 0.06434874|       1|[5.0,2.0,0.0,0.0,...|[0.0643487423658371]|[0.91287092917527...|[0.35156128078979...|[0.66887972031075...|       0.0|\n+----------+----+-----+---+-----+--------+--------+--------+-----------+-----------+--------------+-----------+--------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Create a Pipeline estimator and fit on train DF, predict on test DF\n#model = gbt_model.fit(train_data)\npredictions = gbt_model.transform(test_data)\n\n# Returns as a list (false positive rate, true positive rate)\npreds = predictions.select('prediction','probability').rdd.map(lambda row: (float(row['probability'][1]), float(row['prediction'])))\npoints = CurveMetrics(preds).get_curve('roc')\nroc = CurveMetrics(preds).get_curve('roc')\n\nNameError: name 'CurveMetrics' is not defined\n\n\n\nfrom sklearn.metrics import roc_curve, RocCurveDisplay, auc\nfrom pyspark.ml.functions import vector_to_array\n\npredictions = gbt_model.transform(test_data)\n\ny_score = predictions.select(vector_to_array(\"probability\")[1]).rdd.keys().collect()\ny_true = predictions.select(\"Increase\").rdd.keys().collect()\nfpr, tpr, thresholds = roc_curve(y_true, y_score)\nroc_auc = auc(fpr, tpr)\n\n                                                                                \n\n\n\n\ndef ROC_curve(model, data):\n    preds = model.transform(data)\n    \n    y_score = preds.select(vector_to_array(\"probability\")[1]).rdd.keys().collect()\n    y_true = preds.select(\"Increase\").rdd.keys().collect()\n    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    return fpr, tpr, thresholds, roc_auc\n\n\nfpr_gbt, tpr_gbt, thresholds_gbt, roc_auc_gbt = ROC_curve(gbt_model, test_data)\nfpr_gbt_tuned, tpr_gbt_tuned, thresholds_gbt_tuned, roc_auc_gbt_tuned = ROC_curve(tuned_gbt_model.bestModel, val_data)\nfpr_lr, tpr_lr, thresholds_lr, roc_auc_lr = ROC_curve(lrModel, test_data)\nfpr_lr_tuned, tpr_lr_tuned, thresholds_lr_tuned, roc_auc_lr_tuned = ROC_curve(tuned_lr_model.bestModel, test_data)\n\n                                                                                \n\n\n\nplt.figure()  \nplt.plot(fpr_gbt, tpr_gbt, label='GBT (area = %0.2f)' % roc_auc_gbt)\nplt.plot(fpr_gbt_tuned, tpr_gbt_tuned, label='GBT Tuned (area = %0.2f)' % roc_auc_gbt_tuned)\n#plt.plot(fpr_lr, tpr_lr, thresholds_lr, label='Log Reg. (area = %0.2f)' % roc_auc_lr)\n#plt.plot(fpr_lr_tuned, tpr_lr_tuned, label='Log Reg. Tuned (area = %0.2f)' % roc_auc_lr_tuned)\n\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for GBT Models')\nplt.legend()\nplt.show()\n\n\n\n\n\nplt.figure()  \n\nplt.plot(fpr_lr, tpr_lr+.01, label='Logistic Reg. (area = %0.2f)' % roc_auc_lr)\n\nplt.plot(fpr_lr_tuned, tpr_lr_tuned-.01, label='Logistic Reg. Tuned (area = %0.2f)' % roc_auc_lr_tuned)\n\nplt.plot([0, 1], [0, 1], 'k--', label='No Skill')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Logistic Regression Models')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "ml_copy.html#build-regression-models",
    "href": "ml_copy.html#build-regression-models",
    "title": "Join External and Comment Data",
    "section": "Build Regression Models",
    "text": "Build Regression Models\n\nLinear Regression\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.regression import GBTRegressor\n\n\ntrain_data_cv = train_data.withColumn('label', f.col('Increase'))\n\nlr = LinearRegression(featuresCol=\"features_norm\", labelCol=\"Volatility\", predictionCol=\"predicted_vol\", regParam=0.1)\nlr_model = lr.fit(train_data)\npredictions = lr_model.transform(test_data)\n\n                                                                                \n\n\n\ndef eval_results_reg(model, testdf, labels = 'Volatility', predcol = 'predicted_vol'):\n\n    predictions = model.transform(testdf)\n    \n    evaluator = RegressionEvaluator(labelCol=\"Volatility\", predictionCol=\"predicted_vol\", metricName=\"rmse\")\n    rmse = evaluator.evaluate(predictions)\n    \n    evaluator_r2 = RegressionEvaluator(labelCol=\"Volatility\", predictionCol=\"predicted_vol\", metricName=\"r2\")\n    r2 = evaluator_r2.evaluate(predictions)\n\n    print('rmse: ', rmse) \n    print('r2: ', r2)\n\n\neval_results_reg(lr_model, train_data)\neval_results_reg(lr_model, test_data)\n\n\n                                                                                [Stage 13850:===========================&gt;                           (2 + 2) / 4]                                                                                \n\n\nrmse:  0.02220339615429509\nr2:  0.008431300143542875\nrmse:  0.02213137254223798\nr2:  0.0009947192877705069\n\n\n\nTuned Hyperparameters\n\nlr = LinearRegression(featuresCol=\"features_norm\", labelCol=\"Volatility\", predictionCol=\"predicted_vol\")\n\n\nparamGrid_linreg = ParamGridBuilder()\\\n    .addGrid(lr.regParam, [0.0,0.1,0.2]) \\\n    .addGrid(lr.elasticNetParam, [0.2, 0.4, 0.6, 0.8])\\\n    .build()\n\n\ncrossval_linreg = CrossValidator(estimator=lr,\n                      estimatorParamMaps=paramGrid_linreg,\n                      evaluator=RegressionEvaluator(labelCol=\"Volatility\", predictionCol=\"predicted_vol\", metricName=\"r2\"),\n                      numFolds=3)  \n\n\ntuned_linreg_model = crossval_linreg.fit(train_data)\n\n24/12/07 07:01:43 WARN Instrumentation: [96f402a2] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:45 WARN Instrumentation: [d4c8098b] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:45 WARN Instrumentation: [1f98621c] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:46 WARN Instrumentation: [ce5c9fe2] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:48 WARN Instrumentation: [0bb4ce86] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:50 WARN Instrumentation: [cd12cb60] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:51 WARN Instrumentation: [fa4dbcc2] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:51 WARN Instrumentation: [f5ee3758] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:53 WARN Instrumentation: [ccc08602] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:55 WARN Instrumentation: [d6462d1b] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:55 WARN Instrumentation: [e872d0fb] regParam is zero, which might cause numerical instability and overfitting.\n24/12/07 07:01:56 WARN Instrumentation: [013e0f0d] regParam is zero, which might cause numerical instability and overfitting.\n                                                                                \n\n\n\neval_results_reg(tuned_linreg_model, train_data)\neval_results_reg(tuned_linreg_model, test_data)\n\n                                                                                                                                                                                                                                                \n\n\nrmse:  0.022297593980785187\nr2:  3.3306690738754696e-16\nrmse:  0.02216353106124524\nr2:  -0.0019106469616598787\n\n\n\nprint(tuned_linreg_model.bestModel.explainParam('regParam'))\nprint(tuned_linreg_model.bestModel.explainParam('elasticNetParam'))\n\nregParam: regularization parameter (&gt;= 0). (default: 0.0, current: 0.2)\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.2)\n\n\n\nprint(lr_model.coefficients)\nprint(tuned_linreg_model.bestModel.intercept)\n\n[-0.0006648945929220134,-0.0006146639773429391,-0.000992980630497191,0.006169592546959767,0.000871323960578097,0.002031770773261513]\n0.03420036530507536\n\n\n\n\n\nGradient Boosted Regressor\n\ngbr = GBTRegressor(featuresCol=\"features_norm\", labelCol=\"Volatility\", predictionCol=\"predicted_vol\", maxIter=10)\n\ngbr_model = gbr.fit(train_data)\n\n                                                                                \n\n\n\neval_results_reg(gbr_model, train_data)\neval_results_reg(gbr_model, test_data)\n\n                                                                                [Stage 13858:&gt;                                                      (0 + 4) / 4]                                                                                \n\n\nrmse:  0.015124025795236173\nr2:  0.5399354050685081\nrmse:  0.02540514560521019\nr2:  -0.31641997320583726\n\n\n\ngbr = GBTRegressor(featuresCol=\"features_norm\", labelCol=\"Volatility\", predictionCol=\"predicted_vol\", maxIter=10)\n\n\nparamGrid_gbr = ParamGridBuilder()\\\n    .addGrid(gbr.maxDepth, [6, 5,4, 3, 2]) \\\n    .addGrid(gbr.minInstancesPerNode, [1, 2,3])\\\n    .addGrid(gbr.minInfoGain, [0.0, 0.05, 0.1])\\\n    .build()\n\n\ncrossval_gbr = CrossValidator(estimator=gbr,\n                      estimatorParamMaps=paramGrid_gbr,\n                      evaluator=RegressionEvaluator(labelCol=\"Volatility\", predictionCol=\"predicted_vol\", metricName=\"r2\"),\n                      numFolds=2)  \n\n\ntuned_gbr_model = crossval_gbr.fit(train_data)\n\n                                                                                \n\n\n\neval_results_reg(tuned_gbr_model.bestModel, train_data)\neval_results_reg(tuned_gbr_model.bestModel, test_data)\n\n                                                                                                                                                                                                                                                \n\n\nrmse:  0.022297593980785187\nr2:  3.3306690738754696e-16\nrmse:  0.02216353106124524\nr2:  -0.0019106469616601007\n\n\n\npredictions = gbr_model.transform(test_data)\npredictions2 = lr_model.transform(test_data)\n\nx_ax = range(0, predictions.count())\ny_pred_gbr = predictions.select(\"predicted_vol\").collect()\ny_pred_lr = predictions2.select(\"predicted_vol\").collect()\n\ny_orig = predictions.select(\"Volatility\").collect()  \n\n                                                                                \n\n\n\nplt.plot(x_ax, y_orig, label=\"Original\")\nplt.plot(x_ax, y_pred_gbr, label=\"Predicted - GBR\")\nplt.plot(x_ax, y_pred_lr, label=\"Predicted - LR\")\n\nplt.title(\"Predicted vs. Real Volatility (Test Set)\")\nplt.xlabel('Index')\nplt.ylabel('Volatility (% of Open Price)')\nplt.legend(loc='best',fancybox=True, shadow=True)\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "Intro.html",
    "href": "Intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This project focuses on exploring stock and investment discussions within the r/investing and r/cryptocurrency subreddits. By analyzing the types of investments being discussed and examining how engagement varies between these communities, we aim to uncover meaningful insights about investor behavior and sentiment and how these communities interact with and respond to the financial world.\nWe will analyze the distribution of posts over time, identifying the most frequently discussed topics, and assessing user engagement patterns through metrics such as comment counts and post frequencies. We will also look at key linguistic features, such as word frequencies and the prevalence of specific terms, to gain an initial understanding of the dominant themes within each subreddit. This foundational analysis will provide critical insights into the data’s composition and guide the design of subsequent NLP and machine learning workflows.\nWe will use natural language processing to analyze the content of these discussions, employing techniques like topic modeling to identify key themes and sentiment analysis to gauge community sentiment around specific investments. By tracking sentiment trends over time, we aim to observe how opinions in these subreddits evolve in response to major financial events. Our analysis will also explore the dynamics of user engagement, examining how participation fluctuates and how users engage with posts on different topics.\nTo deepen our understanding, we will investigate the relationship between discussions on these platforms and real-world market events. We will explore how sentiment and engagement align with market movements, such as whether positive discussions correlate with stock rallies or how bearish sentiment reflects cryptocurrency sell-offs. Additionally, we plan to leverage machine learning to examine whether patterns in subreddit sentiment and activity can predict short-term market volatility, particularly for highly discussed financial instruments.\nBy combining advanced NLP techniques with machine learning, we hope to uncover valuable insights into the intersection of online investment discussions and market behavior. This research aims to contribute to the growing understanding of how social media influences and reflects the financial landscape, offering meaningful perspectives for investors, analysts, and researchers. Through this work, we seek to highlight the unique role these communities play in shaping modern investment discourse.\nMain subreddits related to investing of interest for the project:\n\n\n\nBanners of the subreddits analyzed for the project\n\n\n\n\nIn order to better address the overall topic of investor behavior and sentiment analysis in Reddit investment communities, we will look at several subtopics driven by business questions that will be addressed through exploratory analysis, natural language processing, or machine learning.\n\n\n\n\nDetermine the most common words on the CryptoCurrency and Investing subreddits.\n\nTechnical Proposal: Conduct word counts on posts and comments from the CryptoCurrency and Investing subreddits to identify frequently discussed terms and topics. By focusing on commonly occurring words and phrases, the analysis aims to reveal popular themes and community topics. The approach involves text processing, filtering for relevancy, and frequency analysis of terms to capture meaningful insights about interests and trends within the subreddit community.\n\n\n\n\n\nDetermine if certain world events lead to increased discussion volume on the Investing subreddit.\n\nTechnical Proposal: Analyze comment activity over time to identify significant spikes and trends. Correlate these peaks with major global events or market conditions to reveal patterns linking real-world occurrences to changes in subreddit engagement, offering insights into the community’s responsiveness to external influences.\n\n\n\n\n\nIdentify temporal patterns in investor engagement to align outreach strategies and predict shifts in market sentiment.\n\nTechnical Proposal: Utilize a heatmap analysis of engagement levels by hour of day and week of the year to uncover temporal patterns in investor activity. Apply time series analysis techniques to detect significant peaks and trends in comment volume. Correlate these patterns with key financial and global events, such as earnings reports, economic announcements, and geopolitical developments. This analysis will provide actionable insights for optimizing communication timing and anticipating periods of heightened market sentiment or volatility.\n\n\n\n\n\nDetermine which stocks are most frequently mentioned on Reddit to gauge general interest and popularity.\n\nTechnical Proposal: Analyze Reddit posts and comments to identify the most mentioned stocks, focusing on their frequency and trends. Visualize results using a bubble chart to highlight popular stocks and compare these mentions against stock price movements to uncover potential correlations.\n\n\n\n\n\nDetermine which cryptocurrencies are most frequently mentioned on Reddit to gauge general interest and popularity.\n\nTechnical Proposal: Perform text processing and frequency analysis on posts and comments in the CryptoCurrency subreddit to identify the most frequently discussed cryptocurrencies. The goal is to understand which cryptocurrencies attract the most attention and discussion in the community.\n\n\n\n\n\nDetermine how cryptocurrency prices affect the number of posts and comments on the CryptoCurrency subreddit.\n\nTechnical Proposal: Analyze the relationship between cryptocurrency prices and the number of posts and comments on the CryptoCurrency subreddit. Examine correlations between price movements and engagement to determine whether significant price changes lead to more or fewer posts and comments. The process involves data collection, preprocessing, and statistical analysis.\n\n\n\n\n\nIdentify the most frequently discussed topics in investing-related comments.\n\nTechnical Proposal: Use RegEx matching to classify comments based on mentions of topics such as “S&P 500,” “Bitcoin,” and “Federal Reserve.” Quantify the frequency of these mentions to identify the most prevalent topics and assess their relative prominence, providing insights into the subjects driving discussions in investing forums.\n\n\n\n\n\nExplore the variation in comment lengths and distributions across subreddits for NLP analysis.\n\nTechnical Proposal: Analyze comment data characteristics across multiple subreddits by sampling data to manage computational resources. Examine the distribution of comment lengths and analyze temporal patterns in comment activity. Insights from this analysis will inform downstream NLP tasks.\n\n\n\n\n\nAnalyze how sentiment varies across key topics and changes over time in investing-related comments.\n\nTechnical Proposal: Combine topic classification with sentiment analysis to measure the proportion of positive, negative, and neutral comments associated with each subject. Build and utilize a sentiment pipeline using Spark NLP. Examine trends over time to identify shifts in sentiment and their relationships with external events.\n\n\n\n\n\nDoes the volume and sentiment of comments on our subreddit affect next-day Bitcoin volatility?\n\nTechnical Proposal: Calculate Bitcoin volatility from daily price data and analyze its relationship with Reddit comment volume and sentiment. Categorize sentiment and generate topic-specific comment counts. Use models such as linear regression and gradient boosted trees to predict volatility, evaluating performance using RMSE and optimizing through hyperparameter tuning.\n\n\n\n\n\nDoes the volume and sentiment of comments predict whether Bitcoin will increase or decrease in price the next day?\n\nTechnical Proposal: Use logistic regression and gradient boosted trees to model the impact of comment volume and sentiment on next-day price movement (up/down). Predictors include sentiment categories and comment counts. Evaluate performance with accuracy and ROC curves, optimizing models through hyperparameter tuning."
  },
  {
    "objectID": "Intro.html#business-questions",
    "href": "Intro.html#business-questions",
    "title": "Introduction",
    "section": "",
    "text": "In order to better address the overall topic of investor behavior and sentiment analysis in Reddit investment communities, we will look at several subtopics driven by business questions that will be addressed through exploratory analysis, natural language processing, or machine learning.\n\n\n\n\nDetermine the most common words on the CryptoCurrency and Investing subreddits.\n\nTechnical Proposal: Conduct word counts on posts and comments from the CryptoCurrency and Investing subreddits to identify frequently discussed terms and topics. By focusing on commonly occurring words and phrases, the analysis aims to reveal popular themes and community topics. The approach involves text processing, filtering for relevancy, and frequency analysis of terms to capture meaningful insights about interests and trends within the subreddit community.\n\n\n\n\n\nDetermine if certain world events lead to increased discussion volume on the Investing subreddit.\n\nTechnical Proposal: Analyze comment activity over time to identify significant spikes and trends. Correlate these peaks with major global events or market conditions to reveal patterns linking real-world occurrences to changes in subreddit engagement, offering insights into the community’s responsiveness to external influences.\n\n\n\n\n\nIdentify temporal patterns in investor engagement to align outreach strategies and predict shifts in market sentiment.\n\nTechnical Proposal: Utilize a heatmap analysis of engagement levels by hour of day and week of the year to uncover temporal patterns in investor activity. Apply time series analysis techniques to detect significant peaks and trends in comment volume. Correlate these patterns with key financial and global events, such as earnings reports, economic announcements, and geopolitical developments. This analysis will provide actionable insights for optimizing communication timing and anticipating periods of heightened market sentiment or volatility.\n\n\n\n\n\nDetermine which stocks are most frequently mentioned on Reddit to gauge general interest and popularity.\n\nTechnical Proposal: Analyze Reddit posts and comments to identify the most mentioned stocks, focusing on their frequency and trends. Visualize results using a bubble chart to highlight popular stocks and compare these mentions against stock price movements to uncover potential correlations.\n\n\n\n\n\nDetermine which cryptocurrencies are most frequently mentioned on Reddit to gauge general interest and popularity.\n\nTechnical Proposal: Perform text processing and frequency analysis on posts and comments in the CryptoCurrency subreddit to identify the most frequently discussed cryptocurrencies. The goal is to understand which cryptocurrencies attract the most attention and discussion in the community.\n\n\n\n\n\nDetermine how cryptocurrency prices affect the number of posts and comments on the CryptoCurrency subreddit.\n\nTechnical Proposal: Analyze the relationship between cryptocurrency prices and the number of posts and comments on the CryptoCurrency subreddit. Examine correlations between price movements and engagement to determine whether significant price changes lead to more or fewer posts and comments. The process involves data collection, preprocessing, and statistical analysis.\n\n\n\n\n\nIdentify the most frequently discussed topics in investing-related comments.\n\nTechnical Proposal: Use RegEx matching to classify comments based on mentions of topics such as “S&P 500,” “Bitcoin,” and “Federal Reserve.” Quantify the frequency of these mentions to identify the most prevalent topics and assess their relative prominence, providing insights into the subjects driving discussions in investing forums.\n\n\n\n\n\nExplore the variation in comment lengths and distributions across subreddits for NLP analysis.\n\nTechnical Proposal: Analyze comment data characteristics across multiple subreddits by sampling data to manage computational resources. Examine the distribution of comment lengths and analyze temporal patterns in comment activity. Insights from this analysis will inform downstream NLP tasks.\n\n\n\n\n\nAnalyze how sentiment varies across key topics and changes over time in investing-related comments.\n\nTechnical Proposal: Combine topic classification with sentiment analysis to measure the proportion of positive, negative, and neutral comments associated with each subject. Build and utilize a sentiment pipeline using Spark NLP. Examine trends over time to identify shifts in sentiment and their relationships with external events.\n\n\n\n\n\nDoes the volume and sentiment of comments on our subreddit affect next-day Bitcoin volatility?\n\nTechnical Proposal: Calculate Bitcoin volatility from daily price data and analyze its relationship with Reddit comment volume and sentiment. Categorize sentiment and generate topic-specific comment counts. Use models such as linear regression and gradient boosted trees to predict volatility, evaluating performance using RMSE and optimizing through hyperparameter tuning.\n\n\n\n\n\nDoes the volume and sentiment of comments predict whether Bitcoin will increase or decrease in price the next day?\n\nTechnical Proposal: Use logistic regression and gradient boosted trees to model the impact of comment volume and sentiment on next-day price movement (up/down). Predictors include sentiment categories and comment counts. Evaluate performance with accuracy and ROC curves, optimizing models through hyperparameter tuning."
  },
  {
    "objectID": "Investing_EDA.html",
    "href": "Investing_EDA.html",
    "title": "EDA for investing subreddit",
    "section": "",
    "text": "Setup\n\n# Setup - Run only once per Kernel App\n%conda install https://anaconda.org/conda-forge/openjdk/11.0.1/download/linux-64/openjdk-11.0.1-hacce0ff_1021.tar.bz2\n\n# install PySpark\n%pip install pyspark==3.4.0\n\n# restart kernel\nfrom IPython.core.display import HTML\nHTML(\"&lt;script&gt;Jupyter.notebook.kernel.restart()&lt;/script&gt;\")\n\nRetrieving notices: ...working... done\n\nDownloading and Extracting Packages:\n\n\n## Package Plan ##\n\n  environment location: /opt/conda\n\n  added / updated specs:\n    - conda-forge/openjdk/11.0.1/download/linux-64::openjdk==11.0.1=hacce0ff_1021\n\n\nThe following NEW packages will be INSTALLED:\n\n  openjdk            conda-forge/openjdk/11.0.1/download/linux-64::openjdk-11.0.1-hacce0ff_1021 \n\n\n\nDownloading and Extracting Packages:\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n\nNote: you may need to restart the kernel to use updated packages.\nCollecting pyspark==3.4.0\n  Using cached pyspark-3.4.0-py2.py3-none-any.whl\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.11/site-packages (from pyspark==3.4.0) (0.10.9.7)\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.4.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\n\nBuilding the Spark session\n\n# Import pyspark and build Spark session\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession.builder.appName(\"PySparkApp\")\n    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n    .config(\n        \"fs.s3a.aws.credentials.provider\",\n        \"com.amazonaws.auth.ContainerCredentialsProvider\",\n    )\n    .getOrCreate()\n)\n\nprint(spark.version)\n\nWarning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\nIvy Default Cache set to: /home/sagemaker-user/.ivy2/cache\nThe jars for the packages stored in: /home/sagemaker-user/.ivy2/jars\norg.apache.hadoop#hadoop-aws added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-b02b0b31-b2cc-4ff4-a0ef-5bb8148d9c77;1.0\n    confs: [default]\n    found org.apache.hadoop#hadoop-aws;3.2.2 in central\n    found com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n:: resolution report :: resolve 190ms :: artifacts dl 6ms\n    :: modules in use:\n    com.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n    org.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n    ---------------------------------------------------------------------\n    |                  |            modules            ||   artifacts   |\n    |       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n    ---------------------------------------------------------------------\n    |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n    ---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-b02b0b31-b2cc-4ff4-a0ef-5bb8148d9c77\n    confs: [default]\n    0 artifacts copied, 2 already retrieved (0kB/5ms)\n24/12/13 02:15:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n:: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n3.4.0\n\n\n\n\nFetching the data from S3\n\n%%time\nimport sagemaker\nfrom sagemaker.spark.processing import PySparkProcessor\n\n# Setup the PySpark processor to run the job. Note the instance type and instance count parameters. SageMaker will create these many instances of this type for the spark job.\nrole = sagemaker.get_execution_role()\nspark_processor = PySparkProcessor(\n    base_job_name=\"sm-spark-project\",\n    framework_version=\"3.3\",\n    role=role,\n    instance_count=4,\n    instance_type=\"ml.m5.xlarge\",\n    max_runtime_in_seconds=3600,\n)\n\n# s3 paths\nsession = sagemaker.Session()\nbucket = session.default_bucket()\ns3_dataset_path_commments = \"s3://bigdatateaching/reddit-project/reddit/parquet/comments/yyyy=*/mm=*/*.parquet\"\ns3_dataset_path_submissions = \"s3://bigdatateaching/reddit-project/reddit/parquet/submissions/yyyy=*/mm=*/*.parquet\"\noutput_prefix_data = \"project\"\noutput_prefix_logs = f\"spark_logs\"\n\n# modify this comma separated list to choose the subreddits of interest\n#subreddits = \"technology,chatgpt\"\nsubreddits = \"investing,cryptocurrency\"\n    \n\nsagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\nsagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\nCPU times: user 2.51 s, sys: 325 ms, total: 2.84 s\nWall time: 4.14 s\n\n\n\n%%time\ns3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\nprint(f\"reading submissions from {s3_path}\")\ncomments = spark.read.parquet(s3_path, header=True)\nprint(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-381491950264/project/comments\nshape of the comments dataframe is 3,877,393x17\nCPU times: user 103 ms, sys: 50.1 ms, total: 153 ms\nWall time: 3min 58s\n\n\n24/12/13 02:16:34 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n[Stage 1:======================================================&gt;(116 + 1) / 117]                                                                                \n\n\n\ncomments.printSchema()\n\nroot\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n\n\n\n\n# display a subset of columns\ncomments.select(\"subreddit\", \"author\", \"body\", \"parent_id\", \"link_id\", \"id\", \"created_utc\").show()\n\n+--------------+--------------------+--------------------+----------+----------+-------+-----------+\n|     subreddit|              author|                body| parent_id|   link_id|     id|created_utc|\n+--------------+--------------------+--------------------+----------+----------+-------+-----------+\n|CryptoCurrency|       TexasBoyz-713|Dude I wouldn’t s...|t1_jsjxgdf|t3_153et67|jsjxjpu| 1689744125|\n|CryptoCurrency|         bthemonarch|Moons are the rea...|t1_jsjxcqm|t3_153et67|jsjxjuv| 1689744128|\n|CryptoCurrency|        keithwee0909|Should I buy some...|t1_jsjxfk8|t3_153et67|jsjxk0f| 1689744130|\n|CryptoCurrency|           [deleted]|           [removed]|t3_153et67|t3_153et67|jsjxk5v| 1689744133|\n|CryptoCurrency|       Traveler-0854|¢0.63\\n\\nWe are o...|t3_153et67|t3_153et67|jsjxk5x| 1689744133|\n|CryptoCurrency|       Illicitterror|    Prime Trustusbro|t3_153axby|t3_153axby|jsjxk66| 1689744133|\n|CryptoCurrency|              vip887|Yup, a tiny mista...|t3_153ky0e|t3_153ky0e|jsjxk85| 1689744134|\n|CryptoCurrency|         chintokkong|New Yorkers will ...|t3_153et67|t3_153et67|jsjxkfo| 1689744137|\n|CryptoCurrency|       AutoModerator|\\nHere is a [Nitt...|t1_jsjxkfo|t3_153et67|jsjxkgp| 1689744138|\n|CryptoCurrency|           ziggyzago|I’m having crazy ...|t3_153et67|t3_153et67|jsjxkk4| 1689744140|\n|CryptoCurrency|                R4ID|with how fast it ...|t3_153lm7j|t3_153lm7j|jsjxkkr| 1689744140|\n|CryptoCurrency|           RedBunery|&gt; Cathie Wood ass...|t3_153ip93|t3_153ip93|jsjxks6| 1689744143|\n|CryptoCurrency|            ccModBot|Hello!\\n\\nDirect ...|t3_153lm7j|t3_153lm7j|jsjxl1m| 1689744148|\n|CryptoCurrency|CoolCoolPapaOldSkool|Yellow for me for...|t3_153lm7j|t3_153lm7j|jsjxl24| 1689744148|\n|CryptoCurrency|       Agile_Ad_7061|No. Prince from N...|t3_153ky0e|t3_153ky0e|jsjxl4z| 1689744150|\n|CryptoCurrency|           Frogmangy|Literally know a ...|t1_jsjvuyx|t3_153ky0e|jsjxl5i| 1689744150|\n|CryptoCurrency|  ImNotFromThisWorld|Man I should have...|t3_153et67|t3_153et67|jsjxlx6| 1689744163|\n|CryptoCurrency|          send_tacoz|Aren't Doge price...|t3_153krw1|t3_153krw1|jsjxmgi| 1689744172|\n|CryptoCurrency|      OddIndication4|Ask yourself the ...|t3_1539jks|t3_1539jks|jsjxmh7| 1689744172|\n|CryptoCurrency|             chili21|           !gas nova|t3_12pf9pb|t3_12pf9pb|jsjxmpc| 1689744176|\n+--------------+--------------------+--------------------+----------+----------+-------+-----------+\nonly showing top 20 rows\n\n\n\n\n%%time\ns3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\nprint(f\"reading submissions from {s3_path}\")\nsubmissions = spark.read.parquet(s3_path, header=True)\nprint(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-381491950264/project/submissions\nshape of the submissions dataframe is 205,720x21\nCPU times: user 12.9 ms, sys: 11.8 ms, total: 24.7 ms\nWall time: 38.8 s\n\n\n[Stage 6:=====================================================&gt;   (17 + 1) / 18]                                                                                \n\n\n\nsubmissions.printSchema()\n\nroot\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- domain: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- id: string (nullable = true)\n |-- is_self: boolean (nullable = true)\n |-- locked: boolean (nullable = true)\n |-- num_comments: long (nullable = true)\n |-- over_18: boolean (nullable = true)\n |-- quarantine: boolean (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- selftext: string (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n\n\n\n\n# display a subset of columns\nsubmissions.select(\"subreddit\", \"author\", \"title\", \"selftext\", \"num_comments\", \"created_utc\").show()\n\n+--------------+-------------------+--------------------+--------------------+------------+-----------+\n|     subreddit|             author|               title|            selftext|num_comments|created_utc|\n+--------------+-------------------+--------------------+--------------------+------------+-----------+\n|     investing|       WichitaFlyer|Will negative pop...|Millennial and ge...|           0| 1689714390|\n|CryptoCurrency|            V11c7or|            AITrader|           [removed]|           0| 1689714454|\n|     investing| Ok_Supermarket9812|Tools to assess f...|           [removed]|           1| 1689714522|\n|CryptoCurrency|    charlythesecond|Terraform Labs Co...|                    |           0| 1689714532|\n|CryptoCurrency|    badfishbeefcake|Scam Alert: Celsi...|           [removed]|           2| 1689714536|\n|CryptoCurrency|   Fun-Juggernautyy|Opinions on Hatom...|           [removed]|           1| 1689714660|\n|CryptoCurrency|   Fun-Juggernautyy|Opinions on Hatom...|           [removed]|           1| 1689714664|\n|CryptoCurrency|     OneThatNoseOne|Today, many peopl...|Safe to say that ...|           0| 1689714869|\n|     investing| Ok_Supermarket9812|Tool to assess fu...|I am looking for ...|           0| 1689714939|\n|CryptoCurrency|          [deleted]|SUI, its hype and...|           [removed]|           1| 1689715044|\n|CryptoCurrency|         garchmodel|if arkham was rea...|           [removed]|           1| 1689715090|\n|CryptoCurrency|    Fun_Problem_914|hola mrbeast no m...|           [removed]|           1| 1689715093|\n|CryptoCurrency|          syndoms18|SUI, its hype and...|           [removed]|           1| 1689715103|\n|CryptoCurrency|        RassuEst112|Are There Any Non...|           [removed]|           1| 1689715260|\n|     investing|            StophJS|Your ETF portfoli...|This may be flagg...|           0| 1689715325|\n|CryptoCurrency|   DrHunterThompson| MOONS Mass Adoption|           [removed]|           1| 1689715452|\n|CryptoCurrency|           bdiggles|Where can U.S. tr...|           [removed]|           1| 1689715500|\n|CryptoCurrency|        Oneiros1999|Prime Trust Put I...|                    |           0| 1689715584|\n|CryptoCurrency|Artorias_the_hollow|I bought some Moo...|           [removed]|           1| 1689715619|\n|CryptoCurrency|  Nearby_Market3054|Bitcoin mining an...|           [removed]|           1| 1689715639|\n+--------------+-------------------+--------------------+--------------------+------------+-----------+\nonly showing top 20 rows\n\n\n\n\n\nSubmissions data overview and quality check\n\n%%time\ns3_path = f\"s3a://{bucket}/{output_prefix_data}/submissions\"\nprint(f\"reading submissions from {s3_path}\")\nsubmissions = spark.read.parquet(s3_path, header=True)\nprint(f\"shape of the submissions dataframe is {submissions.count():,}x{len(submissions.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-381491950264/project/submissions\nshape of the submissions dataframe is 205,720x21\nCPU times: user 16.3 ms, sys: 2.75 ms, total: 19 ms\nWall time: 26.5 s\n\n\n                                                                                \n\n\n\nsubmissions.printSchema()\n\nroot\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- domain: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- id: string (nullable = true)\n |-- is_self: boolean (nullable = true)\n |-- locked: boolean (nullable = true)\n |-- num_comments: long (nullable = true)\n |-- over_18: boolean (nullable = true)\n |-- quarantine: boolean (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- selftext: string (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n\n\n\n\ninvest_sub =  submissions.filter(submissions.subreddit == \"investing\")\n\n\ninvest_sub.show()\n\n+--------------------+----------------------+-----------------+-----------+-------------+--------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+\n|              author|author_flair_css_class|author_flair_text|created_utc|distinguished|        domain|edited|     id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|            selftext|stickied|subreddit|subreddit_id|               title|                 url|\n+--------------------+----------------------+-----------------+-----------+-------------+--------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+\n|        WichitaFlyer|                  null|             null| 1689714390|         null|self.investing|  null|153af75|   true| false|           0|  false|     false|  1689714416|    1|Millennial and ge...|   false|investing|    t5_2qhhq|Will negative pop...|https://www.reddi...|\n|  Ok_Supermarket9812|                  null|             null| 1689714522|         null|self.investing|  null|153ah5n|   true| false|           1|  false|     false|  1689714551|    1|           [removed]|   false|investing|    t5_2qhhq|Tools to assess f...|https://www.reddi...|\n|  Ok_Supermarket9812|                  null|             null| 1689714939|         null|self.investing|  null|153anlg|   true| false|           0|  false|     false|  1689714967|    1|I am looking for ...|   false|investing|    t5_2qhhq|Tool to assess fu...|https://www.reddi...|\n|             StophJS|                  null|             null| 1689715325|         null|self.investing|  null|153atef|   true| false|           0|  false|     false|  1689715376|    1|This may be flagg...|   false|investing|    t5_2qhhq|Your ETF portfoli...|https://www.reddi...|\n|         MrSilk13642|                  null|             null| 1689716124|         null|self.investing|  null|153b5km|   true| false|           0|  false|     false|  1689716152|    1|So I recently mad...|   false|investing|    t5_2qhhq|Why does my margi...|https://www.reddi...|\n|   Chocolatechip2021|                  null|             null| 1689717578|         null|self.investing|  null|153bryb|   true| false|           1|  false|     false|  1689717610|    1|           [removed]|   false|investing|    t5_2qhhq|Where should I mo...|https://www.reddi...|\n|   Chocolatechip2021|                  null|             null| 1689717830|         null|self.investing|  null|153bw79|   true| false|           0|  false|     false|  1689717854|    1|I deposited 5k in...|   false|investing|    t5_2qhhq|Where should I mo...|https://www.reddi...|\n|          gamerz0111|                  null|             null| 1689719377|         null|self.investing|  null|153ckgh|   true| false|           0|  false|     false|  1689719398|    1|\\*I'm currently r...|   false|investing|    t5_2qhhq|Is Zim Integrated...|https://www.reddi...|\n|             Akhasic|                  null|             null| 1689719684|         null|self.investing|  null|153cp6x|   true| false|           1|  false|     false|  1689719712|    1|           [removed]|   false|investing|    t5_2qhhq|Demotez.com for s...|https://www.reddi...|\n|   Chocolatechip2021|                  null|             null| 1689720385|         null|self.investing|  null|153d01a|   true| false|           0|  false|     false|  1689720417|    1|I am new to the i...|   false|investing|    t5_2qhhq|Which investment ...|https://www.reddi...|\n|           stutunaru|                  null|             null| 1689721920|         null|self.investing|  null|153dno6|   true| false|           1|  false|     false|  1689721944|    1|           [removed]|   false|investing|    t5_2qhhq|Looking for a sma...|https://www.reddi...|\n|         frogview123|                  null|             null| 1689722111|         null|self.investing|  null|153dqcx|   true| false|           0|  false|     false|  1689722139|    1|I recently starte...|   false|investing|    t5_2qhhq|Something like Un...|https://www.reddi...|\n|           rodroprez|                  null|             null| 1689722389|         null|self.investing|  null|153dui4|   true| false|           1|  false|     false|  1689722413|    1|           [removed]|   false|investing|    t5_2qhhq|I'm 41 and have 1...|https://www.reddi...|\n|whosthedoginthisscen|                  null|             null| 1689722416|         null|self.investing|  null|153dux6|   true| false|           0|  false|     false|  1689722440|    1|For instance, I c...|   false|investing|    t5_2qhhq|Is there a source...|https://www.reddi...|\n|    ConsequencesFree|                  null|             null| 1689722976|         null|self.investing|  null|153e2wi|   true| false|           0|  false|     false|  1689722999|    1|How is it legal t...|   false|investing|    t5_2qhhq|When covered call...|https://www.reddi...|\n|            one1jynx|                  null|             null| 1689725323|         null|self.investing|  null|153f06e|   true| false|           1|  false|     false|  1689725353|    1|           [removed]|   false|investing|    t5_2qhhq|I have 149 on Web...|https://www.reddi...|\n|  Wide_Pineapple6373|                  null|             null| 1689726807|         null|self.investing|  null|153fkjn|   true| false|           1|  false|     false|  1689726824|    1|           [removed]|   false|investing|    t5_2qhhq|20 Yr old portfol...|https://www.reddi...|\n|            NickSS38|                  null|             null| 1689726889|         null|self.investing|  null|153flmp|   true| false|           0|  false|     false|  1689726912|    1|Hi everyone.  I h...|   false|investing|    t5_2qhhq|Advice Needed: $8...|https://www.reddi...|\n|       DinkyDoodle69|                  null|             null| 1689728018|         null|self.investing|  null|153g076|   true| false|           0|  false|     false|  1689728041|    1|Walt Disney Corpo...|   false|investing|    t5_2qhhq|As market hits ne...|https://www.reddi...|\n|       DinkyDoodle69|                  null|             null| 1689728068|         null|self.investing|  null|153g0vn|   true| false|           0|  false|     false|  1689728085|    1|Walt Disney Corpo...|   false|investing|    t5_2qhhq|As market hits ne...|https://www.reddi...|\n+--------------------+----------------------+-----------------+-----------+-------------+--------------+------+-------+-------+------+------------+-------+----------+------------+-----+--------------------+--------+---------+------------+--------------------+--------------------+\nonly showing top 20 rows\n\n\n\n\nprint(f\"shape of the submissions dataframe is {invest_sub.count():,}x{len(invest_sub.columns)}\")\n\n[Stage 15:====================================================&gt;   (17 + 1) / 18]                                                                                \n\n\nshape of the submissions dataframe is 43,041x21\n\n\n\nfrom pyspark.sql.functions import col, when, count, isnan\nfrom pyspark.sql.types import DoubleType, FloatType\n\nnull_counts = invest_sub.select([\n    count(when(col(c).isNull() | (col(c).cast(DoubleType()).isNotNull() & isnan(col(c))), c)).alias(c) \n    if invest_sub.schema[c].dataType in [DoubleType(), FloatType()]\n    else count(when(col(c).isNull(), c)).alias(c)\n    for c in invest_sub.columns\n])\n\nnull_counts.show()\n\n[Stage 18:====================================================&gt;   (17 + 1) / 18]                                                                                \n\n\n+------+----------------------+-----------------+-----------+-------------+------+------+---+-------+------+------------+-------+----------+------------+-----+--------+--------+---------+------------+-----+---+\n|author|author_flair_css_class|author_flair_text|created_utc|distinguished|domain|edited| id|is_self|locked|num_comments|over_18|quarantine|retrieved_on|score|selftext|stickied|subreddit|subreddit_id|title|url|\n+------+----------------------+-----------------+-----------+-------------+------+------+---+-------+------+------------+-------+----------+------------+-----+--------+--------+---------+------------+-----+---+\n|     0|                 43041|            43041|          0|        43041|     0| 42776|  0|      0|     0|           0|      0|         0|           0|    0|       0|       0|        0|           0|    0|  0|\n+------+----------------------+-----------------+-----------+-------------+------+------+---+-------+------+------------+-------+----------+------------+-----+--------+--------+---------+------------+-----+---+\n\n\n\n\nfrom pyspark.sql.functions import length\n\nfiltered_invest_sub = invest_sub.filter(length(\"title\") &gt;= 3)\n\nshortest_titles = filtered_invest_sub.select(\"title\") \\\n                        .withColumn(\"title_length\", length(\"title\")) \\\n                        .orderBy(\"title_length\") \\\n                        .limit(20)\n\nshortest_titles.show(truncate=False)\n\n[Stage 21:=================================================&gt;      (16 + 2) / 18]                                                                                \n\n\n+-------------------------+------------+\n|title                    |title_length|\n+-------------------------+------------+\n|Investing lesson for kids|25          |\n|Captive Insurance Program|25          |\n|Yieldmax Has Cult Losers |25          |\n|Investing 150K in Europe?|25          |\n|Simple Wash Sell Question|25          |\n|Where to start to end 9-5|25          |\n|Working to setup taxable |25          |\n|$500 TO INVEST PER MONTH!|25          |\n|Sell ESPP to max out ROTH|25          |\n|Richtech Robotics class B|25          |\n|What happened to biotech?|25          |\n|401k trading and BTC ETFs|25          |\n|What am I actually doing?|25          |\n|Wealthsimple and Fidelity|25          |\n|Needing investment advice|25          |\n|What should I do with 5k?|25          |\n|Honest question of \"Why?\"|25          |\n|Bonds for 60/40 Portfolio|25          |\n|Investing trends question|25          |\n|Strategy for buying stock|25          |\n+-------------------------+------------+\n\n\n\n\nprint(f\"shape of the investing submissions dataframe is {filtered_invest_sub.count():,}x{len(filtered_invest_sub.columns)}\")\n\n[Stage 22:====================================================&gt;   (17 + 1) / 18]                                                                                \n\n\nshape of the investing submissions dataframe is 43,041x21\n\n\n\nremoved_count = invest_sub.filter(invest_sub.title == \"[removed]\").count()\n\nprint(f\"Number of '[removed]' in title: {removed_count}\")\n\n[Stage 25:=================================================&gt;      (16 + 2) / 18]                                                                                \n\n\nNumber of '[removed]' in title: 0\n\n\n\ninvest_sub = filtered_invest_sub\n\n\n\nComments data overview and quality check\n\n%%time\ns3_path = f\"s3a://{bucket}/{output_prefix_data}/comments\"\nprint(f\"reading submissions from {s3_path}\")\ncomments = spark.read.parquet(s3_path, header=True)\nprint(f\"shape of the comments dataframe is {comments.count():,}x{len(comments.columns)}\")\n\nreading submissions from s3a://sagemaker-us-east-1-381491950264/project/comments\nshape of the comments dataframe is 3,877,393x17\nCPU times: user 109 ms, sys: 25.9 ms, total: 135 ms\nWall time: 3min 19s\n\n\n[Stage 29:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\n\ncomments.printSchema()\n\nroot\n |-- author: string (nullable = true)\n |-- author_flair_css_class: string (nullable = true)\n |-- author_flair_text: string (nullable = true)\n |-- body: string (nullable = true)\n |-- controversiality: long (nullable = true)\n |-- created_utc: long (nullable = true)\n |-- distinguished: string (nullable = true)\n |-- edited: double (nullable = true)\n |-- gilded: long (nullable = true)\n |-- id: string (nullable = true)\n |-- link_id: string (nullable = true)\n |-- parent_id: string (nullable = true)\n |-- retrieved_on: long (nullable = true)\n |-- score: long (nullable = true)\n |-- stickied: boolean (nullable = true)\n |-- subreddit: string (nullable = true)\n |-- subreddit_id: string (nullable = true)\n\n\n\n\ninvest_comments =  comments.filter(comments.subreddit == \"investing\")\n\n\ninvest_comments.show()\n\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------+------------+\n|              author|author_flair_css_class|author_flair_text|                body|controversiality|created_utc|distinguished|edited|gilded|     id|   link_id| parent_id|retrieved_on|score|stickied|subreddit|subreddit_id|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------+------------+\n|    Alfred_Love_Song|                  null|             null|Not sure about bi...|               0| 1689744177|         null|  null|     0|jsjxms6|t3_153kjyt|t1_jsjwmaz|  1689744200|    1|   false|investing|    t5_2qhhq|\n|    Alfred_Love_Song|                  null|             null|    Whats a small %?|               0| 1689744195|         null|  null|     0|jsjxnv8|t3_153kjyt|t1_jsjwyi4|  1689744216|    1|   false|investing|    t5_2qhhq|\n|Waste-Temperature626|                  null|             null|&gt; hedge against h...|               0| 1689744409|         null|  null|     0|jsjy02y|t3_153kjyt|t1_jsjwyi4|  1689744428|    1|   false|investing|    t5_2qhhq|\n|      ChengSkwatalot|                  null|             null|All else equal, a...|               0| 1689744548|         null|  null|     0|jsjy7tz|t3_153af75|t3_153af75|  1689744576|    1|   false|investing|    t5_2qhhq|\n|           ImamTrump|                  null|             null|No,because the US...|               0| 1689744870|         null|  null|     0|jsjypf1|t3_153af75|t3_153af75|  1689744894|    1|   false|investing|    t5_2qhhq|\n|           Rooflife1|                  null|             null|I always heard “p...|               0| 1689745044|         null|  null|     0|jsjyyy4|t3_153e2wi|t3_153e2wi|  1689745074|    1|   false|investing|    t5_2qhhq|\n|             Zanna-K|                  null|             null|Productivity can'...|               0| 1689745048|         null|  null|     0|jsjyz69|t3_153af75|t1_jsjml5s|  1689745074|    1|   false|investing|    t5_2qhhq|\n|         MrSilk13642|                  null|             null|Oh ok sounds good...|               0| 1689745216|         null|  null|     0|jsjz8j0|t3_153b5km|t1_jsjv74i|  1689745240|    1|   false|investing|    t5_2qhhq|\n|         bungholio99|                  null|             null|Most banks are bu...|               0| 1689745221|         null|  null|     0|jsjz8v6|t3_153kjyt|t3_153kjyt|  1689745240|    1|   false|investing|    t5_2qhhq|\n|        Vast_Cricket|                  null|             null|There are high yi...|               0| 1689745290|         null|  null|     0|jsjzcjh|t3_153j8sp|t3_153j8sp|  1689745311|    1|   false|investing|    t5_2qhhq|\n|         JenerousJew|                  null|             null|Price it in somet...|               0| 1689745353|         null|  null|     0|jsjzfwf|t3_152lv6c|t3_152lv6c|  1689745371|    1|   false|investing|    t5_2qhhq|\n|  T_Dizzle_My_Nizzle|                  null|             null|This will likely ...|               0| 1689745469|         null|  null|     0|jsjzm7w|t3_153af75|t3_153af75|  1689745493|    1|   false|investing|    t5_2qhhq|\n|              mrclut|                  null|             null|I don't have much...|               0| 1689745743|         null|  null|     0|jsk00nh|t3_153af75|t3_153af75|  1689745766|    1|   false|investing|    t5_2qhhq|\n|        Bates_master|                  null|             null|get it while it's...|               0| 1689745787|         null|  null|     0|jsk02ve|t3_153af75|t3_153af75|  1689745814|    1|   false|investing|    t5_2qhhq|\n|            ddlJunky|                  null|             null|It's inevitable t...|               0| 1689745843|         null|  null|     0|jsk05ve|t3_152lv6c|t1_jsj8cau|  1689745866|    1|   false|investing|    t5_2qhhq|\n|        MidKnight148|                  null|             null|I'm not concerned...|               0| 1689745891|         null|  null|     0|jsk08ie|t3_153af75|t3_153af75|  1689745912|    1|   false|investing|    t5_2qhhq|\n|        YamiChan1016|                  null|             null|No, you misunders...|               0| 1689746298|         null|  null|     0|jsk0twr|t3_152lv6c|t1_jsk05ve|  1689746320|    1|   false|investing|    t5_2qhhq|\n|        YamiChan1016|                  null|             null|Wine or art is ju...|               0| 1689746342|         null|  null|     0|jsk0wa9|t3_152lv6c|t1_jsk05ve|  1689746367|    1|   false|investing|    t5_2qhhq|\n|           dCrumpets|                  null|             null|The republicans c...|               0| 1689746393|         null|  null|     0|jsk0yxk|t3_153af75|t1_jsj7uro|  1689746422|    1|   false|investing|    t5_2qhhq|\n|           fraylovze|                  null|             null|Are you just hold...|               0| 1689746440|         null|  null|     0|jsk11f7|t3_15357vy|t3_15357vy|  1689746470|    1|   false|investing|    t5_2qhhq|\n+--------------------+----------------------+-----------------+--------------------+----------------+-----------+-------------+------+------+-------+----------+----------+------------+-----+--------+---------+------------+\nonly showing top 20 rows\n\n\n\n\nfrom pyspark.sql.functions import col, count, when, isnan\nfrom pyspark.sql.types import DoubleType, FloatType\n\n# Calculate null counts for each column\ncomments_null_counts = invest_comments.select([\n    count(when(col(c).isNull() | (col(c).cast(DoubleType()).isNotNull() & isnan(col(c))), c)).alias(c) \n    if invest_comments.schema[c].dataType in [DoubleType(), FloatType()]\n    else count(when(col(c).isNull(), c)).alias(c)\n    for c in invest_comments.columns\n])\n\ncomments_null_counts.show()\n\n[Stage 33:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+\n|author|author_flair_css_class|author_flair_text|body|controversiality|created_utc|distinguished|edited|gilded| id|link_id|parent_id|retrieved_on|score|stickied|subreddit|subreddit_id|\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+\n|     0|                516482|           516482|   0|               0|          0|       485737|507049|     0|  0|      0|        0|           0|    0|       0|        0|           0|\n+------+----------------------+-----------------+----+----------------+-----------+-------------+------+------+---+-------+---------+------------+-----+--------+---------+------------+\n\n\n\n\nfrom pyspark.sql.functions import length\nfiltered_invest_comments = invest_comments.filter(length(\"body\") &gt;= 3)\n\nshortest_comments = filtered_invest_comments.select(\"body\") \\\n                        .withColumn(\"body_length\", length(\"body\")) \\\n                        .orderBy(\"body_length\") \\\n                        .limit(20)\n\nshortest_comments.show(truncate=False)\n\n[Stage 36:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\n+----+-----------+\n|body|body_length|\n+----+-----------+\n|VTI |3          |\n|ISO |3          |\n|Lol |3          |\n|Lol |3          |\n|Lol |3          |\n|idk |3          |\n|VTI |3          |\n|Oof |3          |\n|Lol |3          |\n|Yes |3          |\n|Yep |3          |\n|XOM |3          |\n|Yup |3          |\n|??? |3          |\n|Lol |3          |\n|lol |3          |\n|VOO |3          |\n|Yes |3          |\n|VGT |3          |\n|VTI |3          |\n+----+-----------+\n\n\n\n\nprint(f\"shape of the comments dataframe is {filtered_invest_comments.count():,}x{len(filtered_invest_comments.columns)}\")\n\n[Stage 37:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\nshape of the comments dataframe is 515,133x17\n\n\n\ncomments_removed_count = invest_comments.filter(invest_comments.body == \"[removed]\").count()\n\nprint(f\"Number of '[removed]' in body: {comments_removed_count}\")\n\n[Stage 40:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\nNumber of '[removed]' in body: 7083\n\n\n\ninvest_comments = filtered_invest_comments.filter(invest_comments.body != \"[removed]\")\n\n\nprint(f\"shape of the invest_comments dataframe is {invest_comments.count():,}x{len(invest_comments.columns)}\")\n\n[Stage 43:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\nshape of the invest_comments dataframe is 508,050x17\n\n\n\n\nEDA Questions and Analysis\n\nBusiness goal 1\nDetermining the most common words/topics on investing on sub reddit.\n\nTechnical proposal\n\nfrom pyspark.sql.functions import col, lower, regexp_replace, explode, split\nfrom pyspark.ml.feature import StopWordsRemover\n\n# Step 1: Text Preprocessing\n# Convert 'body' text to lowercase and remove punctuation\ninvest_comments_clean = invest_comments.withColumn(\n    \"cleaned_body\", \n    lower(regexp_replace(col(\"body\"), \"[^a-zA-Z\\\\s]\", \"\"))\n)\n\n# Split text into words\ninvest_comments_words = invest_comments_clean.withColumn(\n    \"words\", split(col(\"cleaned_body\"), \"\\\\s+\")\n)\n\n# Remove stop words using Spark's StopWordsRemover\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\ninvest_comments_filtered = remover.transform(invest_comments_words)\n\n# Step 2: Word Frequency Analysis\n# Explode the 'filtered_words' column to count word occurrences\nwords_df = invest_comments_filtered.select(explode(col(\"filtered_words\")).alias(\"word\"))\n\n# Filter out any empty strings that might appear\nwords_df = words_df.filter(col(\"word\") != \"\")\n\n# Count the frequency of each word\nword_counts = words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n\n# Show the top 20 most common words\nword_counts.show(20)\n\n[Stage 46:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\n+-------------+-----+\n|         word|count|\n+-------------+-----+\n|         like|92144|\n|        money|85407|\n|       please|74849|\n|            k|71039|\n|         dont|68591|\n|       market|67559|\n|        years|59518|\n|          get|56161|\n|           im|53827|\n|        think|52949|\n|         time|49489|\n|         make|48734|\n|       people|48451|\n|         year|46358|\n|automatically|46324|\n|      removed|46290|\n|         post|45833|\n|         also|45677|\n|      account|44345|\n|          one|43235|\n+-------------+-----+\nonly showing top 20 rows\n\n\n\n\ncomments_word_counts_top50 = word_counts.limit(50).toPandas()\n\ncomments_word_counts_top50.to_csv(\"comments_word_counts_top50.csv\", index=False)\n\n                                                                                \n\n\n\nimport matplotlib.pyplot as plt\n\n# Converting the Spark DataFrame to Pandas\ntop_words = word_counts.limit(20).toPandas()\n\n# Plot the top 20 most common words\nplt.figure(figsize=(12, 6))\nplt.bar(top_words['word'], top_words['count'], alpha=0.7)\nplt.title('Top 20 Most Common Words in Investing Subreddit', fontsize=16)\nplt.xlabel('Words', fontsize=14)\nplt.ylabel('Frequency of words (in thousands)', fontsize=14)\nplt.xticks(rotation=45, ha='right', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n                                                                                \n\n\n\n\n\n\n\n\nBusiness goal 2\nIdentify trending topics and key investment terms in Reddit discussions to inform content strategy and product development for financial services.\n\nTechnical proposal\n\n!pip install wordcloud\n\nCollecting wordcloud\n  Using cached wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: numpy&gt;=1.6.1 in /opt/conda/lib/python3.11/site-packages (from wordcloud) (1.26.4)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from wordcloud) (10.4.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from wordcloud) (3.9.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (4.54.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (1.4.7)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (24.1)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (3.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib-&gt;wordcloud) (2.9.0)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;wordcloud) (1.16.0)\nUsing cached wordcloud-1.9.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\nInstalling collected packages: wordcloud\nSuccessfully installed wordcloud-1.9.4\n\n\n\nfrom wordcloud import WordCloud\nimport re\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nimport nltk\n\n# Download stopwords if not already available\nnltk.download('stopwords')\n\n# Preprocess text and create a single text corpus from the 'body' column in invest_comments\ntext = \" \".join(comment for comment in invest_comments.select('body').rdd.flatMap(lambda x: x).collect())\ntext = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n\n# Define and remove common stopwords\nstop_words = set(stopwords.words('english'))\nwordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stop_words, colormap='viridis').generate(text)\n\n# Plot the word cloud\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Popular Terms in Investing Subreddit Comments')\nplt.show()\nplt.savefig('wordcloud.png')\n\n[nltk_data] Downloading package stopwords to /home/sagemaker-\n[nltk_data]     user/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n                                                                                \n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\nBusiness goal 3\nDetermine whether certain events lead to increased discussion volume.\n\nTechnical proposal\n\n!pip install seaborn\n\nCollecting seaborn\n  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: numpy!=1.24.0,&gt;=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (1.26.4)\nRequirement already satisfied: pandas&gt;=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.3)\nRequirement already satisfied: matplotlib!=3.6.1,&gt;=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.9.2)\nRequirement already satisfied: contourpy&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (0.12.1)\nRequirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (4.54.1)\nRequirement already satisfied: kiwisolver&gt;=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.4.7)\nRequirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (24.1)\nRequirement already satisfied: pillow&gt;=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (10.4.0)\nRequirement already satisfied: pyparsing&gt;=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (3.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (2.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2023.3)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.2-&gt;seaborn) (2024.2)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib!=3.6.1,&gt;=3.4-&gt;seaborn) (1.16.0)\nUsing cached seaborn-0.13.2-py3-none-any.whl (294 kB)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.13.2\n\n\n\nfrom pyspark.sql.functions import from_unixtime, weekofyear, hour, avg\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Convert created_utc to datetime and extract week_of_the_year and hour_of_the_day\ninvest_comments = invest_comments.withColumn(\"datetime\", from_unixtime(invest_comments.created_utc))\ninvest_comments = invest_comments.withColumn(\"week_of_the_year\", weekofyear(\"datetime\"))\ninvest_comments = invest_comments.withColumn(\"hour_of_the_day\", hour(\"datetime\"))\n\n# Group by week_of_the_year and hour_of_the_day and calculate average score\nengagement_data = invest_comments.groupBy(\"week_of_the_year\", \"hour_of_the_day\").agg(avg(\"score\").alias(\"average_score\"))\n\n# Convert to Pandas for visualization\nengagement_pd = engagement_data.toPandas()\n\n# Pivot the data for heatmap\nheatmap_data = engagement_pd.pivot(index=\"week_of_the_year\", columns=\"hour_of_the_day\", values=\"average_score\")\n\n# Basic plot for checking data\nplt.figure(figsize=(15, 10))\nsns.heatmap(heatmap_data, cmap='viridis', annot=False, fmt='.2f', linewidths=0.5, cbar=True)\nplt.title('Average Number of Comments in Investing subreddit per Hour and Week of the Year', fontsize=16)\nplt.xlabel('Hour of the Day (in hours)', fontsize=14)\nplt.ylabel('Week of the Year (in weeks)', fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.tight_layout()  # Ensure everything fits nicely\nplt.show()\n\n                                                                                \n\n\n\n\n\n\n\n\nBusiness goal 4\nDetermine which stocks are most frequently mentioned on Reddit to gauge general interest and popularity.\n\nTechnical proposal\n\n# Extracting Stock Tickers from Reddit Data \n\nfrom pyspark.sql.functions import regexp_extract, col\n\n# Defining a regular expression to match stock tickers (e.g., $AAPL)\n# Adjust regex if needed for more accurate ticker detection\nticker_regex = r\"\\b[A-Z]{1,5}\\b\"\n\n# Extracting possible tickers from the 'body' column\npotential_tickers = invest_comments.withColumn(\"ticker\", regexp_extract(col(\"body\"), ticker_regex, 0))\n\n# Filtering non-empty tickers\nvalid_tickers = potential_tickers.filter(col(\"ticker\") != \"\")\n\n# Counting mentions of each ticker\nticker_counts = valid_tickers.groupBy(\"ticker\").count().orderBy(col(\"count\").desc())\n\n# Showing the most mentioned tickers\nticker_counts.show(20)\n\n[Stage 73:=====================================================&gt;(116 + 1) / 117]                                                                                \n\n\n+------+------+\n|ticker| count|\n+------+------+\n|     I|180388|\n|     A|  7143|\n|   IRA|  6795|\n|    US|  5708|\n|     S|  4823|\n|    OP|  3990|\n|   VOO|  3343|\n|   ETF|  3053|\n|  HYSA|  2392|\n|    AI|  2347|\n|   VTI|  2250|\n|   DCA|  1449|\n|   SPY|  1383|\n|    CD|  1296|\n|   BTC|  1205|\n|  NVDA|  1091|\n|     T|  1062|\n|   SEC|  1015|\n|    DM|  1011|\n|   WSB|   872|\n+------+------+\nonly showing top 20 rows\n\n\n\n\n!pip install yfinance\n\nCollecting yfinance\n  Using cached yfinance-0.2.50-py2.py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: pandas&gt;=1.3.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.2.3)\nRequirement already satisfied: numpy&gt;=1.16.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (1.26.4)\nRequirement already satisfied: requests&gt;=2.31 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.32.3)\nCollecting multitasking&gt;=0.0.7 (from yfinance)\n  Using cached multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\nRequirement already satisfied: lxml&gt;=4.9.1 in /opt/conda/lib/python3.11/site-packages (from yfinance) (5.3.0)\nRequirement already satisfied: platformdirs&gt;=2.0.0 in /opt/conda/lib/python3.11/site-packages (from yfinance) (3.11.0)\nRequirement already satisfied: pytz&gt;=2022.5 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2023.3)\nRequirement already satisfied: frozendict&gt;=2.3.4 in /opt/conda/lib/python3.11/site-packages (from yfinance) (2.4.6)\nCollecting peewee&gt;=3.16.2 (from yfinance)\n  Using cached peewee-3.17.8-cp311-cp311-linux_x86_64.whl\nRequirement already satisfied: beautifulsoup4&gt;=4.11.1 in /opt/conda/lib/python3.11/site-packages (from yfinance) (4.12.3)\nCollecting html5lib&gt;=1.1 (from yfinance)\n  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: soupsieve&gt;1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4&gt;=4.11.1-&gt;yfinance) (2.5)\nRequirement already satisfied: six&gt;=1.9 in /opt/conda/lib/python3.11/site-packages (from html5lib&gt;=1.1-&gt;yfinance) (1.16.0)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.11/site-packages (from html5lib&gt;=1.1-&gt;yfinance) (0.5.1)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.3.0-&gt;yfinance) (2.9.0)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=1.3.0-&gt;yfinance) (2024.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.31-&gt;yfinance) (3.4.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.31-&gt;yfinance) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.31-&gt;yfinance) (1.26.19)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.31-&gt;yfinance) (2024.8.30)\nUsing cached yfinance-0.2.50-py2.py3-none-any.whl (102 kB)\nUsing cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\nUsing cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\nInstalling collected packages: peewee, multitasking, html5lib, yfinance\nSuccessfully installed html5lib-1.1 multitasking-0.0.11 peewee-3.17.8 yfinance-0.2.50\n\n\n\nimport pandas as pd\nimport plotly.express as px\nimport yfinance as yf\nfrom pyspark.sql.functions import col, regexp_extract, count\n\n# Step 1: Extract Stock Tickers and Mentions from Reddit Data\ndef extract_ticker_mentions(reddit_df, ticker_regex):\n    \"\"\"\n    Extract and count stock tickers mentioned in Reddit posts.\n    :param reddit_df: PySpark DataFrame with Reddit comments/submissions\n    :param ticker_regex: Regex pattern to identify stock tickers (e.g., capital letters 1-5 characters)\n    :return: Pandas DataFrame with tickers and their mention counts\n    \"\"\"\n\n    tickers_df = (\n        reddit_df\n        .withColumn('ticker', regexp_extract(col('body'), ticker_regex, 0))\n        .filter(col('ticker') != \"\")\n        .groupBy('ticker')\n        .agg(count('*').alias('mention_count'))\n        .orderBy(col('mention_count').desc())\n        .limit(50)  # Top 50 tickers\n        .toPandas()\n    )\n    return tickers_df\n\nticker_regex = r'\\b[A-Z]{1,5}\\b'\nreddit_df = spark.read.parquet(s3_path)\ntop_tickers = extract_ticker_mentions(reddit_df, ticker_regex)\n\n                                                                                \n\n\n\n# Step 2: Fetch Market Data with Company Names from Yahoo Finance\ndef fetch_market_data_yahoo(tickers):\n    \"\"\"\n    Fetch market data including company names, market capitalization, and outstanding shares.\n    :param tickers: List of stock tickers.\n    :return: DataFrame with ticker, company name, market_cap, and trade_count.\n    \"\"\"\n    market_data = []\n    \n    for ticker in tickers:\n        try:\n            stock = yf.Ticker(ticker)\n            info = stock.info\n            market_data.append({\n                \"ticker\": ticker,\n                \"company_name\": info.get(\"shortName\", ticker),  # Use ticker if name is unavailable\n                \"market_cap\": info.get(\"marketCap\", 0),  # Market Capitalization\n                \"trade_count\": info.get(\"sharesOutstanding\", 0)  # Outstanding Shares\n            })\n        except Exception as e:\n            print(f\"Error fetching data for {ticker}: {e}\")\n    \n    return pd.DataFrame(market_data)\n\nmarket_data = fetch_market_data_yahoo(top_tickers['ticker'].tolist())\n\n# Step 3: Merge Data\nmerged_data = pd.merge(top_tickers, market_data, on='ticker', how='inner')\n\n# Remove invalid or zero values\nmerged_data = merged_data[\n    (merged_data['market_cap'] &gt; 0) & \n    (merged_data['trade_count'] &gt; 0) &\n    (merged_data['mention_count'] &gt; 0)\n]\n\ndef create_interactive_bubble_plot(data, save_as=\"html\"):\n    \"\"\"\n    Create an interactive bubble plot without a dropdown and save it.\n    :param data: Pandas DataFrame with 'mention_count', 'market_cap', 'trade_count', and 'company_name'\n    :param save_as: Format to save the plot ('png' or 'html')\n    \"\"\"\n    # Combine ticker and company name for display\n    data['label'] = data['ticker'] + \" - \" + data['company_name']\n    \n    # Create the figure\n    fig = px.scatter(\n        data,\n        x='mention_count',\n        y='market_cap',\n        size='trade_count',\n        color='ticker',\n        hover_name='label',  # Show company name and ticker in hover\n        size_max=60,\n        labels={\n            'mention_count': 'Mention Count (in thousands)',\n            'market_cap': 'Market Capitalization in Billions (USD)',\n            'trade_count': 'Trade Volume'\n        },\n        title=\"Stock Popularity vs Market Cap and Trade Volume\",\n    )\n\n    # Save the plot\n    if save_as == \"png\":\n        fig.write_image(\"bubble_plot.png\")\n        print(\"Plot saved as 'bubble_plot.png'\")\n    elif save_as == \"html\":\n        fig.write_html(\"bubble_plot.html\")\n        print(\"Plot saved as 'bubble_plot.html'\")\n\n    fig.show()\n\n# Example usage\ncreate_interactive_bubble_plot(merged_data, save_as=\"html\")\n\nERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/DYOR?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=DYOR&crumb=CuJz0cgaUfS\nERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/USDT?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=USDT&crumb=CuJz0cgaUfS\nERROR:yfinance:404 Client Error: Not Found for url: https://query2.finance.yahoo.com/v10/finance/quoteSummary/MATIC?modules=financialData%2CquoteType%2CdefaultKeyStatistics%2CassetProfile%2CsummaryDetail&corsDomain=finance.yahoo.com&formatted=false&symbol=MATIC&crumb=CuJz0cgaUfS\n\n\nPlot saved as 'bubble_plot.html'"
  },
  {
    "objectID": "eda_investing.html",
    "href": "eda_investing.html",
    "title": "Investing Sub reddit Exploratory Data Analysis",
    "section": "",
    "text": "Link to full code:\nlink\n\n\nThis exploratory data analysis (EDA) investigates comment trends, topics, and sentiment in the investing subreddit. By analyzing Reddit discussions, we aim to uncover insights that can be leveraged for content strategy, product development, and investment-related decision-making. Social media platforms like Reddit have become influential in shaping investor sentiment, and identifying patterns within these discussions can help understand emerging trends, popular topics, and interest in specific using Alpha Vantage API to gather stock prices .\n\n\n\n\n\n\n\nObjective: * Identify frequently used words in the investing subreddit to understand the main discussion themes.\n\n\n\n\n\n\nWords like “money,” “market,” “years,” and “invest” suggest that users are primarily discussing financial concepts, market conditions, and long-term investment strategies. The presence of words such as “think” and “get” might indicate that many comments involve opinions, advice, or inquiries about market trends. Frequent words like “please” and “question” imply that users may seek guidance, support, or clarification from others in the community. Understanding the frequent terms can help financial service providers tailor their content to address the most common concerns of the subreddit community, such as providing guidance on market strategies or long-term investing.\n\n\n\n\n\n\n\nObjective: * Use a word cloud to visualize the most popular terms, providing insights for content strategy and product development.\n\n\n\n\n\n\nWords like “money,” “year,” “post,” and “question” stand out as highly discussed topics. This indicates that users are focused on financial matters and often seek information or clarification. Terms such as “contact,” “invest,” “market,” and “action” suggest users are actively discussing investment actions, market conditions, and seeking advice. Words like “please” and “would” show a polite and inquisitive tone, suggesting that many comments are requests or inquiries rather than statements. This visualization helps content creators and financial service providers to focus on popular investment topics, tailoring educational resources and FAQs to address these specific areas of interest within the investing community.\n\n\n\n\n\n\n\nObjective: * Analyze comment volume over time to identify peaks and trends that might correlate with specific events or market conditions.\n\n\n\n\n\n\nWe can observe periods of increased comment volume, possibly correlating with events like quarterly earnings reports, major economic announcements, or high market volatility. Peaks in comment volume could indicate heightened interest or concern, potentially tied to external factors impacting the stock market (e.g., inflation reports, geopolitical events). A sustained increase or decrease in comment volume could suggest changes in investor engagement or interest over time, which may be useful for predicting shifts in market sentiment. Financial analysts and investment firms can use this data to monitor when investors are most engaged, providing an early indicator of market sentiment and potential volatility. Recognizing these spikes can help them align their outreach efforts with periods of high investor activity.\n\n\n\n\n\n\n\nObjective: * Use a bubble chart to gauge the popularity of specific stocks based on Reddit mentions and compare these against stock prices.\n\n\n\n\n\n\nStocks with high mentions but varying prices show that price alone does not drive popularity. For example, highly discussed low-priced stocks may indicate interest in speculative or trending assets, while high-priced, frequently mentioned stocks likely represent established companies. Stocks with larger bubbles have the highest number of mentions, indicating strong interest or high relevance within the subreddit community. Stocks with both high prices and high mention volumes might be subject to more scrutiny and could exhibit higher volatility, as many investors track and discuss them closely. Understanding which stocks are most discussed can help analysts gauge retail investor sentiment and identify stocks that may be susceptible to price movements due to public interest. This data can inform investment strategies, risk management, and the focus of research reports."
  },
  {
    "objectID": "eda_investing.html#introduction",
    "href": "eda_investing.html#introduction",
    "title": "Investing Sub reddit Exploratory Data Analysis",
    "section": "",
    "text": "This exploratory data analysis (EDA) investigates comment trends, topics, and sentiment in the investing subreddit. By analyzing Reddit discussions, we aim to uncover insights that can be leveraged for content strategy, product development, and investment-related decision-making. Social media platforms like Reddit have become influential in shaping investor sentiment, and identifying patterns within these discussions can help understand emerging trends, popular topics, and interest in specific using Alpha Vantage API to gather stock prices ."
  },
  {
    "objectID": "eda_investing.html#business-questions-and-analysis",
    "href": "eda_investing.html#business-questions-and-analysis",
    "title": "Investing Sub reddit Exploratory Data Analysis",
    "section": "",
    "text": "Objective: * Identify frequently used words in the investing subreddit to understand the main discussion themes.\n\n\n\n\n\n\nWords like “money,” “market,” “years,” and “invest” suggest that users are primarily discussing financial concepts, market conditions, and long-term investment strategies. The presence of words such as “think” and “get” might indicate that many comments involve opinions, advice, or inquiries about market trends. Frequent words like “please” and “question” imply that users may seek guidance, support, or clarification from others in the community. Understanding the frequent terms can help financial service providers tailor their content to address the most common concerns of the subreddit community, such as providing guidance on market strategies or long-term investing.\n\n\n\n\n\n\n\nObjective: * Use a word cloud to visualize the most popular terms, providing insights for content strategy and product development.\n\n\n\n\n\n\nWords like “money,” “year,” “post,” and “question” stand out as highly discussed topics. This indicates that users are focused on financial matters and often seek information or clarification. Terms such as “contact,” “invest,” “market,” and “action” suggest users are actively discussing investment actions, market conditions, and seeking advice. Words like “please” and “would” show a polite and inquisitive tone, suggesting that many comments are requests or inquiries rather than statements. This visualization helps content creators and financial service providers to focus on popular investment topics, tailoring educational resources and FAQs to address these specific areas of interest within the investing community.\n\n\n\n\n\n\n\nObjective: * Analyze comment volume over time to identify peaks and trends that might correlate with specific events or market conditions.\n\n\n\n\n\n\nWe can observe periods of increased comment volume, possibly correlating with events like quarterly earnings reports, major economic announcements, or high market volatility. Peaks in comment volume could indicate heightened interest or concern, potentially tied to external factors impacting the stock market (e.g., inflation reports, geopolitical events). A sustained increase or decrease in comment volume could suggest changes in investor engagement or interest over time, which may be useful for predicting shifts in market sentiment. Financial analysts and investment firms can use this data to monitor when investors are most engaged, providing an early indicator of market sentiment and potential volatility. Recognizing these spikes can help them align their outreach efforts with periods of high investor activity.\n\n\n\n\n\n\n\nObjective: * Use a bubble chart to gauge the popularity of specific stocks based on Reddit mentions and compare these against stock prices.\n\n\n\n\n\n\nStocks with high mentions but varying prices show that price alone does not drive popularity. For example, highly discussed low-priced stocks may indicate interest in speculative or trending assets, while high-priced, frequently mentioned stocks likely represent established companies. Stocks with larger bubbles have the highest number of mentions, indicating strong interest or high relevance within the subreddit community. Stocks with both high prices and high mention volumes might be subject to more scrutiny and could exhibit higher volatility, as many investors track and discuss them closely. Understanding which stocks are most discussed can help analysts gauge retail investor sentiment and identify stocks that may be susceptible to price movements due to public interest. This data can inform investment strategies, risk management, and the focus of research reports."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "Banners of the subreddits analyzed for the project\n\n\nBy analyzing a vast collection of investment-related subreddits, including r/cryptocurrency, and r/investing, we were able to extract rich insights that enhanced our understanding of the discussions and behaviors shaping these online communities. Through exploratory data analysis (EDA), natural language processing (NLP), and machine learning (ML) models, we explored several avenues to understand patterns, sentiments, and their relationship with financial market dynamics.\nFirst, we sought to determine the types of investments and stocks most frequently discussed in these subreddits. Using textual content analysis and visualizations, we identified key discussion themes, emerging trends, and the influence of market events on user engagement. Peaks in comment volumes often correlated with major financial announcements or market movements, underscoring the potential for Reddit as an indicator of retail investor sentiment.\nNext, we conducted NLP-driven sentiment analysis to evaluate the tone of subreddit discussions. Our analysis revealed an overall optimistic sentiment, especially in cryptocurrency-related discussions, where comments exhibited a higher proportion of positive sentiment. By identifying topics like the Federal Reserve and Bitcoin, we further analyzed how sentiment fluctuated over time, linking spikes to external factors such as economic reports or regulatory announcements.\nWe then applied ML techniques to investigate whether these Reddit discussions could predict Bitcoin price movements or volatility. Despite rigorous preprocessing, feature engineering, and experimentation with various models—including regression and gradient-boosted trees—the predictive power of these discussions proved limited. This result highlights the challenges of using social media data to predict financial markets, which are inherently complex and influenced by numerous variables beyond online sentiment.\nAdditionally, we observed disparities in engagement levels across subreddits and user types, revealing insights into how different communities discuss financial topics. Understanding these dynamics can be valuable for market analysts, content creators, and financial institutions seeking to tailor their strategies to retail investor trends.\n\n\n\nHaving explored several aspects of investment-related discussions on Reddit, there are many opportunities to further enhance our understanding:\n\nAdvanced NLP Techniques: Employing transformer-based models like BERT or GPT for topic modeling and sentiment analysis could provide deeper insights into user behavior and sentiment nuances.\nMulti-Platform Analysis: Expanding the scope to include other social media platforms, such as Twitter or Discord, could reveal additional patterns and correlations in retail investor sentiment.\nPredictive Modeling: Investigating alternative datasets or features, such as user engagement metrics or cross-subreddit interactions, may improve model performance for financial predictions.\nSentiment and Market Dynamics: Studying the temporal lag between sentiment shifts and market reactions could uncover predictive relationships not captured in this project.\n\n\n\n\n\nThis project provided valuable insights into the dynamic and complex relationships between social media discussions and financial markets. By leveraging techniques like EDA, NLP, and ML, we demonstrated the potential of Reddit as a tool for understanding retail investor sentiment and engagement trends. While our findings suggest limitations in using these discussions for direct market predictions, they highlight the significant role of social platforms in shaping investment narratives.\nA project by Group 12."
  },
  {
    "objectID": "summary.html#future-work",
    "href": "summary.html#future-work",
    "title": "Summary",
    "section": "",
    "text": "Having explored several aspects of investment-related discussions on Reddit, there are many opportunities to further enhance our understanding:\n\nAdvanced NLP Techniques: Employing transformer-based models like BERT or GPT for topic modeling and sentiment analysis could provide deeper insights into user behavior and sentiment nuances.\nMulti-Platform Analysis: Expanding the scope to include other social media platforms, such as Twitter or Discord, could reveal additional patterns and correlations in retail investor sentiment.\nPredictive Modeling: Investigating alternative datasets or features, such as user engagement metrics or cross-subreddit interactions, may improve model performance for financial predictions.\nSentiment and Market Dynamics: Studying the temporal lag between sentiment shifts and market reactions could uncover predictive relationships not captured in this project."
  },
  {
    "objectID": "summary.html#final-thoughts",
    "href": "summary.html#final-thoughts",
    "title": "Summary",
    "section": "",
    "text": "This project provided valuable insights into the dynamic and complex relationships between social media discussions and financial markets. By leveraging techniques like EDA, NLP, and ML, we demonstrated the potential of Reddit as a tool for understanding retail investor sentiment and engagement trends. While our findings suggest limitations in using these discussions for direct market predictions, they highlight the significant role of social platforms in shaping investment narratives.\nA project by Group 12."
  }
]